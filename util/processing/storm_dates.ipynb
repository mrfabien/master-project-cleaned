{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_eu = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project-cleaned/pre_processing/maps/QGIS/square_europe_updated.csv')\n",
    "storm_dates = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project-cleaned/pre_processing/tracks/storms_dates_work.csv',\n",
    "                          delimiter = ';')\n",
    "name_storm = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project-cleaned/pre_processing/tracks/C3S_StormTracks_era5_19792021_0100_v3.csv',\n",
    "                         delimiter=';')\n",
    "\n",
    "landfall_eu = sq_eu.drop_duplicates(subset=['layer'], keep='first')\n",
    "landfall_eu = landfall_eu.drop(columns=['lon_east', 'lon_west', 'lat_north', 'lat_south', 'path','center_lon','center_lat'])\n",
    "\n",
    "name_storm_index = name_storm[name_storm['Time&Longitude&Latitude'].str.startswith('TRACK_ID')]\n",
    "name_storm = name_storm[name_storm['Time&Longitude&Latitude'].str.startswith('TRACK_ID')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/tensor_flow/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m all_storms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(name_storm)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m storm \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,all_storms):\n\u001b[0;32m----> 9\u001b[0m     match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRACK_ID (\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+).*?ENSMBLE_MEMB 0 (.+)\u001b[39m\u001b[38;5;124m'\u001b[39m, line[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime&Longitude&Latitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[storm])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m match:\n\u001b[1;32m     12\u001b[0m     track_id_value \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor_flow/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor_flow/lib/python3.11/site-packages/pandas/core/indexing.py:1431\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m-> 1431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_label(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor_flow/lib/python3.11/site-packages/pandas/core/indexing.py:1381\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: AxisInt):\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[0;32m-> 1381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mxs(label, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor_flow/lib/python3.11/site-packages/pandas/core/generic.py:4298\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   4296\u001b[0m             new_index \u001b[38;5;241m=\u001b[39m index[loc]\n\u001b[1;32m   4297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4298\u001b[0m     loc \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m   4301\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m loc\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor_flow/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "'''import re\n",
    "\n",
    "# Input string\n",
    "line = name_storm[name_storm['Time&Longitude&Latitude'].str.startswith('TRACK_ID')]\n",
    "\n",
    "# Use regex to find the number after TRACK_ID and the text after 0\n",
    "all_storms = len(name_storm)\n",
    "for storm in range(0,all_storms):\n",
    "    match = re.search(r'TRACK_ID (\\d+).*?ENSMBLE_MEMB 0 (.+)', line['Time&Longitude&Latitude'].loc[storm])\n",
    "\n",
    "if match:\n",
    "    track_id_value = match.group(1)\n",
    "    after_zero = match.group(2)\n",
    "\n",
    "    print(\"TRACK_ID value:\", track_id_value)\n",
    "    print(\"Text after 0:\", after_zero)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"storm_eu_dates = storm_dates.drop(columns=['end_date', '3_hour_steps', '1_hour_steps'])\\nlandfall_eu['storm_index'] = pd.to_numeric(landfall_eu['storm_index'], errors='coerce')\\n\\n# keep only the name of the storms\\n\\nname_storm = name_storm['Time&Longitude&Latitude'].str.split(' 0 ', n=1).str[1].str.strip()\\nname_storm = name_storm.apply(lambda x: x.replace('NAME', '', 1).strip() if x.startswith('NAME') else x)\\nname_storm.reset_index(drop=True, inplace=True)\\nname_storm = pd.DataFrame(name_storm)\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the column 'layer' rename each fid_reproj_tc_irad_i_interp by storm_i\n",
    "\n",
    "landfall_eu = landfall_eu.rename(columns={'layer': 'storm_index'})\n",
    "landfall_eu['storm_index'] = landfall_eu['storm_index'].str.replace('fid_reproj_tc_irad_', '')\n",
    "landfall_eu['storm_index'] = landfall_eu['storm_index'].str.replace('_interp', '')\n",
    "\n",
    "# clean the storm_dates dataframe\n",
    "\n",
    "'''storm_eu_dates = storm_dates.drop(columns=['end_date', '3_hour_steps', '1_hour_steps'])\n",
    "landfall_eu['storm_index'] = pd.to_numeric(landfall_eu['storm_index'], errors='coerce')\n",
    "\n",
    "# keep only the name of the storms\n",
    "\n",
    "name_storm = name_storm['Time&Longitude&Latitude'].str.split(' 0 ', n=1).str[1].str.strip()\n",
    "name_storm = name_storm.apply(lambda x: x.replace('NAME', '', 1).strip() if x.startswith('NAME') else x)\n",
    "name_storm.reset_index(drop=True, inplace=True)\n",
    "name_storm = pd.DataFrame(name_storm)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"test = storm_eu_dates['start_date'][0+1]\\nhours_to_add = landfall_eu.loc[landfall_eu['storm_index'] == 1]\\nhours_to_add = int(hours_to_add['fid'].values[0])\\ndate_time_obj = datetime.strptime(test, '%Y-%m-%dT%H:%M:%S')\\ndate_time_obj = date_time_obj + timedelta(hours=hours_to_add)\\n\\n# convert back to string\\n\\ndate_time_obj.strftime('%Y-%m-%dT%H:%M:%S')\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''test = storm_eu_dates['start_date'][0+1]\n",
    "hours_to_add = landfall_eu.loc[landfall_eu['storm_index'] == 1]\n",
    "hours_to_add = int(hours_to_add['fid'].values[0])\n",
    "date_time_obj = datetime.strptime(test, '%Y-%m-%dT%H:%M:%S')\n",
    "date_time_obj = date_time_obj + timedelta(hours=hours_to_add)\n",
    "\n",
    "# convert back to string\n",
    "\n",
    "date_time_obj.strftime('%Y-%m-%dT%H:%M:%S')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storm 90 not found\n"
     ]
    }
   ],
   "source": [
    "storm_eu_arrivals = []\n",
    "steps_added = []\n",
    "\n",
    "for i in range(len(storm_eu_dates)):\n",
    "    try:\n",
    "        start = storm_eu_dates['start_date'][i]\n",
    "        hours_to_add = landfall_eu.loc[landfall_eu['storm_index'] == i+1]\n",
    "        hours_to_add = int(hours_to_add['fid'].values[0])\n",
    "        start_reformat = datetime.strptime(start, '%Y-%m-%dT%H:%M:%S')\n",
    "        start_reformat = start_reformat + timedelta(hours=hours_to_add)\n",
    "        storm_eu_arrivals.append(start_reformat.strftime('%Y-%m-%dT%H:%M:%S'))\n",
    "        steps_added.append(hours_to_add)\n",
    "    except:\n",
    "        print(f'storm {i} not found')\n",
    "        storm_eu_arrivals.append(-1)\n",
    "        steps_added.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the column 'arrival_date' to storm_dates\n",
    "\n",
    "storm_dates['eu_landfall_date'] = storm_eu_arrivals\n",
    "storm_dates['nb_steps_before_eu_landfall_1_hour'] = steps_added\n",
    "#storm_dates['storm_name'] = name_storm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    storm_index_match                   storm_name\n",
      "0                   1                        DARIA\n",
      "1                   2        C3S_STORM_TRACKS_ERA5\n",
      "2                   3        C3S_STORM_TRACKS_ERA5\n",
      "3                   7        C3S_STORM_TRACKS_ERA5\n",
      "4                   4                       VIVIAN\n",
      "..                ...                          ...\n",
      "90                 91                       CIARA2\n",
      "91                 92                       DENNIS\n",
      "92                 93                         ALEX\n",
      "93                 94  AIDEN C3S_STORM_TRACKS_ERA5\n",
      "94                 95        C3S_STORM_TRACKS_ERA5\n",
      "\n",
      "[95 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "matched_data = []\n",
    "df_storm_eu_arrivals= []\n",
    "df_steps_added = []\n",
    "\n",
    "df_storm_eu_arrivals = pd.DataFrame(storm_eu_arrivals)\n",
    "df_steps_added = pd.DataFrame(steps_added)\n",
    "\n",
    "# Loop through storm_dates and find matches\n",
    "for idx in storm_eu_dates['storm_index']:\n",
    "    if idx in name_storm.index:\n",
    "        matched_storm_name = name_storm['Time&Longitude&Latitude'].loc[idx]\n",
    "        #matched_storm_eu_arrivals = df_storm_eu_arrivals.loc[idx][0] they are alredy in the right order !\n",
    "        #matched_steps_added = df_steps_added.loc[idx][0]\n",
    "        matched_data.append([idx, matched_storm_name])\n",
    "\n",
    "# Convert matched data to a new DataFrame\n",
    "matched_df = pd.DataFrame(matched_data, columns=['storm_index_match','storm_name'])\n",
    "print(matched_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990-01-23T17:00:00',\n",
       " '1990-01-25T05:00:00',\n",
       " '1990-02-01T13:00:00',\n",
       " '1990-02-02T16:00:00',\n",
       " '1990-02-20T18:00:00',\n",
       " '1990-02-28T09:00:00',\n",
       " '1990-02-24T20:00:00',\n",
       " '1991-01-05T10:00:00',\n",
       " '1991-12-19T06:00:00',\n",
       " '1992-01-01T00:00:00',\n",
       " '1992-02-26T04:00:00',\n",
       " '1992-12-18T03:00:00',\n",
       " '1992-12-25T06:00:00',\n",
       " '1993-01-09T17:00:00',\n",
       " '1993-01-10T21:00:00',\n",
       " '1993-01-14T20:00:00',\n",
       " '1993-01-14T14:00:00',\n",
       " '1993-02-02T18:00:00',\n",
       " '1993-12-08T13:00:00',\n",
       " '1994-01-22T23:00:00',\n",
       " '1994-01-27T06:00:00',\n",
       " '1994-12-08T01:00:00',\n",
       " '1995-01-17T11:00:00',\n",
       " '1995-01-23T00:00:00',\n",
       " '1995-10-24T13:00:00',\n",
       " '1996-10-11T13:00:00',\n",
       " '1996-10-28T03:00:00',\n",
       " '1996-11-06T00:00:00',\n",
       " '1997-02-19T12:00:00',\n",
       " '1997-02-22T23:00:00',\n",
       " '1997-12-23T12:00:00',\n",
       " '1998-01-04T06:00:00',\n",
       " '1998-01-03T01:00:00',\n",
       " '1998-10-30T00:00:00',\n",
       " '1998-12-26T09:00:00',\n",
       " '1999-01-13T12:00:00',\n",
       " '1999-12-03T02:00:00',\n",
       " '1999-12-26T01:00:00',\n",
       " '1999-12-28T23:00:00',\n",
       " '2000-01-01T17:00:00',\n",
       " '2000-10-30T06:00:00',\n",
       " '2000-11-25T03:00:00',\n",
       " '2001-12-27T20:00:00',\n",
       " '2002-01-28T07:00:00',\n",
       " '2002-02-25T22:00:00',\n",
       " '2002-10-26T21:00:00',\n",
       " '2005-01-07T22:00:00',\n",
       " '2005-01-11T20:00:00',\n",
       " '2005-11-07T17:00:00',\n",
       " '2005-11-11T09:00:00',\n",
       " '2006-01-10T21:00:00',\n",
       " '2006-12-31T11:00:00',\n",
       " '2007-01-16T04:00:00',\n",
       " '2007-03-19T11:00:00',\n",
       " '2007-11-07T17:00:00',\n",
       " '2007-12-09T01:00:00',\n",
       " '2008-01-08T13:00:00',\n",
       " '2008-01-24T23:00:00',\n",
       " '2008-02-07T07:00:00',\n",
       " '2008-02-28T21:00:00',\n",
       " '2008-03-12T11:00:00',\n",
       " '2008-12-19T16:00:00',\n",
       " '2009-01-17T15:00:00',\n",
       " '2009-01-23T21:00:00',\n",
       " '2010-02-27T01:00:00',\n",
       " '2011-02-03T19:00:00',\n",
       " '2011-11-24T22:00:00',\n",
       " '2011-12-08T08:00:00',\n",
       " '2011-12-25T12:00:00',\n",
       " '2012-01-03T02:00:00',\n",
       " '2013-10-28T00:00:00',\n",
       " '2013-11-16T01:00:00',\n",
       " '2013-12-05T04:00:00',\n",
       " '2013-12-12T10:00:00',\n",
       " '2013-12-15T11:00:00',\n",
       " '2013-12-15T18:00:00',\n",
       " '2013-12-15T15:00:00',\n",
       " '2013-12-26T22:00:00',\n",
       " '2014-02-12T08:00:00',\n",
       " '2015-01-10T00:00:00',\n",
       " '2015-01-09T00:00:00',\n",
       " '2015-01-12T06:00:00',\n",
       " '2015-01-14T21:00:00',\n",
       " '2015-02-06T18:00:00',\n",
       " '2016-01-29T06:00:00',\n",
       " '2016-02-01T21:00:00',\n",
       " '2016-02-08T03:00:00',\n",
       " '2016-12-23T18:00:00',\n",
       " '2016-12-25T06:00:00',\n",
       " '2017-10-14T16:00:00',\n",
       " -1,\n",
       " '2020-02-09T07:00:00',\n",
       " '2020-02-15T15:00:00',\n",
       " '2020-10-01T21:00:00',\n",
       " '2020-10-31T04:00:00',\n",
       " '2021-03-23T19:00:00']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storm_eu_arrivals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([storm_dates, matched_df], axis=1)\n",
    "final_df = final_df.drop(columns=['Unnamed: 0.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the new storm_dates\n",
    "\n",
    "final_df.to_csv('/Users/fabienaugsburger/Documents/GitHub/master-project-cleaned/pre_processing/tracks/storms_dates_steps_index_landfall_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go back to scratch, dates are not correct\n",
    "\n",
    "# original file for storm dates from C3S\n",
    "\n",
    "original_storm_dates = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project-cleaned/pre_processing/tracks/C3S_StormTracks_era5_19792021_0100_v3.csv',\n",
    "                                      delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all columns except the 1st one \n",
    "\n",
    "transformed_storm_dates = original_storm_dates.drop(original_storm_dates.columns[1:], axis=1)\n",
    "\n",
    "# then split the column 'Time&Longitude&Latitude' into 3 columns\n",
    "\n",
    "transformed_storm_dates[['Time&Longitude&Latitude','Longitude','Latitude','whateverthatis']] = transformed_storm_dates['Time&Longitude&Latitude'].str.split(' ', n=3, expand=True)\n",
    "transformed_storm_dates.rename(columns={'Time&Longitude&Latitude': 'Time'}, inplace=True)\n",
    "\n",
    "# extract each line that starts with POINT_NUM (it represents the number of points in the track)\n",
    "# Get the number of steps stored in 'Longitude' and 'Latitude' with the line that starts with 'POINT_NUM' in the column 'Time'\n",
    "steps_longitude = transformed_storm_dates[transformed_storm_dates['Time'].str.startswith('POINT_NUM')]['Longitude'].to_frame().reset_index(drop=True)\n",
    "steps_latitude = transformed_storm_dates[transformed_storm_dates['Time'].str.startswith('POINT_NUM')]['Latitude'].to_frame().reset_index(drop=True)\n",
    "# Replace spaces (' ') with actual empty strings ('') in 'Longitude'\n",
    "steps_longitude = steps_longitude.replace(' ', '')\n",
    "\n",
    "# Use steps stored in Longitude where it's not an empty string, otherwise use the value in Latitude\n",
    "steps = pd.DataFrame(np.where(steps_longitude != '', steps_longitude, steps_latitude), columns=['nb_steps'])\n",
    "\n",
    "# extract each line that starts with TRACK_ID (it represents the index of the storm)\n",
    "\n",
    "index = transformed_storm_dates[transformed_storm_dates['Time'].str.startswith('TRACK_ID')]['Longitude'].to_frame().reset_index(drop=True)\n",
    "index.rename(columns={'Longitude': 'storm_index'}, inplace=True)\n",
    "# substract 24 from the index to start at 1 and the storm names\n",
    "index['storm_index'] = pd.to_numeric(index['storm_index']) - 24\n",
    "\n",
    "# extract the names of the storms\n",
    "index['name'] = transformed_storm_dates[transformed_storm_dates['Time'].str.startswith('TRACK_ID')]['whateverthatis'].to_frame().reset_index(drop=True)\n",
    "index['name'] = index['name'].str.split(' 0 ', n=1).str[1].str.strip()\n",
    "index['name'] = index['name'].apply(lambda x: x.replace('NAME', '', 1).strip() if x.startswith('NAME') else x)\n",
    "# remove the C3S_STORM_TRACKS_ERA5 for the storm 95\n",
    "index_storm_95 = index.index[index['storm_index'] == 95].values[0]\n",
    "# Update the 'name' column in place\n",
    "index.loc[index_storm_95, 'name'] = index.loc[index_storm_95, 'name'].replace('C3S_STORM_TRACKS_ERA5', '', 1)\n",
    "\n",
    "# extract the date of start and end of the storm\n",
    "# THIS PART IS NOT THE START DATE FOR WHATEVER REASON\n",
    "'''start_date = transformed_storm_dates[transformed_storm_dates['Time'].str.startswith('TRACK_ID')]['whateverthatis'].to_frame().reset_index(drop=True)\n",
    "start_date = start_date['whateverthatis'].str.split(' ', n=1).str[0].str.strip().to_frame()\n",
    "start_date = start_date.rename(columns={'whateverthatis': 'start_date'})'''\n",
    "\n",
    "# find the index of the row where 'Time' is \"POINT_NUM\"\n",
    "start_indexes = transformed_storm_dates.index[transformed_storm_dates['Time'] == 'POINT_NUM'].to_list()\n",
    "start_date = []\n",
    "for start_index in start_indexes:\n",
    "    previous_row_values = transformed_storm_dates.iloc[start_index + 1]['Time']\n",
    "    start_date.append(previous_row_values)\n",
    "\n",
    "\n",
    "# same for the end date\n",
    "# find the index of the row where 'Time' is \"TRACK_ID\"\n",
    "end_indexes = transformed_storm_dates.index[transformed_storm_dates['Time'] == 'TRACK_ID'].to_list()\n",
    "# remove the first value in end_indexes and add the last index of the transformed_storm_dates\n",
    "end_indexes.pop(0)\n",
    "\n",
    "last_index = transformed_storm_dates.index[0]\n",
    "end_indexes.append(last_index)\n",
    "# Get the values of the row before \"TRACK_ID\"\n",
    "end_date = []\n",
    "for end_index in end_indexes:\n",
    "    previous_row_values = transformed_storm_dates.iloc[end_index - 1]['Time']\n",
    "    end_date.append(previous_row_values)\n",
    "\n",
    "# transform the end date and start date into the format 'YYYY-MM-DDTHH:MM:SS'\n",
    "end_date_formatted = pd.to_datetime(end_date,format=\"%Y%m%d%H\")\n",
    "end_date_formatted = end_date_formatted.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "end_date_formatted = pd.DataFrame(end_date_formatted, columns=['end_date'])\n",
    "\n",
    "start_date_formatted = pd.to_datetime(start_date,format=\"%Y%m%d%H\")\n",
    "start_date_formatted = start_date_formatted.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "start_date_formatted = pd.DataFrame(start_date_formatted, columns=['start_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one dataframe with index, name, start_date, end_date and nb_steps\n",
    "final_storm_dates = pd.concat([index, start_date_formatted], axis=1)\n",
    "final_storm_dates['end_date'] = end_date_formatted\n",
    "final_storm_dates = pd.concat([final_storm_dates, steps], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of steps is correct\n"
     ]
    }
   ],
   "source": [
    "# double check if the nb_steps actually corresponds of steps of 3 hours between start and end date\n",
    "test = pd.DataFrame(columns=['start_date'])\n",
    "\n",
    "test['start_date'] = pd.to_datetime(final_storm_dates['start_date'])\n",
    "test['end_date'] = pd.to_datetime(final_storm_dates['end_date'])\n",
    "test['diff'] = test['end_date'] - test['start_date']\n",
    "test['diff'] = test['diff'] / np.timedelta64(1, 'h')\n",
    "test['diff'] = test['diff'] / 3\n",
    "test['diff'] = test['diff'].astype(int)+1\n",
    "test['nb_steps'] = final_storm_dates['nb_steps'].astype(int)\n",
    "\n",
    "if test['diff'].equals(test['nb_steps']):\n",
    "    print('The number of steps is correct')\n",
    "else:\n",
    "    print('The number of steps is not correct at storm number: ')\n",
    "    print(test[test['diff'] != test['nb_steps']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of the square with a certain degree of latitude and longitude\n",
    "\n",
    "number_of_storms = len(final_storm_dates)\n",
    "degree = 4\n",
    "path_tracks = '/Users/fabienaugsburger/Documents/GitHub/master-project-cleaned/pre_processing/tracks/ALL_TRACKS/'\n",
    "\n",
    "for storm in range(0,number_of_storms):\n",
    "\n",
    "    lon_east = []\n",
    "    lon_west = []\n",
    "    lat_north = []\n",
    "    lat_south = []\n",
    "\n",
    "    for step in range(start_indexes[storm], end_indexes[storm]-1):\n",
    "        # get the latitude and longitude of the storm\n",
    "        lat = float(transformed_storm_dates['Latitude'][step+1])\n",
    "        lon = float(transformed_storm_dates['Longitude'][step+1])\n",
    "\n",
    "        lon_east_temp = (lon + degree) % 360\n",
    "        lon_west_temp = (lon - degree + 360 ) % 360\n",
    "        lat_north_temp = (lat + degree)\n",
    "        lat_south_temp = (lat - degree)\n",
    "\n",
    "        lon_east.append(round(lon_east_temp, 6))\n",
    "        lon_west.append(round(lon_west_temp, 6))\n",
    "        lat_north.append(round(lat_north_temp, 6))\n",
    "        lat_south.append(round(lat_south_temp, 6))\n",
    "\n",
    "    # create the square\n",
    "    storm_str = str(storm+1)\n",
    "    square_name = f'storm_{storm_str}'\n",
    "    square = pd.DataFrame({'lon_east': lon_east, 'lon_west': lon_west, 'lat_north': lat_north, 'lat_south': lat_south})\n",
    "\n",
    "    # Define the folder and file path\n",
    "    folder = 'tracks_3h/'\n",
    "    file_name = f'{square_name}.csv'  # Specify the file name\n",
    "    full_path = os.path.join(path_tracks, folder)\n",
    "\n",
    "    # Create the folder if it doesn't exist\n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "    # Define the full path to the file, including the file name\n",
    "    file_path = os.path.join(full_path, file_name)\n",
    "\n",
    "    # Save the square in a CSV file in the folder 'tracks_3h'\n",
    "    # If the file doesn't exist, it will be created\n",
    "    square.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the same but the conversion from 0-360 to -180-180\n",
    "\n",
    "for storm in range(0,number_of_storms):\n",
    "    \n",
    "        lon_east = []\n",
    "        lon_west = []\n",
    "        lat_north = []\n",
    "        lat_south = []\n",
    "    \n",
    "        for step in range(start_indexes[storm], end_indexes[storm]-1):\n",
    "            # get the latitude and longitude of the storm\n",
    "            lat = float(transformed_storm_dates['Latitude'][step+1])\n",
    "            lon = float(transformed_storm_dates['Longitude'][step+1])\n",
    "    \n",
    "            lon_east_temp = (lon + degree) % 360\n",
    "            lon_west_temp = (lon - degree + 360 ) % 360\n",
    "            lat_north_temp = (lat + degree)\n",
    "            lat_south_temp = (lat - degree)\n",
    "    \n",
    "            if lon_east_temp > 180:\n",
    "                lon_east_temp = lon_east_temp - 360\n",
    "            if lon_west_temp > 180:\n",
    "                lon_west_temp = lon_west_temp - 360\n",
    "    \n",
    "            lon_east.append(round(lon_east_temp, 6))\n",
    "            lon_west.append(round(lon_west_temp, 6))\n",
    "            lat_north.append(round(lat_north_temp, 6))\n",
    "            lat_south.append(round(lat_south_temp, 6))\n",
    "    \n",
    "        # create the square\n",
    "        storm_str = str(storm+1)\n",
    "        square_name = f'storm_{storm_str}'\n",
    "        square = pd.DataFrame({'lon_east': lon_east, 'lon_west': lon_west, 'lat_north': lat_north, 'lat_south': lat_south})\n",
    "    \n",
    "        # Define the folder and file path\n",
    "        folder = 'tracks_3h_GIS_friendly/'\n",
    "        file_name = f'{square_name}.csv'  # Specify the file name\n",
    "        full_path = os.path.join(path_tracks, folder)\n",
    "    \n",
    "        # Create the folder if it doesn't exist\n",
    "        os.makedirs(full_path, exist_ok=True)\n",
    "    \n",
    "        # Define the full path to the file, including the file name\n",
    "        file_path = os.path.join(full_path, file_name)\n",
    "    \n",
    "        # Save the square in a CSV file in the folder 'tracks_3h'\n",
    "        # If the file doesn't exist, it will be created\n",
    "        square.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now extrapolate the steps from 3 hours to 1 hour\n",
    "\n",
    "def interpolate_vector(data, factor):\n",
    "    n = len(data)\n",
    "    # X interpolation points. For factor=4, it is [0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, ...]\n",
    "    x = np.linspace(0, n - 1, (n - 1) * factor + 1)\n",
    "    # Alternatively:\n",
    "    # x = np.arange((n - 1) * factor + 1) / factor\n",
    "    # X data points: [0, 1, 2, ...]\n",
    "    xp = np.arange(n)\n",
    "    # Interpolate\n",
    "    return np.round(np.interp(x, xp, np.asarray(data)),6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
