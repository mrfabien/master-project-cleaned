{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from math import sqrt\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "# now you can import normally from model_selection\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "operating_system = 'mac'\n",
    "\n",
    "if operating_system == 'win':\n",
    "    os.chdir('C:/Users/fabau/OneDrive/Documents/GitHub/master-project-cleaned/')\n",
    "elif operating_system == 'curnagl':\n",
    "    os.chdir('/work/FAC/FGSE/IDyST/tbeucler/default/fabien/repos/cleaner_version/')\n",
    "else:\n",
    "    os.chdir('/Users/fabienaugsburger/Documents/GitHub/master-project-cleaned/')\n",
    "\n",
    "# Add the path to the custom library\n",
    "custom_library_path = os.path.abspath('util/processing/')\n",
    "sys.path.append(custom_library_path)\n",
    "custom_library_path = os.path.abspath('util/gev/')\n",
    "sys.path.append(custom_library_path)\n",
    "custom_library_path = os.path.abspath('util/feature_selection/')\n",
    "sys.path.append(custom_library_path)\n",
    "custom_library_path = os.path.abspath('util/ml/')\n",
    "sys.path.append(custom_library_path)\n",
    "\n",
    "import extraction_squares, pre_processing_data, data_processing, selection_vars, sensitivity_test\n",
    "\n",
    "'''if operating_system == 'curnagl':\n",
    "    name_of_variable= pd.read_csv('/work/FAC/FGSE/IDyST/tbeucler/default/fabien/repos/curnagl/DATASETS/variable_list_80_mean.csv')\n",
    "    path_data = '/work/FAC/FGSE/IDyST/tbeucler/default/fabien/repos/curnagl/DATASETS'\n",
    "else:'''\n",
    "name_of_variable_20 = pd.read_csv('pre_processing/feature_selection/fs_corr_timeseries/corr_inst_max_20.csv')['Unnamed: 0']#('data/variable_list_levels.csv')\n",
    "name_of_variable_30 = pd.read_csv('pre_processing/feature_selection/fs_corr_timeseries/corr_inst_max_30.csv')['Unnamed: 0']#('data/variable_list_levels.csv')\n",
    "name_of_variable_40 = pd.read_csv('pre_processing/feature_selection/fs_corr_timeseries/corr_inst_max_40.csv')['Unnamed: 0']#('data/variable_list_levels.csv')\n",
    "\n",
    "path_data = 'data'\n",
    "\n",
    "storm_dates = pd.read_csv('pre_processing/tracks/storm_dates.csv')\n",
    "#path_tracks_1h_non_EU = 'pre_processing/tracks/ALL_TRACKS/tracks_1h_non_EU'\n",
    "#dataset = 'datasets_1h'\n",
    "#dataset_non_EU = 'datasets_1h_non_EU'\n",
    "levels = pd.read_csv('data/levels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/01/5ryz4pnn581dj9gk6r1nn5qr0000gn/T/ipykernel_20600/2136302016.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  transposed_data_20['storm_number'] = storm_numbers\n",
      "/var/folders/01/5ryz4pnn581dj9gk6r1nn5qr0000gn/T/ipykernel_20600/2136302016.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  transposed_data_30['storm_number'] = storm_numbers\n",
      "/var/folders/01/5ryz4pnn581dj9gk6r1nn5qr0000gn/T/ipykernel_20600/2136302016.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  transposed_data_40['storm_number'] = storm_numbers\n"
     ]
    }
   ],
   "source": [
    "# import the all_loadings data\n",
    "all_loadings = pd.read_csv('pre_processing/nestedMLR/all_loadings.csv')\n",
    "\n",
    "# Extract variable names and storm data\n",
    "variables = all_loadings['variable']  # First column\n",
    "storm_data = all_loadings.iloc[:, 1:]  # All columns from the second onward\n",
    "\n",
    "# Transpose storm data and set variable names as columns\n",
    "transposed_data = storm_data.T\n",
    "transposed_data.columns = variables\n",
    "\n",
    "# Optionally reset index to name storms\n",
    "transposed_data.index.name = 'storm_number'\n",
    "transposed_data.reset_index(inplace=True)\n",
    "\n",
    "# extract the storm number\n",
    "storm_numbers = transposed_data['storm_number'].copy()\n",
    "\n",
    "# Extract variables most correlated with the target and leaving the storm number\n",
    "columns_to_select_20 = [col for col in name_of_variable_20.tolist() if col in transposed_data.columns]\n",
    "transposed_data_20 = transposed_data[columns_to_select_20]\n",
    "columns_to_select_30 = [col for col in name_of_variable_30.tolist() if col in transposed_data.columns]\n",
    "transposed_data_30 = transposed_data[columns_to_select_30]\n",
    "columns_to_select_40 = [col for col in name_of_variable_40.tolist() if col in transposed_data.columns]\n",
    "transposed_data_40 = transposed_data[columns_to_select_40]\n",
    "\n",
    "# add the storm number to the transposed data\n",
    "transposed_data_20['storm_number'] = storm_numbers\n",
    "transposed_data_30['storm_number'] = storm_numbers\n",
    "transposed_data_40['storm_number'] = storm_numbers\n",
    "\n",
    "'''original_data = transposed_data.copy()\n",
    "original_columns = transposed_data.columns\n",
    "original_data['storm_number'] = original_data['storm_number'].astype(int)'''\n",
    "\n",
    "# Add PCA numbers to each variable to differentiate modes\n",
    "\n",
    "# Count how many times each variable appears in the column names\n",
    "variable_counts_20 = transposed_data_20.columns.value_counts()\n",
    "# Create a mapping with PCA numbers appended to each variable\n",
    "updated_columns_20 = []\n",
    "pca_tracker_20 = {}\n",
    "# for 20 variables\n",
    "for var in transposed_data_20.columns:\n",
    "    if var not in pca_tracker_20:\n",
    "        pca_tracker_20[var] = 1\n",
    "    else:\n",
    "        pca_tracker_20[var] += 1\n",
    "    # Append PCA number to the variable name\n",
    "    updated_columns_20.append(f\"{var}_PCA_{pca_tracker_20[var]}\")\n",
    "# Update the column names\n",
    "transposed_data_20.columns = updated_columns_20\n",
    "# rename the first column to storm_number\n",
    "transposed_data_20 = transposed_data_20.rename(columns={'storm_number_PCA_1': 'storm_number'})\n",
    "transposed_data_20['storm_number'] = transposed_data_20['storm_number'].astype(int)\n",
    "\n",
    "# for 30 variables\n",
    "updated_columns_30 = []\n",
    "pca_tracker_30 = {}\n",
    "for var in transposed_data_30.columns:\n",
    "    if var not in pca_tracker_30:\n",
    "        pca_tracker_30[var] = 1\n",
    "    else:\n",
    "        pca_tracker_30[var] += 1\n",
    "    # Append PCA number to the variable name\n",
    "    updated_columns_30.append(f\"{var}_PCA_{pca_tracker_30[var]}\")\n",
    "# Update the column names\n",
    "transposed_data_30.columns = updated_columns_30\n",
    "# rename the first column to storm_number\n",
    "transposed_data_30 = transposed_data_30.rename(columns={'storm_number_PCA_1': 'storm_number'})\n",
    "transposed_data_30['storm_number'] = transposed_data_30['storm_number'].astype(int)\n",
    "\n",
    "# for 40 variables\n",
    "updated_columns_40 = []\n",
    "pca_tracker_40 = {}\n",
    "for var in transposed_data_40.columns:\n",
    "    if var not in pca_tracker_40:\n",
    "        pca_tracker_40[var] = 1\n",
    "    else:\n",
    "        pca_tracker_40[var] += 1\n",
    "    # Append PCA number to the variable name\n",
    "    updated_columns_40.append(f\"{var}_PCA_{pca_tracker_40[var]}\")\n",
    "# Update the column names\n",
    "transposed_data_40.columns = updated_columns_40\n",
    "# rename the first column to storm_number\n",
    "transposed_data_40 = transposed_data_40.rename(columns={'storm_number_PCA_1': 'storm_number'})\n",
    "transposed_data_40['storm_number'] = transposed_data_40['storm_number'].astype(int)\n",
    "\n",
    "# load the actual y values\n",
    "\n",
    "y_all_cdf = pd.read_csv('data/climatology_dm_winter_per_cluster/GEV_CDF_max/log_cdf_max_combined.csv')\n",
    "y_all_max = pd.read_csv('data/climatology_dm_winter_per_cluster/EVENT_max/max_event_combined.csv')\n",
    "\n",
    "# Extract storm indices\n",
    "storm_indices = transposed_data_20['storm_number'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To keep 50 storms in the training set, storms 45 and 87 are removed from the test set.\n",
      "Storm Training: [1, 2, 3, 5, 7, 8, 11, 12, 13, 16, 19, 26, 27, 31, 32, 34, 39, 43, 45, 46, 49, 50, 51, 53, 54, 56, 60, 61, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 85, 87, 89, 90, 95]\n",
      "Storm Test: [6, 29, 38, 48, 66, 86, 93]\n",
      "Storm Valid: [21, 33, 44, 47, 58, 83]\n",
      "y is already a numpy array\n",
      "Selected features: Index(['mean_surface_latent_heat_flux_std_PCA_2',\n",
      "       'mean_surface_latent_heat_flux_std_PCA_3', 'geopotential_500_max_PCA_1',\n",
      "       'surface_pressure_max_PCA_2', 'geopotential_1000_std_PCA_2'],\n",
      "      dtype='object')\n",
      "Target: cdf, Variables: 20\n",
      "RMSE: 1.6723358439700844, MAE: 1.1508057535607916\n",
      "Selected Features: Index(['mean_surface_latent_heat_flux_std_PCA_2',\n",
      "       'mean_surface_latent_heat_flux_std_PCA_3', 'geopotential_500_max_PCA_1',\n",
      "       'surface_pressure_max_PCA_2', 'geopotential_1000_std_PCA_2'],\n",
      "      dtype='object')\n",
      "------------------------------\n",
      "y is already a numpy array\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 68\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     62\u001b[0m param_grid_xgb \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m500\u001b[39m],\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m40\u001b[39m],\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     66\u001b[0m }\n\u001b[0;32m---> 68\u001b[0m results \u001b[38;5;241m=\u001b[39m sensitivity_test\u001b[38;5;241m.\u001b[39mprocess_xgboost_workflow(\n\u001b[1;32m     69\u001b[0m     X_train_pca\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;241m20\u001b[39m: X_train_pca_20,\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;241m30\u001b[39m: X_train_pca_30,\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;241m40\u001b[39m: X_train_pca_40,\n\u001b[1;32m     73\u001b[0m     },\n\u001b[1;32m     74\u001b[0m     X_validation_pca\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;241m20\u001b[39m: X_validation_pca_20,\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;241m30\u001b[39m: X_validation_pca_30,\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;241m40\u001b[39m: X_validation_pca_40,\n\u001b[1;32m     78\u001b[0m     },\n\u001b[1;32m     79\u001b[0m     y_train\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcdf\u001b[39m\u001b[38;5;124m'\u001b[39m: y_train_cdf,\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m: y_train_max,\n\u001b[1;32m     82\u001b[0m     },\n\u001b[1;32m     83\u001b[0m     y_validation\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcdf\u001b[39m\u001b[38;5;124m'\u001b[39m: y_validation_cdf,\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m: y_validation_max,\n\u001b[1;32m     86\u001b[0m     },\n\u001b[1;32m     87\u001b[0m     variable_counts\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m40\u001b[39m],\n\u001b[1;32m     88\u001b[0m     target_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcdf\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;66;03m# 'max' is out of scope for now\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid_xgb\n\u001b[1;32m     90\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/master-project-cleaned/util/ml/sensitivity_test.py:45\u001b[0m, in \u001b[0;36mprocess_xgboost_workflow\u001b[0;34m(X_train_pca, X_validation_pca, y_train, y_validation, variable_counts, target_types, param_grid)\u001b[0m\n\u001b[1;32m     42\u001b[0m results[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m search\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Feature Selection\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m selected_vars \u001b[38;5;241m=\u001b[39m selection_vars\u001b[38;5;241m.\u001b[39mfeature_selection(\n\u001b[1;32m     46\u001b[0m     X_train_pca[var_count],\n\u001b[1;32m     47\u001b[0m     X_train_np,\n\u001b[1;32m     48\u001b[0m     y_train[target_type],\n\u001b[1;32m     49\u001b[0m     model\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m results[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselected_features\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m selected_vars\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Log results\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/master-project-cleaned/util/feature_selection/selection_vars.py:85\u001b[0m, in \u001b[0;36mfeature_selection\u001b[0;34m(df_X_all_vars, scaled_X, df_y, model)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my is already a numpy array\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m sfs\u001b[38;5;241m.\u001b[39mfit(scaled_X, df_y)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Get the selected feature indices\u001b[39;00m\n\u001b[1;32m     88\u001b[0m selected_indices \u001b[38;5;241m=\u001b[39m sfs\u001b[38;5;241m.\u001b[39mget_support(indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/feature_selection/_sequential.py:251\u001b[0m, in \u001b[0;36mSequentialFeatureSelector.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    249\u001b[0m is_auto_select \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_to_select \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iterations):\n\u001b[0;32m--> 251\u001b[0m     new_feature_idx, new_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_best_new_feature_score(\n\u001b[1;32m    252\u001b[0m         cloned_estimator, X, y, cv, current_mask\n\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_auto_select \u001b[38;5;129;01mand\u001b[39;00m ((new_score \u001b[38;5;241m-\u001b[39m old_score) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol):\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/feature_selection/_sequential.py:282\u001b[0m, in \u001b[0;36mSequentialFeatureSelector._get_best_new_feature_score\u001b[0;34m(self, estimator, X, y, cv, current_mask)\u001b[0m\n\u001b[1;32m    280\u001b[0m         candidate_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mcandidate_mask\n\u001b[1;32m    281\u001b[0m     X_new \u001b[38;5;241m=\u001b[39m X[:, candidate_mask]\n\u001b[0;32m--> 282\u001b[0m     scores[feature_idx] \u001b[38;5;241m=\u001b[39m cross_val_score(\n\u001b[1;32m    283\u001b[0m         estimator,\n\u001b[1;32m    284\u001b[0m         X_new,\n\u001b[1;32m    285\u001b[0m         y,\n\u001b[1;32m    286\u001b[0m         cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[1;32m    287\u001b[0m         scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring,\n\u001b[1;32m    288\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    289\u001b[0m     )\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    290\u001b[0m new_feature_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(scores, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m feature_idx: scores[feature_idx])\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_feature_idx, scores[new_feature_idx]\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:719\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    717\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 719\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[1;32m    720\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    721\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    722\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    723\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[1;32m    724\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[1;32m    725\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[1;32m    726\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    727\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    728\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m    729\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    730\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[1;32m    731\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[1;32m    732\u001b[0m )\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:430\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 430\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    431\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    432\u001b[0m         clone(estimator),\n\u001b[1;32m    433\u001b[0m         X,\n\u001b[1;32m    434\u001b[0m         y,\n\u001b[1;32m    435\u001b[0m         scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[1;32m    436\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    437\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    438\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    439\u001b[0m         parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    440\u001b[0m         fit_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit,\n\u001b[1;32m    441\u001b[0m         score_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mscorer\u001b[38;5;241m.\u001b[39mscore,\n\u001b[1;32m    442\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[1;32m    443\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    444\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[1;32m    445\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[1;32m    446\u001b[0m     )\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[1;32m    448\u001b[0m )\n\u001b[1;32m    450\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seeds = [42, 1996, 45319, 43709]\n",
    "\n",
    "for seed in seeds:\n",
    "    # separate the data in training and testing\n",
    "    storm_index_training, storm_index_test, storm_index_validation = extraction_squares.split_storm_numbers(storm_indices, 0.12, seed, 'number')\n",
    "\n",
    "    # order the index of the storms\n",
    "\n",
    "    storm_index_training.sort()\n",
    "    storm_index_test.sort()\n",
    "    storm_index_validation.sort()\n",
    "\n",
    "    # add +1 to the storm index to match the storm index in the storm_dates dataframe (it's actually storm index for this set, so +1 is needed)\n",
    "    #storm_index_training = [x+1 for x in storm_index_training]\n",
    "    #storm_index_test = [x+1 for x in storm_index_test]\n",
    "    #storm_index_validation = [x+1 for x in storm_index_validation]\n",
    "\n",
    "    print(\"Storm Training:\", storm_index_training)\n",
    "    print(\"Storm Test:\", storm_index_test)\n",
    "    print(\"Storm Valid:\", storm_index_validation) \n",
    "\n",
    "    # remove the variable convective_rain_rate and vertical_velocity\n",
    "    #columns_to_drop = transposed_data.columns[transposed_data.columns.str.startswith(('convective_rain_rate', 'vertical_velocity'))]\n",
    "    #transposed_data = transposed_data.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Update the column names\n",
    "    #updated_columns = transposed_data.columns\n",
    "\n",
    "    X_train_pca_20 = selection_vars.prepare_training_data(transposed_data_20, storm_index_training, updated_columns_20)\n",
    "    X_test_pca_20 = selection_vars.prepare_training_data(transposed_data_20, storm_index_test, updated_columns_20)\n",
    "    X_validation_pca_20 = selection_vars.prepare_training_data(transposed_data_20, storm_index_validation, updated_columns_20)\n",
    "\n",
    "    X_train_pca_30 = selection_vars.prepare_training_data(transposed_data_30, storm_index_training, updated_columns_30)\n",
    "    X_test_pca_30 = selection_vars.prepare_training_data(transposed_data_30, storm_index_test, updated_columns_30)\n",
    "    X_validation_pca_30 = selection_vars.prepare_training_data(transposed_data_30, storm_index_validation, updated_columns_30)\n",
    "\n",
    "    X_train_pca_40 = selection_vars.prepare_training_data(transposed_data_40, storm_index_training, updated_columns_40)\n",
    "    X_test_pca_40 = selection_vars.prepare_training_data(transposed_data_40, storm_index_test, updated_columns_40)\n",
    "    X_validation_pca_40 = selection_vars.prepare_training_data(transposed_data_40, storm_index_validation, updated_columns_40)\n",
    "\n",
    "    # without the PCA in the names :\n",
    "\n",
    "    '''X_train_original = prepare_training_data(original_data, storm_index_training, original_columns)\n",
    "    X_validation_original = prepare_training_data(original_data, storm_index_validation, original_columns)\n",
    "    X_test_original = prepare_training_data(original_data, storm_index_test, original_columns)\n",
    "\n",
    "    X_train_original = X_train_original[columns_to_select]\n",
    "    X_validation_original = X_validation_original[columns_to_select]\n",
    "    X_test_original = X_test_original[columns_to_select]'''\n",
    "\n",
    "    # load the actual y values\n",
    "\n",
    "    y_train_cdf = selection_vars.process_y_data(y_all_cdf, storm_index_training)\n",
    "    y_test_cdf = selection_vars.process_y_data(y_all_cdf, storm_index_test)\n",
    "    y_validation_cdf = selection_vars.process_y_data(y_all_cdf, storm_index_validation)\n",
    "\n",
    "    y_train_max = selection_vars.process_y_data(y_all_max, storm_index_training)\n",
    "    y_test_max = selection_vars.process_y_data(y_all_max, storm_index_test)\n",
    "    y_validation_max = selection_vars.process_y_data(y_all_max, storm_index_validation)\n",
    "\n",
    "        # Example usage\n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': [50, 100, 200, 500],\n",
    "        'max_depth': [3, 5, 10, 20, 40],\n",
    "        'learning_rate': np.linspace(0.05, 0.2, 4)\n",
    "    }\n",
    "\n",
    "    results = sensitivity_test.process_xgboost_workflow(\n",
    "        X_train_pca={\n",
    "            20: X_train_pca_20,\n",
    "            30: X_train_pca_30,\n",
    "            40: X_train_pca_40,\n",
    "        },\n",
    "        X_validation_pca={\n",
    "            20: X_validation_pca_20,\n",
    "            30: X_validation_pca_30,\n",
    "            40: X_validation_pca_40,\n",
    "        },\n",
    "        y_train={\n",
    "            'cdf': y_train_cdf,\n",
    "            'max': y_train_max,\n",
    "        },\n",
    "        y_validation={\n",
    "            'cdf': y_validation_cdf,\n",
    "            'max': y_validation_max,\n",
    "        },\n",
    "        variable_counts=[20, 30, 40],\n",
    "        target_types=['cdf'], # 'max' is out of scope for now\n",
    "        param_grid=param_grid_xgb\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 2\n",
      "n_required_iterations: 4\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 10\n",
      "max_resources_: 50\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 80\n",
      "n_resources: 10\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 27\n",
      "n_resources: 30\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "y is already a numpy array\n",
      "Selected features: Index(['mean_sea_level_pressure_min_PCA_1', 'surface_pressure_mean_PCA_2',\n",
      "       'geopotential_500_max_PCA_1', 'mean_sea_level_pressure_max_PCA_2',\n",
      "       'convective_precipitation_mean_PCA_1'],\n",
      "      dtype='object')\n",
      "Target: cdf, Variables: 20\n",
      "RMSE: 2.013381796243133, MAE: 1.3409397456685754\n",
      "Selected Features: Index(['mean_sea_level_pressure_min_PCA_1', 'surface_pressure_mean_PCA_2',\n",
      "       'geopotential_500_max_PCA_1', 'mean_sea_level_pressure_max_PCA_2',\n",
      "       'convective_precipitation_mean_PCA_1'],\n",
      "      dtype='object')\n",
      "------------------------------\n",
      "n_iterations: 2\n",
      "n_required_iterations: 4\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 10\n",
      "max_resources_: 50\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 80\n",
      "n_resources: 10\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 27\n",
      "n_resources: 30\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "y is already a numpy array\n",
      "Selected features: Index(['mean_sea_level_pressure_min_PCA_1', 'surface_pressure_mean_PCA_2',\n",
      "       'mean_sea_level_pressure_max_PCA_2',\n",
      "       'convective_precipitation_mean_PCA_1', 'surface_pressure_min_PCA_2'],\n",
      "      dtype='object')\n",
      "Target: cdf, Variables: 30\n",
      "RMSE: 1.6907028504017425, MAE: 1.104081940301722\n",
      "Selected Features: Index(['mean_sea_level_pressure_min_PCA_1', 'surface_pressure_mean_PCA_2',\n",
      "       'mean_sea_level_pressure_max_PCA_2',\n",
      "       'convective_precipitation_mean_PCA_1', 'surface_pressure_min_PCA_2'],\n",
      "      dtype='object')\n",
      "------------------------------\n",
      "n_iterations: 2\n",
      "n_required_iterations: 4\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 10\n",
      "max_resources_: 50\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 80\n",
      "n_resources: 10\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 27\n",
      "n_resources: 30\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "y is already a numpy array\n",
      "Selected features: Index(['mean_sea_level_pressure_min_PCA_1',\n",
      "       'surface_latent_heat_flux_min_PCA_3', 'surface_pressure_std_PCA_4',\n",
      "       'large_scale_snowfall_max_PCA_1', 'large_scale_snowfall_max_PCA_3'],\n",
      "      dtype='object')\n",
      "Target: cdf, Variables: 40\n",
      "RMSE: 1.6658822646928704, MAE: 1.147089796745763\n",
      "Selected Features: Index(['mean_sea_level_pressure_min_PCA_1',\n",
      "       'surface_latent_heat_flux_min_PCA_3', 'surface_pressure_std_PCA_4',\n",
      "       'large_scale_snowfall_max_PCA_1', 'large_scale_snowfall_max_PCA_3'],\n",
      "      dtype='object')\n",
      "------------------------------\n",
      "n_iterations: 2\n",
      "n_required_iterations: 4\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 10\n",
      "max_resources_: 50\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 80\n",
      "n_resources: 10\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 27\n",
      "n_resources: 30\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "y is already a numpy array\n",
      "Selected features: Index(['10m_u_component_of_wind_std_PCA_2',\n",
      "       'mean_sea_level_pressure_mean_PCA_1', 'surface_pressure_mean_PCA_1',\n",
      "       'convective_precipitation_mean_PCA_1',\n",
      "       'convective_precipitation_mean_PCA_2'],\n",
      "      dtype='object')\n",
      "Target: max, Variables: 20\n",
      "RMSE: 7.296714260105329, MAE: 5.584415413861757\n",
      "Selected Features: Index(['10m_u_component_of_wind_std_PCA_2',\n",
      "       'mean_sea_level_pressure_mean_PCA_1', 'surface_pressure_mean_PCA_1',\n",
      "       'convective_precipitation_mean_PCA_1',\n",
      "       'convective_precipitation_mean_PCA_2'],\n",
      "      dtype='object')\n",
      "------------------------------\n",
      "n_iterations: 2\n",
      "n_required_iterations: 4\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 10\n",
      "max_resources_: 50\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 80\n",
      "n_resources: 10\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 27\n",
      "n_resources: 30\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "y is already a numpy array\n",
      "Selected features: Index(['mean_sea_level_pressure_mean_PCA_1',\n",
      "       '10m_v_component_of_wind_max_PCA_2',\n",
      "       'convective_precipitation_mean_PCA_2', '2m_temperature_std_PCA_2',\n",
      "       '10m_v_component_of_wind_min_PCA_1'],\n",
      "      dtype='object')\n",
      "Target: max, Variables: 30\n",
      "RMSE: 6.759196097478498, MAE: 5.2129290071809775\n",
      "Selected Features: Index(['mean_sea_level_pressure_mean_PCA_1',\n",
      "       '10m_v_component_of_wind_max_PCA_2',\n",
      "       'convective_precipitation_mean_PCA_2', '2m_temperature_std_PCA_2',\n",
      "       '10m_v_component_of_wind_min_PCA_1'],\n",
      "      dtype='object')\n",
      "------------------------------\n",
      "n_iterations: 2\n",
      "n_required_iterations: 4\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 10\n",
      "max_resources_: 50\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 80\n",
      "n_resources: 10\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 27\n",
      "n_resources: 30\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "y is already a numpy array\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m      2\u001b[0m param_grid_xgb \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m500\u001b[39m],\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m40\u001b[39m],\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      6\u001b[0m }\n\u001b[0;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m sensitivity_test\u001b[38;5;241m.\u001b[39mprocess_xgboost_workflow(\n\u001b[1;32m      9\u001b[0m     X_train_pca\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;241m20\u001b[39m: X_train_pca_20,\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;241m30\u001b[39m: X_train_pca_30,\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;241m40\u001b[39m: X_train_pca_40,\n\u001b[1;32m     13\u001b[0m     },\n\u001b[1;32m     14\u001b[0m     X_validation_pca\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;241m20\u001b[39m: X_validation_pca_20,\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;241m30\u001b[39m: X_validation_pca_30,\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;241m40\u001b[39m: X_validation_pca_40,\n\u001b[1;32m     18\u001b[0m     },\n\u001b[1;32m     19\u001b[0m     y_train\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcdf\u001b[39m\u001b[38;5;124m'\u001b[39m: y_train_cdf,\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m: y_train_max,\n\u001b[1;32m     22\u001b[0m     },\n\u001b[1;32m     23\u001b[0m     y_validation\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcdf\u001b[39m\u001b[38;5;124m'\u001b[39m: y_validation_cdf,\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m: y_validation_max,\n\u001b[1;32m     26\u001b[0m     },\n\u001b[1;32m     27\u001b[0m     variable_counts\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m40\u001b[39m],\n\u001b[1;32m     28\u001b[0m     target_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcdf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     29\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid_xgb\n\u001b[1;32m     30\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/master-project-cleaned/util/ml/sensitivity_test.py:45\u001b[0m, in \u001b[0;36mprocess_xgboost_workflow\u001b[0;34m(X_train_pca, X_validation_pca, y_train, y_validation, variable_counts, target_types, param_grid)\u001b[0m\n\u001b[1;32m     42\u001b[0m results[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m search\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Feature Selection\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m selected_vars \u001b[38;5;241m=\u001b[39m selection_vars\u001b[38;5;241m.\u001b[39mfeature_selection(\n\u001b[1;32m     46\u001b[0m     X_train_pca[var_count],\n\u001b[1;32m     47\u001b[0m     X_train_np,\n\u001b[1;32m     48\u001b[0m     y_train[target_type],\n\u001b[1;32m     49\u001b[0m     model\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m results[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselected_features\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m selected_vars\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Log results\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/master-project-cleaned/util/feature_selection/selection_vars.py:85\u001b[0m, in \u001b[0;36mfeature_selection\u001b[0;34m(df_X_all_vars, scaled_X, df_y, model)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my is already a numpy array\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m sfs\u001b[38;5;241m.\u001b[39mfit(scaled_X, df_y)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Get the selected feature indices\u001b[39;00m\n\u001b[1;32m     88\u001b[0m selected_indices \u001b[38;5;241m=\u001b[39m sfs\u001b[38;5;241m.\u001b[39mget_support(indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/feature_selection/_sequential.py:251\u001b[0m, in \u001b[0;36mSequentialFeatureSelector.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    249\u001b[0m is_auto_select \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_to_select \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iterations):\n\u001b[0;32m--> 251\u001b[0m     new_feature_idx, new_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_best_new_feature_score(\n\u001b[1;32m    252\u001b[0m         cloned_estimator, X, y, cv, current_mask\n\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_auto_select \u001b[38;5;129;01mand\u001b[39;00m ((new_score \u001b[38;5;241m-\u001b[39m old_score) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol):\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/feature_selection/_sequential.py:282\u001b[0m, in \u001b[0;36mSequentialFeatureSelector._get_best_new_feature_score\u001b[0;34m(self, estimator, X, y, cv, current_mask)\u001b[0m\n\u001b[1;32m    280\u001b[0m         candidate_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mcandidate_mask\n\u001b[1;32m    281\u001b[0m     X_new \u001b[38;5;241m=\u001b[39m X[:, candidate_mask]\n\u001b[0;32m--> 282\u001b[0m     scores[feature_idx] \u001b[38;5;241m=\u001b[39m cross_val_score(\n\u001b[1;32m    283\u001b[0m         estimator,\n\u001b[1;32m    284\u001b[0m         X_new,\n\u001b[1;32m    285\u001b[0m         y,\n\u001b[1;32m    286\u001b[0m         cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[1;32m    287\u001b[0m         scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring,\n\u001b[1;32m    288\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    289\u001b[0m     )\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    290\u001b[0m new_feature_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(scores, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m feature_idx: scores[feature_idx])\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_feature_idx, scores[new_feature_idx]\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:719\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    717\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 719\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[1;32m    720\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    721\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    722\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    723\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[1;32m    724\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[1;32m    725\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[1;32m    726\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    727\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    728\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m    729\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    730\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[1;32m    731\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[1;32m    732\u001b[0m )\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:430\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 430\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    431\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    432\u001b[0m         clone(estimator),\n\u001b[1;32m    433\u001b[0m         X,\n\u001b[1;32m    434\u001b[0m         y,\n\u001b[1;32m    435\u001b[0m         scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[1;32m    436\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    437\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    438\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    439\u001b[0m         parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    440\u001b[0m         fit_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit,\n\u001b[1;32m    441\u001b[0m         score_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mscorer\u001b[38;5;241m.\u001b[39mscore,\n\u001b[1;32m    442\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[1;32m    443\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    444\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[1;32m    445\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[1;32m    446\u001b[0m     )\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[1;32m    448\u001b[0m )\n\u001b[1;32m    450\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Default usage\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'max_depth': [3, 5, 10, 20, 40],\n",
    "    'learning_rate': np.linspace(0.05, 0.2, 4)\n",
    "}\n",
    "\n",
    "results = sensitivity_test.process_xgboost_workflow(\n",
    "    X_train_pca={\n",
    "        20: X_train_pca_20,\n",
    "        30: X_train_pca_30,\n",
    "        40: X_train_pca_40,\n",
    "    },\n",
    "    X_validation_pca={\n",
    "        20: X_validation_pca_20,\n",
    "        30: X_validation_pca_30,\n",
    "        40: X_validation_pca_40,\n",
    "    },\n",
    "    y_train={\n",
    "        'cdf': y_train_cdf,\n",
    "        'max': y_train_max,\n",
    "    },\n",
    "    y_validation={\n",
    "        'cdf': y_validation_cdf,\n",
    "        'max': y_validation_max,\n",
    "    },\n",
    "    variable_counts=[20, 30, 40],\n",
    "    target_types=['cdf', 'max'],\n",
    "    param_grid=param_grid_xgb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def sensitivity_analysis(seeds, split_function, process_workflow, data_dict):\n",
    "    sensitivity_results = defaultdict(list)\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\nProcessing for seed: {seed}\")\n",
    "        \n",
    "        # Split the data\n",
    "        storm_index_training, storm_index_test, storm_index_validation = split_function(\n",
    "            data_dict['storm_indices'], 0.12, seed, 'number'\n",
    "        )\n",
    "        storm_index_training.sort()\n",
    "        storm_index_test.sort()\n",
    "        storm_index_validation.sort()\n",
    "        \n",
    "        print(\"Storm Training:\", storm_index_training)\n",
    "        print(\"Storm Test:\", storm_index_test)\n",
    "        print(\"Storm Validation:\", storm_index_validation)\n",
    "        \n",
    "        # Prepare PCA datasets\n",
    "        X_train_pca = {count: selection_vars.prepare_training_data(\n",
    "            data_dict[f'transposed_data_{count}'], storm_index_training, data_dict[f'updated_columns_{count}']\n",
    "        ) for count in [20, 30, 40]}\n",
    "        \n",
    "        X_validation_pca = {count: selection_vars.prepare_training_data(\n",
    "            data_dict[f'transposed_data_{count}'], storm_index_validation, data_dict[f'updated_columns_{count}']\n",
    "        ) for count in [20, 30, 40]}\n",
    "\n",
    "        # Prepare y data\n",
    "        y_train = {\n",
    "            'cdf': data_dict['selection_vars'].process_y_data(data_dict['y_all_cdf'], storm_index_training),\n",
    "            'max': data_dict['selection_vars'].process_y_data(data_dict['y_all_max'], storm_index_training),\n",
    "        }\n",
    "        y_validation = {\n",
    "            'cdf': data_dict['selection_vars'].process_y_data(data_dict['y_all_cdf'], storm_index_validation),\n",
    "            'max': data_dict['selection_vars'].process_y_data(data_dict['y_all_max'], storm_index_validation),\n",
    "        }\n",
    "\n",
    "        # Process the workflow for this seed\n",
    "        results = process_workflow(\n",
    "            X_train_pca=X_train_pca,\n",
    "            X_validation_pca=X_validation_pca,\n",
    "            y_train=y_train,\n",
    "            y_validation=y_validation,\n",
    "            variable_counts=[20, 30, 40],\n",
    "            target_types=['cdf', 'max'],  # or ['cdf', 'max']\n",
    "            param_grid=data_dict['param_grid']\n",
    "        )\n",
    "\n",
    "        # Collect selected features for each variable count and target type\n",
    "        for key, res in results.items():\n",
    "            sensitivity_results[key].append(set(res['selected_features']))\n",
    "\n",
    "    return sensitivity_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing for seed: 42\n",
      "To keep 50 storms in the training set, storms 45 and 87 are removed from the test set.\n",
      "Storm Training: [1, 2, 3, 5, 7, 8, 11, 12, 13, 16, 19, 26, 27, 31, 32, 34, 39, 43, 45, 46, 49, 50, 51, 53, 54, 56, 60, 61, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 85, 87, 89, 90, 95]\n",
      "Storm Test: [6, 29, 38, 48, 66, 86, 93]\n",
      "Storm Validation: [21, 33, 44, 47, 58, 83]\n",
      "y is already a numpy array\n",
      "Target: cdf, Variables: 20\n",
      "RMSE Before Tuning: 1.6723358439700844, MAE Before Tuning: 1.1508057535607916\n",
      "RMSE After Tuning: 1.5142854430073445, MAE After Tuning: 1.090793386336624\n",
      "R2 Before Tuning: -5.938676235281977, R2 After Tuning: -4.169612219524453\n",
      "Relative Variance on validation set: 0.7341266511692504\n",
      "Relative Variance on training set: 1.2214691375806768\n",
      "Best Params: {'learning_rate': 0.2, 'max_depth': 2, 'n_estimators': 100}\n",
      "Selected Features: Index(['10m_u_component_of_wind_std_PCA_1',\n",
      "       'mean_surface_latent_heat_flux_std_PCA_3',\n",
      "       '10m_v_component_of_wind_max_PCA_2', 'geopotential_500_max_PCA_1',\n",
      "       'geopotential_1000_std_PCA_2'],\n",
      "      dtype='object')\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m\n\u001b[1;32m     11\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstorm_indices\u001b[39m\u001b[38;5;124m'\u001b[39m: storm_indices,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransposed_data_20\u001b[39m\u001b[38;5;124m'\u001b[39m: transposed_data_20,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     }\n\u001b[1;32m     27\u001b[0m }\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Run sensitivity analysis\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m sensitivity_results \u001b[38;5;241m=\u001b[39m sensitivity_analysis(\n\u001b[1;32m     31\u001b[0m     seeds\u001b[38;5;241m=\u001b[39mseeds,\n\u001b[1;32m     32\u001b[0m     split_function\u001b[38;5;241m=\u001b[39mextraction_squares\u001b[38;5;241m.\u001b[39msplit_storm_numbers,\n\u001b[1;32m     33\u001b[0m     process_workflow\u001b[38;5;241m=\u001b[39msensitivity_test\u001b[38;5;241m.\u001b[39mprocess_xgboost_workflow,  \u001b[38;5;66;03m# Assuming this is the earlier provided function\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     data_dict\u001b[38;5;241m=\u001b[39mdata_dict\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Analyze the sensitivity\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, selected_features \u001b[38;5;129;01min\u001b[39;00m sensitivity_results\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m, in \u001b[0;36msensitivity_analysis\u001b[0;34m(seeds, split_function, process_workflow, data_dict)\u001b[0m\n\u001b[1;32m     35\u001b[0m y_validation \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcdf\u001b[39m\u001b[38;5;124m'\u001b[39m: data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselection_vars\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mprocess_y_data(data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_all_cdf\u001b[39m\u001b[38;5;124m'\u001b[39m], storm_index_validation),\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m: data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselection_vars\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mprocess_y_data(data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_all_max\u001b[39m\u001b[38;5;124m'\u001b[39m], storm_index_validation),\n\u001b[1;32m     38\u001b[0m }\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Process the workflow for this seed\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m results \u001b[38;5;241m=\u001b[39m process_workflow(\n\u001b[1;32m     42\u001b[0m     X_train_pca\u001b[38;5;241m=\u001b[39mX_train_pca,\n\u001b[1;32m     43\u001b[0m     X_validation_pca\u001b[38;5;241m=\u001b[39mX_validation_pca,\n\u001b[1;32m     44\u001b[0m     y_train\u001b[38;5;241m=\u001b[39my_train,\n\u001b[1;32m     45\u001b[0m     y_validation\u001b[38;5;241m=\u001b[39my_validation,\n\u001b[1;32m     46\u001b[0m     variable_counts\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m40\u001b[39m],\n\u001b[1;32m     47\u001b[0m     target_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcdf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# or ['cdf', 'max']\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mdata_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparam_grid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Collect selected features for each variable count and target type\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, res \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Documents/GitHub/master-project-cleaned/util/ml/sensitivity_test.py:97\u001b[0m, in \u001b[0;36mprocess_xgboost_workflow\u001b[0;34m(X_train_pca, X_validation_pca, y_train, y_validation, variable_counts, target_types, param_grid, print_info)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Hyperparameter tuning\u001b[39;00m\n\u001b[1;32m     96\u001b[0m search \u001b[38;5;241m=\u001b[39m HalvingGridSearchCV(model, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m search\u001b[38;5;241m.\u001b[39mfit(X_train_np, y_train[target_type])\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Get best model after tuning\u001b[39;00m\n\u001b[1;32m    100\u001b[0m best_model \u001b[38;5;241m=\u001b[39m search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/model_selection/_search_successive_halving.py:251\u001b[0m, in \u001b[0;36mBaseSuccessiveHalving.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_input_parameters(\n\u001b[1;32m    246\u001b[0m     X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, split_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit\n\u001b[1;32m    247\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_samples_orig \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y\u001b[38;5;241m=\u001b[39my, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# Set best_score_: BaseSearchCV does not set it, as refit is a callable\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_score_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv_results_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_test_score\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_index_]\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/model_selection/_search_successive_halving.py:355\u001b[0m, in \u001b[0;36mBaseSuccessiveHalving._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m    348\u001b[0m     cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checked_cv_orig\n\u001b[1;32m    350\u001b[0m more_results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miter\u001b[39m\u001b[38;5;124m\"\u001b[39m: [itr] \u001b[38;5;241m*\u001b[39m n_candidates,\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_resources\u001b[39m\u001b[38;5;124m\"\u001b[39m: [n_resources] \u001b[38;5;241m*\u001b[39m n_candidates,\n\u001b[1;32m    353\u001b[0m }\n\u001b[0;32m--> 355\u001b[0m results \u001b[38;5;241m=\u001b[39m evaluate_candidates(\n\u001b[1;32m    356\u001b[0m     candidate_params, cv, more_results\u001b[38;5;241m=\u001b[39mmore_results\n\u001b[1;32m    357\u001b[0m )\n\u001b[1;32m    359\u001b[0m n_candidates_to_keep \u001b[38;5;241m=\u001b[39m ceil(n_candidates \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfactor)\n\u001b[1;32m    360\u001b[0m candidate_params \u001b[38;5;241m=\u001b[39m _top_k(results, n_candidates_to_keep, itr)\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    917\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    918\u001b[0m         clone(base_estimator),\n\u001b[1;32m    919\u001b[0m         X,\n\u001b[1;32m    920\u001b[0m         y,\n\u001b[1;32m    921\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    922\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    923\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    924\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    925\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    927\u001b[0m     )\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    929\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[1;32m    930\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[1;32m    931\u001b[0m     )\n\u001b[1;32m    932\u001b[0m )\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.11/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TAKES 71 MINUTES TO RUN (with 10 seeds) otherwise 24 minutes with 4 seeds\n",
    "\n",
    "# Define seeds\n",
    "seeds = [42, 1996, 45319, 43709, 19961106, 28012025, 15012025, 2019, 111194, 19052024]\n",
    "\n",
    "# or generate random seeds\n",
    "#seeds = np.random.randint(0, 100000, 10).tolist()\n",
    "print_info = 'yes'\n",
    "\n",
    "# Define data and required functions in a dictionary for modularity\n",
    "data_dict = {\n",
    "    'storm_indices': storm_indices,\n",
    "    'transposed_data_20': transposed_data_20,\n",
    "    'transposed_data_30': transposed_data_30,\n",
    "    'transposed_data_40': transposed_data_40,\n",
    "    'updated_columns_20': updated_columns_20,\n",
    "    'updated_columns_30': updated_columns_30,\n",
    "    'updated_columns_40': updated_columns_40,\n",
    "    'selection_vars': selection_vars,\n",
    "    'y_all_cdf': y_all_cdf,\n",
    "    'y_all_max': y_all_max,\n",
    "    'param_grid': {\n",
    "        'n_estimators': [5, 10, 20, 50, 100, 200, 500],\n",
    "        'max_depth': [1, 2, 3, 5, 10, 20, 40],\n",
    "        'learning_rate': np.linspace(0.05, 0.2, 8)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run sensitivity analysis\n",
    "sensitivity_results = sensitivity_analysis(\n",
    "    seeds=seeds,\n",
    "    split_function=extraction_squares.split_storm_numbers,\n",
    "    process_workflow=sensitivity_test.process_xgboost_workflow,  # Assuming this is the earlier provided function\n",
    "    data_dict=data_dict\n",
    ")\n",
    "\n",
    "# Analyze the sensitivity\n",
    "for key, selected_features in sensitivity_results.items():\n",
    "    union_features = set.union(*selected_features)\n",
    "    intersection_features = set.intersection(*selected_features)\n",
    "    print(f\"\\nTarget and Variables: {key}\")\n",
    "    print(f\"Selected Features Union: {union_features}\")\n",
    "    print(f\"Selected Features Intersection: {intersection_features}\")\n",
    "    print(f\"Variability: {len(union_features) - len(intersection_features)}\")\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data for the DataFrame\n",
    "results_data = []\n",
    "\n",
    "for key, feature_sets in sensitivity_results.items():\n",
    "    target_type, var_count = key.split('_')  # Extract target type and variable count\n",
    "    all_features = [features for features in feature_sets]  # List of feature sets across seeds\n",
    "\n",
    "    # Compute union and intersection\n",
    "    union_features = set.union(*all_features)\n",
    "    intersection_features = set.intersection(*all_features)\n",
    "    variability = len(union_features) - len(intersection_features)\n",
    "    consistency_score = len(intersection_features) / len(union_features) if len(union_features) > 0 else 0\n",
    "\n",
    "    # Append data to results list\n",
    "    results_data.append({\n",
    "        'Target Type': target_type,\n",
    "        'Variable Count': var_count,\n",
    "        'All Features': all_features,\n",
    "        #'Union Features': union_features,\n",
    "        #'Intersection Features': intersection_features,\n",
    "        #'Variability': variability,\n",
    "        #'Consistency Score': consistency_score\n",
    "    })\n",
    "\n",
    "# Create a DataFrame\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "results_cdf = results_df[results_df['Target Type'] == 'cdf']\n",
    "results_max = results_df[results_df['Target Type'] == 'max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v4'\n",
    "\n",
    "# Extract the results for each dataset (20-30-40 vars) variables\n",
    "results_cdf_20 = pd.DataFrame(results_cdf['All Features'][0])\n",
    "results_cdf_30 = pd.DataFrame(results_cdf['All Features'][1])\n",
    "results_cdf_40 = pd.DataFrame(results_cdf['All Features'][2])\n",
    "\n",
    "# collapse into a single list\n",
    "results_cdf_20 = [item for sublist in results_cdf['All Features'][0] for item in sublist]\n",
    "results_cdf_30 = [item for sublist in results_cdf['All Features'][1] for item in sublist]\n",
    "results_cdf_40 = [item for sublist in results_cdf['All Features'][2] for item in sublist]\n",
    "\n",
    "# combine the 3 lists into one\n",
    "results_cdf_all_vars = results_cdf_20 + results_cdf_30 + results_cdf_40\n",
    "\n",
    "# count the number of times each variable appears in the list\n",
    "results_cdf_count = pd.Series(results_cdf_all_vars).value_counts()\n",
    "\n",
    "# repeat the same for the max dataset\n",
    "results_max_20 = pd.DataFrame(results_max['All Features'][3])\n",
    "results_max_30 = pd.DataFrame(results_max['All Features'][4])\n",
    "results_max_40 = pd.DataFrame(results_max['All Features'][5])\n",
    "\n",
    "# collapse into a single list\n",
    "results_max_20 = [item for sublist in results_max['All Features'][3] for item in sublist]\n",
    "results_max_30 = [item for sublist in results_max['All Features'][4] for item in sublist]\n",
    "results_max_40 = [item for sublist in results_max['All Features'][5] for item in sublist]\n",
    "\n",
    "# combine the 3 lists into one\n",
    "results_max_all_vars = results_max_20 + results_max_30 + results_max_40\n",
    "\n",
    "# count the number of times each variable appears in the list\n",
    "results_max_count = pd.Series(results_max_all_vars).value_counts()\n",
    "\n",
    "# create a new list with the 2 preivous results_target_all_vars and count each variable\n",
    "results_target_all_vars = results_cdf_all_vars + results_max_all_vars\n",
    "results_target_count = pd.Series(results_target_all_vars).value_counts()\n",
    "\n",
    "# export the 3 lists to a csv file\n",
    "results_cdf_count.to_csv(f'pre_processing/feature_selection/fs_mls_ts_tests/cdf_count_{version}.csv')\n",
    "results_max_count.to_csv(f'pre_processing/feature_selection/fs_mls_ts_tests/max_count_{version}.csv')\n",
    "results_target_count.to_csv(f'pre_processing/feature_selection/fs_mls_ts_tests/both_target_count_{version}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the top 10 % of the variables\n",
    "\n",
    "percentage = 0.5\n",
    "results_cdf_count_10 = results_cdf_count[results_cdf_count > percentage*len(seeds)]\n",
    "results_max_count_10 = results_max_count[results_max_count > percentage*len(seeds)]\n",
    "results_target_count_10 = results_target_count[results_target_count > percentage*len(seeds)]\n",
    "\n",
    "# export the 3 lists to a csv file\n",
    "\n",
    "results_cdf_count_10.to_csv(f'pre_processing/feature_selection/fs_mls_ts_tests/cdf_count_50_{version}.csv')\n",
    "results_max_count_10.to_csv(f'pre_processing/feature_selection/fs_mls_ts_tests/max_count_50_{version}.csv')\n",
    "results_target_count_10.to_csv(f'pre_processing/feature_selection/fs_mls_ts_tests/both_target_count_50_{version}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
