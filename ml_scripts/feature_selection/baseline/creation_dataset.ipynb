{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from math import sqrt\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "# now you can import normally from model_selection\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "operating_system = 'mac'\n",
    "\n",
    "if operating_system == 'win':\n",
    "    os.chdir('C:/Users/fabau/OneDrive/Documents/GitHub/master-project-cleaned/')\n",
    "elif operating_system == 'curnagl':\n",
    "    os.chdir('/work/FAC/FGSE/IDyST/tbeucler/default/fabien/repos/cleaner_version/')\n",
    "else:\n",
    "    os.chdir('/Users/fabienaugsburger/Documents/GitHub/master-project-cleaned/')\n",
    "\n",
    "# Add the path to the custom library\n",
    "custom_library_path = os.path.abspath('util/processing/')\n",
    "sys.path.append(custom_library_path)\n",
    "custom_library_path = os.path.abspath('util/gev/')\n",
    "sys.path.append(custom_library_path)\n",
    "custom_library_path = os.path.abspath('util/feature_selection/')\n",
    "sys.path.append(custom_library_path)\n",
    "custom_library_path = os.path.abspath('util/ml/')\n",
    "sys.path.append(custom_library_path)\n",
    "\n",
    "import extraction_squares, pre_processing_data, data_processing, selection_vars, sensitivity_test\n",
    "\n",
    "'''if operating_system == 'curnagl':\n",
    "    name_of_variable= pd.read_csv('/work/FAC/FGSE/IDyST/tbeucler/default/fabien/repos/curnagl/DATASETS/variable_list_80_mean.csv')\n",
    "    path_data = '/work/FAC/FGSE/IDyST/tbeucler/default/fabien/repos/curnagl/DATASETS'\n",
    "else:'''\n",
    "name_of_variable_20 = pd.read_csv('ml_scripts/feature_selection/corr_timeseries/corr_inst_max_20.csv')['Unnamed: 0']#('data/variable_list_levels.csv')\n",
    "name_of_variable_30 = pd.read_csv('ml_scripts/feature_selection/corr_timeseries/corr_inst_max_30.csv')['Unnamed: 0']#('data/variable_list_levels.csv')\n",
    "name_of_variable_40 = pd.read_csv('ml_scripts/feature_selection/corr_timeseries/corr_inst_max_40.csv')['Unnamed: 0']#('data/variable_list_levels.csv')\n",
    "\n",
    "path_data = 'data'\n",
    "\n",
    "storm_dates = pd.read_csv('pre_processing/tracks/storm_dates.csv')\n",
    "#path_tracks_1h_non_EU = 'pre_processing/tracks/ALL_TRACKS/tracks_1h_non_EU'\n",
    "#dataset = 'datasets_1h'\n",
    "#dataset_non_EU = 'datasets_1h_non_EU'\n",
    "levels = pd.read_csv('data/levels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/01/5ryz4pnn581dj9gk6r1nn5qr0000gn/T/ipykernel_16677/892942925.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  transposed_data_20['storm_number'] = storm_numbers.astype(int)\n",
      "/var/folders/01/5ryz4pnn581dj9gk6r1nn5qr0000gn/T/ipykernel_16677/892942925.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  transposed_data_30['storm_number'] = storm_numbers.astype(int)\n",
      "/var/folders/01/5ryz4pnn581dj9gk6r1nn5qr0000gn/T/ipykernel_16677/892942925.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  transposed_data_40['storm_number'] = storm_numbers.astype(int)\n"
     ]
    }
   ],
   "source": [
    "# import the all_loadings data\n",
    "all_ts = pd.read_csv('data/time_series_1h_non_EU/all_time_series.csv')\n",
    "all_loadings = pd.read_csv('ml_scripts/nestedMLR/all_loadings_1000.csv')\n",
    "\n",
    "'''# Extract variable names and storm data\n",
    "#variables = all_loadings['variable']  # First column\n",
    "storm_data = all_loadings.iloc[:, 1:]  # All columns from the second onward\n",
    "\n",
    "# Transpose storm data and set variable names as columns\n",
    "transposed_data = storm_data.T\n",
    "transposed_data.columns = variables'''\n",
    "\n",
    "\n",
    "# extract the storm number\n",
    "storm_numbers = all_ts['storm_index'].copy()\n",
    "transposed_data = all_ts\n",
    "transposed_data_loadings = all_loadings\n",
    "\n",
    "# Extract the base names from transposed_data columns by removing \"_step_x\"\n",
    "base_names_in_data = set(col.split('_step_')[0] for col in transposed_data.columns)\n",
    "base_names_in_data_loadings = set(col.split('_step_')[0] for col in transposed_data_loadings.columns)\n",
    "\n",
    "# Filter the columns based on base names from name_of_variable_20\n",
    "columns_to_select_20 = [col for col in transposed_data.columns if col.split('_step_')[0] in name_of_variable_20.tolist()]\n",
    "# Select the corresponding columns from the dataframe\n",
    "transposed_data_20 = transposed_data[columns_to_select_20]\n",
    "# Filter the columns based on base names from name_of_variable_30\n",
    "columns_to_select_30 = [col for col in transposed_data.columns if col.split('_step_')[0] in name_of_variable_30.tolist()]\n",
    "# Select the corresponding columns from the dataframe\n",
    "transposed_data_30 = transposed_data[columns_to_select_30]\n",
    "# Filter the columns based on base names from name_of_variable_40\n",
    "columns_to_select_40 = [col for col in transposed_data.columns if col.split('_step_')[0] in name_of_variable_40.tolist()]\n",
    "# Select the corresponding columns from the dataframe\n",
    "transposed_data_40 = transposed_data[columns_to_select_40]\n",
    "# Select all the data\n",
    "all_columns = transposed_data.columns\n",
    "transposed_data_all = transposed_data[all_columns]\n",
    "\n",
    "# For the loadings\n",
    "all_columns_loadings = transposed_data_loadings.columns\n",
    "transposed_data_loadings_all = transposed_data_loadings[all_columns_loadings]\n",
    "\n",
    "# add the storm number to the transposed data\n",
    "transposed_data_20['storm_number'] = storm_numbers.astype(int)\n",
    "transposed_data_30['storm_number'] = storm_numbers.astype(int)\n",
    "transposed_data_40['storm_number'] = storm_numbers.astype(int)\n",
    "transposed_data_all['storm_number'] = storm_numbers.astype(int)\n",
    "transposed_data_loadings_all['storm_number'] = storm_numbers.astype(int)\n",
    "\n",
    "# set storm number as first column\n",
    "cols_20 = transposed_data_20.columns.tolist()\n",
    "cols_20 = cols_20[-1:] + cols_20[:-1]\n",
    "transposed_data_20 = transposed_data_20[cols_20]\n",
    "#cols_20 = cols_20[1:]\n",
    "\n",
    "cols_30 = transposed_data_30.columns.tolist()\n",
    "cols_30 = cols_30[-1:] + cols_30[:-1]\n",
    "transposed_data_30 = transposed_data_30[cols_30]\n",
    "#cols_30 = cols_30[1:]\n",
    "\n",
    "cols_40 = transposed_data_40.columns.tolist()\n",
    "cols_40 = cols_40[-1:] + cols_40[:-1]\n",
    "transposed_data_40 = transposed_data_40[cols_40]\n",
    "#cols_40 = cols_40[1:]\n",
    "\n",
    "cols_all = transposed_data_all.columns.tolist()\n",
    "cols_all = cols_all[-1:] + cols_all[:-1]\n",
    "transposed_data_all = transposed_data_all[cols_all]\n",
    "\n",
    "cols_loadings_all = transposed_data_loadings_all.columns.tolist()\n",
    "cols_loadings_all = cols_loadings_all[-1:] + cols_loadings_all[:-1]\n",
    "transposed_data_loadings_all = transposed_data_loadings_all[cols_loadings_all]\n",
    "\n",
    "\n",
    "# Add storm number to the original data\n",
    "\n",
    "'''original_data = transposed_data.copy()\n",
    "original_columns = transposed_data.columns\n",
    "original_data['storm_number'] = original_data['storm_number'].astype(int)'''\n",
    "\n",
    "# Add PCA numbers to each variable to differentiate modes\n",
    "'''\n",
    "# Count how many times each variable appears in the column names\n",
    "variable_counts_20 = transposed_data_20.columns.value_counts()\n",
    "# Create a mapping with PCA numbers appended to each variable\n",
    "updated_columns_20 = []\n",
    "pca_tracker_20 = {}\n",
    "# for 20 variables\n",
    "for var in transposed_data_20.columns:\n",
    "    if var not in pca_tracker_20:\n",
    "        pca_tracker_20[var] = 1\n",
    "    else:\n",
    "        pca_tracker_20[var] += 1\n",
    "    # Append PCA number to the variable name\n",
    "    updated_columns_20.append(f\"{var}_PCA_{pca_tracker_20[var]}\")\n",
    "# Update the column names\n",
    "transposed_data_20.columns = updated_columns_20\n",
    "# rename the first column to storm_number\n",
    "transposed_data_20 = transposed_data_20.rename(columns={'storm_number_PCA_1': 'storm_number'})\n",
    "transposed_data_20['storm_number'] = transposed_data_20['storm_number'].astype(int)\n",
    "\n",
    "# for 30 variables\n",
    "updated_columns_30 = []\n",
    "pca_tracker_30 = {}\n",
    "for var in transposed_data_30.columns:\n",
    "    if var not in pca_tracker_30:\n",
    "        pca_tracker_30[var] = 1\n",
    "    else:\n",
    "        pca_tracker_30[var] += 1\n",
    "    # Append PCA number to the variable name\n",
    "    updated_columns_30.append(f\"{var}_PCA_{pca_tracker_30[var]}\")\n",
    "# Update the column names\n",
    "transposed_data_30.columns = updated_columns_30\n",
    "# rename the first column to storm_number\n",
    "transposed_data_30 = transposed_data_30.rename(columns={'storm_number_PCA_1': 'storm_number'})\n",
    "transposed_data_30['storm_number'] = transposed_data_30['storm_number'].astype(int)\n",
    "\n",
    "# for 40 variables\n",
    "updated_columns_40 = []\n",
    "pca_tracker_40 = {}\n",
    "for var in transposed_data_40.columns:\n",
    "    if var not in pca_tracker_40:\n",
    "        pca_tracker_40[var] = 1\n",
    "    else:\n",
    "        pca_tracker_40[var] += 1\n",
    "    # Append PCA number to the variable name\n",
    "    updated_columns_40.append(f\"{var}_PCA_{pca_tracker_40[var]}\")\n",
    "# Update the column names\n",
    "transposed_data_40.columns = updated_columns_40\n",
    "# rename the first column to storm_number\n",
    "transposed_data_40 = transposed_data_40.rename(columns={'storm_number_PCA_1': 'storm_number'})\n",
    "transposed_data_40['storm_number'] = transposed_data_40['storm_number'].astype(int)'''\n",
    "\n",
    "# load the actual y values\n",
    "\n",
    "y_all_cdf = pd.read_csv('data/climatology_dm_winter_per_cluster/GEV_CDF_max/log_cdf_max_combined.csv')\n",
    "y_all_max = pd.read_csv('data/climatology_dm_winter_per_cluster/EVENT_max/max_event_combined.csv')\n",
    "\n",
    "# Extract storm indices\n",
    "storm_indices = transposed_data_20['storm_number'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To keep 50 storms in the training set, storms 45 and 87 are removed from the test set.\n",
      "Storm Training: [1, 2, 3, 5, 7, 8, 11, 12, 13, 16, 19, 26, 27, 31, 32, 34, 39, 43, 45, 46, 49, 50, 51, 53, 54, 56, 60, 61, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 85, 87, 89, 90, 95]\n",
      "Storm Test: [6, 29, 38, 48, 66, 86, 93]\n",
      "Storm Valid: [21, 33, 44, 47, 58, 83]\n",
      "To keep 50 storms in the training set, storms 45 and 87 are removed from the test set.\n",
      "Storm Training: [1, 2, 5, 7, 8, 11, 12, 13, 16, 19, 21, 26, 27, 31, 32, 33, 34, 39, 43, 45, 46, 47, 49, 50, 51, 56, 58, 60, 61, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 77, 78, 79, 81, 82, 83, 85, 87, 89, 90, 95]\n",
      "Storm Test: [6, 29, 38, 48, 66, 86, 93]\n",
      "Storm Valid: [3, 44, 53, 54, 76, 80]\n",
      "To keep 50 storms in the training set, storms 45 and 87 are removed from the test set.\n",
      "Storm Training: [1, 2, 3, 5, 7, 8, 11, 12, 13, 16, 19, 26, 27, 31, 33, 34, 39, 43, 44, 45, 46, 47, 49, 50, 51, 53, 54, 56, 58, 60, 61, 62, 63, 64, 67, 68, 69, 71, 73, 76, 78, 79, 80, 81, 82, 83, 85, 87, 90, 95]\n",
      "Storm Test: [6, 29, 38, 48, 66, 86, 93]\n",
      "Storm Valid: [21, 32, 65, 72, 77, 89]\n",
      "To keep 50 storms in the training set, storms 45 and 87 are removed from the test set.\n",
      "Storm Training: [1, 2, 3, 5, 7, 8, 12, 13, 16, 19, 26, 27, 31, 32, 33, 34, 39, 43, 44, 45, 46, 47, 49, 50, 51, 53, 58, 60, 61, 62, 64, 65, 67, 68, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 85, 87, 89, 90, 95]\n",
      "Storm Test: [6, 29, 38, 48, 66, 86, 93]\n",
      "Storm Valid: [11, 21, 54, 56, 63, 69]\n",
      "To keep 50 storms in the training set, storms 45 and 87 are removed from the test set.\n",
      "Storm Training: [1, 2, 3, 7, 8, 11, 13, 16, 19, 21, 26, 27, 31, 32, 33, 34, 39, 43, 44, 46, 47, 49, 50, 51, 53, 54, 56, 58, 60, 61, 62, 63, 65, 67, 68, 69, 71, 73, 76, 77, 79, 80, 81, 82, 83, 85, 87, 89, 90, 95]\n",
      "Storm Test: [6, 29, 38, 48, 66, 86, 93]\n",
      "Storm Valid: [5, 12, 45, 64, 72, 78]\n",
      "To keep 50 storms in the training set, storms 45 and 87 are removed from the test set.\n",
      "Storm Training: [2, 3, 5, 7, 8, 11, 12, 13, 16, 21, 26, 27, 31, 32, 33, 34, 39, 43, 45, 46, 47, 49, 51, 54, 56, 58, 60, 61, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 85, 87, 89, 90]\n",
      "Storm Test: [6, 29, 38, 48, 66, 86, 93]\n",
      "Storm Valid: [1, 19, 44, 50, 53, 95]\n",
      "To keep 50 storms in the training set, storms 45 and 87 are removed from the test set.\n",
      "Storm Training: [1, 2, 3, 5, 7, 8, 11, 12, 13, 16, 19, 21, 26, 27, 31, 33, 34, 39, 43, 44, 46, 49, 51, 53, 54, 56, 58, 60, 61, 63, 64, 65, 67, 68, 69, 71, 72, 73, 76, 78, 79, 80, 81, 82, 83, 85, 87, 89, 90, 95]\n",
      "Storm Test: [6, 29, 38, 48, 66, 86, 93]\n",
      "Storm Valid: [32, 45, 47, 50, 62, 77]\n",
      "To keep 50 storms in the training set, storms 45 and 87 are removed from the test set.\n",
      "Storm Training: [1, 2, 3, 5, 7, 8, 11, 12, 13, 16, 19, 27, 31, 32, 33, 34, 39, 44, 45, 46, 47, 49, 50, 51, 53, 54, 56, 58, 60, 61, 63, 64, 68, 69, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 83, 85, 87, 89, 90, 95]\n",
      "Storm Test: [6, 29, 38, 48, 66, 86, 93]\n",
      "Storm Valid: [21, 26, 43, 62, 65, 67]\n",
      "To keep 50 storms in the training set, storms 45 and 87 are removed from the test set.\n",
      "Storm Training: [1, 2, 3, 5, 7, 8, 11, 12, 13, 16, 19, 21, 27, 31, 32, 33, 34, 39, 43, 44, 45, 47, 49, 50, 51, 53, 54, 56, 58, 60, 61, 62, 63, 64, 65, 67, 68, 71, 72, 73, 76, 78, 79, 80, 81, 82, 85, 87, 89, 95]\n",
      "Storm Test: [6, 29, 38, 48, 66, 86, 93]\n",
      "Storm Valid: [26, 46, 69, 77, 83, 90]\n",
      "To keep 50 storms in the training set, storms 45 and 87 are removed from the test set.\n",
      "Storm Training: [1, 2, 5, 7, 8, 11, 12, 13, 16, 19, 21, 26, 27, 32, 33, 34, 39, 43, 44, 45, 46, 47, 50, 51, 53, 54, 56, 58, 60, 61, 62, 63, 64, 67, 69, 71, 72, 76, 77, 78, 79, 80, 81, 82, 83, 85, 87, 89, 90, 95]\n",
      "Storm Test: [6, 29, 38, 48, 66, 86, 93]\n",
      "Storm Valid: [3, 31, 49, 65, 68, 73]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    y_train_cdf = selection_vars.process_y_data(y_all_cdf, storm_index_training)\\n    y_test_cdf = selection_vars.process_y_data(y_all_cdf, storm_index_test)\\n    y_validation_cdf = selection_vars.process_y_data(y_all_cdf, storm_index_validation)\\n\\n    y_train_max = selection_vars.process_y_data(y_all_max, storm_index_training)\\n    y_test_max = selection_vars.process_y_data(y_all_max, storm_index_test)\\n    y_validation_max = selection_vars.process_y_data(y_all_max, storm_index_validation)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seeds = [42, 1996, 45319, 43709, 19961106, 28012025, 15012025, 2019, 111194, 19052024]\n",
    "\n",
    "for seed in seeds:\n",
    "    # separate the data in training and testing\n",
    "    storm_index_training, storm_index_test, storm_index_validation = extraction_squares.split_storm_numbers(storm_indices, 0.12, seed, 'number')\n",
    "\n",
    "    # order the index of the storms\n",
    "\n",
    "    storm_index_training.sort()\n",
    "    storm_index_test.sort()\n",
    "    storm_index_validation.sort()\n",
    "\n",
    "    # add +1 to the storm index to match the storm index in the storm_dates dataframe (it's actually storm index for this set, so +1 is needed)\n",
    "    #storm_index_training = [x+1 for x in storm_index_training]\n",
    "    #storm_index_test = [x+1 for x in storm_index_test]\n",
    "    #storm_index_validation = [x+1 for x in storm_index_validation]\n",
    "\n",
    "    print(\"Storm Training:\", storm_index_training)\n",
    "    print(\"Storm Test:\", storm_index_test)\n",
    "    print(\"Storm Valid:\", storm_index_validation) \n",
    "\n",
    "    # remove the variable convective_rain_rate and vertical_velocity\n",
    "    #columns_to_drop = transposed_data.columns[transposed_data.columns.str.startswith(('convective_rain_rate', 'vertical_velocity'))]\n",
    "    #transposed_data = transposed_data.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Update the column names\n",
    "    #updated_columns = transposed_data.columns\n",
    "\n",
    "    X_train_pca_20 = selection_vars.prepare_training_data(transposed_data_20, storm_index_training, cols_20)\n",
    "    X_test_pca_20 = selection_vars.prepare_training_data(transposed_data_20, storm_index_test, cols_20)\n",
    "    X_validation_pca_20 = selection_vars.prepare_training_data(transposed_data_20, storm_index_validation, cols_20)\n",
    "\n",
    "    X_train_pca_30 = selection_vars.prepare_training_data(transposed_data_30, storm_index_training, cols_30)\n",
    "    X_test_pca_30 = selection_vars.prepare_training_data(transposed_data_30, storm_index_test, cols_30)\n",
    "    X_validation_pca_30 = selection_vars.prepare_training_data(transposed_data_30, storm_index_validation, cols_30)\n",
    "\n",
    "    X_train_pca_40 = selection_vars.prepare_training_data(transposed_data_40, storm_index_training, cols_40)\n",
    "    X_test_pca_40 = selection_vars.prepare_training_data(transposed_data_40, storm_index_test, cols_40)\n",
    "    X_validation_pca_40 = selection_vars.prepare_training_data(transposed_data_40, storm_index_validation, cols_40)\n",
    "\n",
    "    X_train_all = selection_vars.prepare_training_data(transposed_data_all, storm_index_training, cols_all)\n",
    "    X_test_all = selection_vars.prepare_training_data(transposed_data_all, storm_index_test, cols_all)\n",
    "    X_validation_all = selection_vars.prepare_training_data(transposed_data_all, storm_index_validation, cols_all)\n",
    "\n",
    "    X_train_all_loadings = selection_vars.prepare_training_data(transposed_data_loadings_all, storm_index_training, cols_loadings_all)\n",
    "    X_test_all_loadings = selection_vars.prepare_training_data(transposed_data_loadings_all, storm_index_test, cols_loadings_all)\n",
    "    X_validation_all_loadings = selection_vars.prepare_training_data(transposed_data_loadings_all, storm_index_validation, cols_loadings_all)\n",
    "\n",
    "    X_train_pca_20.to_csv(f'ml_scripts/new_feature_selection/seed_{seed}/X_train_ts_20.csv', index=False)\n",
    "    X_test_pca_20.to_csv(f'ml_scripts/new_feature_selection/seed_{seed}/X_test_ts_20.csv', index=False)\n",
    "    X_validation_pca_20.to_csv(f'ml_scripts/new_feature_selection/seed_{seed}/X_validation_ts_20.csv', index=False)\n",
    "\n",
    "    X_train_pca_30.to_csv(f'ml_scripts/new_feature_selection/seed_{seed}/X_train_ts_30.csv', index=False)\n",
    "    X_test_pca_30.to_csv(f'ml_scripts/new_feature_selection/seed_{seed}/X_test_ts_30.csv', index=False)\n",
    "    X_validation_pca_30.to_csv(f'ml_scripts/new_feature_selection/seed_{seed}/X_validation_ts_30.csv', index=False)\n",
    "\n",
    "    X_train_pca_40.to_csv(f'ml_scripts/new_feature_selection/seed_{seed}/X_train_ts_40.csv', index=False)\n",
    "    X_test_pca_40.to_csv(f'ml_scripts/new_feature_selection/seed_{seed}/X_test_ts_40.csv', index=False)\n",
    "    X_validation_pca_40.to_csv(f'ml_scripts/new_feature_selection/seed_{seed}/X_validation_ts_40.csv', index=False)\n",
    "\n",
    "    X_train_all.to_csv(f'ml_scripts/new_feature_selection/seed_{seed}/X_train_ts_all.csv', index=False)\n",
    "    X_test_all.to_csv(f'ml_scripts/new_feature_selection/seed_{seed}/X_test_ts_all.csv', index=False)\n",
    "    X_validation_all.to_csv(f'ml_scripts/new_feature_selection/seed_{seed}/X_validation_ts_all.csv', index=False)\n",
    "\n",
    "    X_train_all_loadings.to_csv(f'ml_scripts/new_feature_selection/seed_{seed}/X_train_all.csv', index=False)\n",
    "    X_test_all_loadings.to_csv(f'ml_scripts/new_feature_selection/seed_{seed}/X_test_all.csv', index=False)\n",
    "    X_validation_all_loadings.to_csv(f'ml_scripts/new_feature_selection/seed_{seed}/X_validation_all.csv', index=False)\n",
    "\n",
    "    # load the actual y values\n",
    "'''\n",
    "    y_train_cdf = selection_vars.process_y_data(y_all_cdf, storm_index_training)\n",
    "    y_test_cdf = selection_vars.process_y_data(y_all_cdf, storm_index_test)\n",
    "    y_validation_cdf = selection_vars.process_y_data(y_all_cdf, storm_index_validation)\n",
    "\n",
    "    y_train_max = selection_vars.process_y_data(y_all_max, storm_index_training)\n",
    "    y_test_max = selection_vars.process_y_data(y_all_max, storm_index_test)\n",
    "    y_validation_max = selection_vars.process_y_data(y_all_max, storm_index_validation)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
