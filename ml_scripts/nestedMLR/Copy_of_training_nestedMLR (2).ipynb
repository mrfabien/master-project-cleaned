{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0moAoFIzCFV"
      },
      "source": [
        "# Code to process winter storm information\n",
        "\n",
        "$\\textbf{Introduction}$: The notebook contains code to process the input and output data for the European winter storm severe winds project. Specifically, we introduce a basic nested PC (Principal Components) Regression model with dropouts, and a Variational Encoder-Decoder (VEDs) to predict the quantile function (inversed CDF) of maximum surface wind gusts\n",
        "\n",
        "The quantile function can be calculated from a given Cumulative Distribution Function (CDF) with the following equation:\n",
        "\n",
        "$ Q = log_{10} (1 - CDF) $\n",
        "\n",
        "The quantile function is particularly useful for our project because it accentuates the difference in distribution tail values. This property is useful in our context of predicting probabilities of rare severe winds in winter storms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcmreaBi3Q5x"
      },
      "source": [
        "## Import relevant packages and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa4qlthVI1tr",
        "outputId": "cbc13a30-9a19-40cf-ecac-946c71861d40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optuna in /Users/fabienaugsburger/anaconda3/envs/tensor_flow/lib/python3.9/site-packages (4.1.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /Users/fabienaugsburger/anaconda3/envs/tensor_flow/lib/python3.9/site-packages (from optuna) (1.14.0)\n",
            "Requirement already satisfied: colorlog in /Users/fabienaugsburger/anaconda3/envs/tensor_flow/lib/python3.9/site-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /Users/fabienaugsburger/anaconda3/envs/tensor_flow/lib/python3.9/site-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/fabienaugsburger/anaconda3/envs/tensor_flow/lib/python3.9/site-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/fabienaugsburger/anaconda3/envs/tensor_flow/lib/python3.9/site-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /Users/fabienaugsburger/anaconda3/envs/tensor_flow/lib/python3.9/site-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /Users/fabienaugsburger/anaconda3/envs/tensor_flow/lib/python3.9/site-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /Users/fabienaugsburger/anaconda3/envs/tensor_flow/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (1.3.8)\n",
            "Requirement already satisfied: typing-extensions>=4 in /Users/fabienaugsburger/anaconda3/envs/tensor_flow/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/fabienaugsburger/anaconda3/envs/tensor_flow/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "! pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AygV5YB0yXjq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/fabienaugsburger/anaconda3/envs/tensor_flow/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import optuna\n",
        "import os\n",
        "import sys\n",
        "\n",
        "operating_system = 'mac'\n",
        "\n",
        "if operating_system == 'win':\n",
        "    os.chdir('C:/Users/fabau/OneDrive/Documents/GitHub/master-project-cleaned/')\n",
        "elif operating_system == 'curnagl':\n",
        "    os.chdir('/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/cleaner_version/')\n",
        "else:\n",
        "    os.chdir('/Users/fabienaugsburger/Documents/GitHub/master-project-cleaned/')\n",
        "\n",
        "util_perso = os.path.abspath('util/processing')\n",
        "sys.path.append(util_perso)\n",
        "util_perso = os.path.abspath('util/feature_selection')\n",
        "sys.path.append(util_perso)\n",
        "\n",
        "from extraction_squares import split_storm_numbers\n",
        "from selection_vars import prepare_training_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5UqGTOD13nMA"
      },
      "outputs": [],
      "source": [
        "# Read the csv files for training time series (inputs)\n",
        "all_loadings = pd.read_csv('pre_processing/nestedMLR/all_loadings.csv', header=0)\n",
        "\n",
        "cdf = 'cdf'\n",
        "selected_var = pd.read_csv(f'pre_processing/feature_selection/mls_ts_tests/{cdf}_count_v5.csv')['Unnamed: 0'].values\n",
        "\n",
        "# Read the csv files for processed quantile function outputs\n",
        "output_quantile = pd.read_csv('pre_processing/nestedMLR/log_cdf_max_dm_combined.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To keep 50 storms in the training set, storms 45 and 87 are removed from the test set.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/01/5ryz4pnn581dj9gk6r1nn5qr0000gn/T/ipykernel_8099/3925337382.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  transposed_data_5['storm_number'] = storm_numbers\n"
          ]
        }
      ],
      "source": [
        "# Extract variable names and storm data\n",
        "variables = all_loadings['variable']  # First column\n",
        "storm_data = all_loadings.iloc[:, 1:]  # All columns from the second onward\n",
        "\n",
        "# Transpose storm data and set variable names as columns\n",
        "transposed_data = storm_data.T\n",
        "transposed_data.columns = variables\n",
        "\n",
        "# Optionally reset index to name storms\n",
        "transposed_data.index.name = 'storm_number'\n",
        "transposed_data.reset_index(inplace=True)\n",
        "\n",
        "# extract the storm number\n",
        "storm_numbers = transposed_data['storm_number'].copy().to_numpy().astype(int)\n",
        "\n",
        "updated_columns = []\n",
        "pca_tracker = {}\n",
        "# for 20 variables\n",
        "for var in transposed_data.columns:\n",
        "    if var not in pca_tracker:\n",
        "        pca_tracker[var] = 1\n",
        "    else:\n",
        "        pca_tracker[var] += 1\n",
        "    # Append PCA number to the variable name\n",
        "    updated_columns.append(f\"{var}_PCA_{pca_tracker[var]}\")\n",
        "# Update the column names\n",
        "transposed_data.columns = updated_columns\n",
        "# rename the first column to storm_number\n",
        "transposed_data = transposed_data.rename(columns={'storm_number_PCA_1': 'storm_number'})\n",
        "transposed_data['storm_number'] = transposed_data['storm_number'].astype(int)\n",
        "\n",
        "# Extract variables most correlated with the target and leaving the storm number\n",
        "columns_to_select_5 = [col for col in selected_var if col in transposed_data.columns]\n",
        "transposed_data_5 = transposed_data[columns_to_select_5]\n",
        "\n",
        "# separate the data in training and testing\n",
        "storm_index_training, storm_index_test, storm_index_validation = split_storm_numbers(storm_numbers, 0.12, 42, 'number')\n",
        "\n",
        "# Sort the storm indices\n",
        "storm_index_training.sort()\n",
        "storm_index_test.sort()\n",
        "storm_index_validation.sort()\n",
        "\n",
        "# add the storm number to the transposed data\n",
        "transposed_data_5['storm_number'] = storm_numbers\n",
        "columns_with_storm = transposed_data_5.columns\n",
        "\n",
        "X_train = prepare_training_data(transposed_data_5, storm_index_training, columns_with_storm)\n",
        "X_test = prepare_training_data(transposed_data_5, storm_index_test, columns_with_storm)\n",
        "X_validation = prepare_training_data(transposed_data_5, storm_index_validation, columns_with_storm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qENqnFDQ54tY"
      },
      "source": [
        "## Check data\n",
        "Let's do a prelimiary check to familiarize ourselves with the data first. What informations are stored in the csv files?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3PKgyyw52e2",
        "outputId": "742aa67e-d363-4b45-9ad0-5515f93877a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 894 entries, 0 to 893\n",
            "Data columns (total 64 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   variable  894 non-null    object \n",
            " 1   1         894 non-null    float64\n",
            " 2   2         894 non-null    float64\n",
            " 3   3         894 non-null    float64\n",
            " 4   5         894 non-null    float64\n",
            " 5   6         894 non-null    float64\n",
            " 6   7         894 non-null    float64\n",
            " 7   8         894 non-null    float64\n",
            " 8   11        894 non-null    float64\n",
            " 9   12        894 non-null    float64\n",
            " 10  13        894 non-null    float64\n",
            " 11  16        894 non-null    float64\n",
            " 12  19        894 non-null    float64\n",
            " 13  21        894 non-null    float64\n",
            " 14  26        894 non-null    float64\n",
            " 15  27        894 non-null    float64\n",
            " 16  29        894 non-null    float64\n",
            " 17  31        894 non-null    float64\n",
            " 18  32        894 non-null    float64\n",
            " 19  33        894 non-null    float64\n",
            " 20  34        894 non-null    float64\n",
            " 21  38        894 non-null    float64\n",
            " 22  39        894 non-null    float64\n",
            " 23  43        894 non-null    float64\n",
            " 24  44        894 non-null    float64\n",
            " 25  45        894 non-null    float64\n",
            " 26  46        894 non-null    float64\n",
            " 27  47        894 non-null    float64\n",
            " 28  48        894 non-null    float64\n",
            " 29  49        894 non-null    float64\n",
            " 30  50        894 non-null    float64\n",
            " 31  51        894 non-null    float64\n",
            " 32  53        894 non-null    float64\n",
            " 33  54        894 non-null    float64\n",
            " 34  56        894 non-null    float64\n",
            " 35  58        894 non-null    float64\n",
            " 36  60        894 non-null    float64\n",
            " 37  61        894 non-null    float64\n",
            " 38  62        894 non-null    float64\n",
            " 39  63        894 non-null    float64\n",
            " 40  64        894 non-null    float64\n",
            " 41  65        894 non-null    float64\n",
            " 42  66        894 non-null    float64\n",
            " 43  67        894 non-null    float64\n",
            " 44  68        894 non-null    float64\n",
            " 45  69        894 non-null    float64\n",
            " 46  71        894 non-null    float64\n",
            " 47  72        894 non-null    float64\n",
            " 48  73        894 non-null    float64\n",
            " 49  76        894 non-null    float64\n",
            " 50  77        894 non-null    float64\n",
            " 51  78        894 non-null    float64\n",
            " 52  79        894 non-null    float64\n",
            " 53  80        894 non-null    float64\n",
            " 54  81        894 non-null    float64\n",
            " 55  82        894 non-null    float64\n",
            " 56  83        894 non-null    float64\n",
            " 57  85        894 non-null    float64\n",
            " 58  86        894 non-null    float64\n",
            " 59  87        894 non-null    float64\n",
            " 60  89        894 non-null    float64\n",
            " 61  90        894 non-null    float64\n",
            " 62  93        894 non-null    float64\n",
            " 63  95        894 non-null    float64\n",
            "dtypes: float64(63), object(1)\n",
            "memory usage: 447.1+ KB\n"
          ]
        }
      ],
      "source": [
        "all_loadings.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWlEFGVO6ufF"
      },
      "source": [
        "The input csv files contain entries of different PC loadings associated with a particular physical variable. In this case, we see 3 different loadings associated with maximum 1000 hPa geopotential heights. Each row entry represents the time series shape characteristics for 1 storm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "5QwneCW16SEd",
        "outputId": "645126af-512a-456c-e676-d086bdae6eff"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>storm_number</th>\n",
              "      <th>storm_name</th>\n",
              "      <th>cluster_0</th>\n",
              "      <th>cluster_1</th>\n",
              "      <th>cluster_2</th>\n",
              "      <th>cluster_3</th>\n",
              "      <th>cluster_4</th>\n",
              "      <th>cluster_5</th>\n",
              "      <th>cluster_6</th>\n",
              "      <th>cluster_7</th>\n",
              "      <th>cluster_8</th>\n",
              "      <th>cluster_9</th>\n",
              "      <th>cluster_10</th>\n",
              "      <th>cluster_11</th>\n",
              "      <th>cluster_12</th>\n",
              "      <th>cluster_13</th>\n",
              "      <th>cluster_14</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>82</td>\n",
              "      <td>C3S_STORM_TRACKS_ERA5</td>\n",
              "      <td>2.242762</td>\n",
              "      <td>1.441315</td>\n",
              "      <td>1.452814</td>\n",
              "      <td>1.705352</td>\n",
              "      <td>1.482131</td>\n",
              "      <td>2.280985</td>\n",
              "      <td>4.591923</td>\n",
              "      <td>1.618292</td>\n",
              "      <td>3.075134</td>\n",
              "      <td>0.556794</td>\n",
              "      <td>1.619725</td>\n",
              "      <td>2.903208</td>\n",
              "      <td>3.807250</td>\n",
              "      <td>0.856282</td>\n",
              "      <td>0.391914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>C3S_STORM_TRACKS_ERA5</td>\n",
              "      <td>2.549915</td>\n",
              "      <td>0.631354</td>\n",
              "      <td>1.490921</td>\n",
              "      <td>3.063364</td>\n",
              "      <td>0.401477</td>\n",
              "      <td>0.314291</td>\n",
              "      <td>3.869775</td>\n",
              "      <td>0.122579</td>\n",
              "      <td>0.081556</td>\n",
              "      <td>0.006147</td>\n",
              "      <td>1.818020</td>\n",
              "      <td>1.396851</td>\n",
              "      <td>1.348957</td>\n",
              "      <td>0.248937</td>\n",
              "      <td>0.762328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13</td>\n",
              "      <td>C3S_STORM_TRACKS_ERA5</td>\n",
              "      <td>0.388078</td>\n",
              "      <td>0.111882</td>\n",
              "      <td>2.341128</td>\n",
              "      <td>0.081180</td>\n",
              "      <td>0.461070</td>\n",
              "      <td>0.290018</td>\n",
              "      <td>1.320162</td>\n",
              "      <td>0.097615</td>\n",
              "      <td>0.172890</td>\n",
              "      <td>0.229829</td>\n",
              "      <td>1.615095</td>\n",
              "      <td>2.983414</td>\n",
              "      <td>4.470059</td>\n",
              "      <td>0.174978</td>\n",
              "      <td>0.238000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>95</td>\n",
              "      <td>AIDEN</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>0.788324</td>\n",
              "      <td>0.375771</td>\n",
              "      <td>2.405377</td>\n",
              "      <td>0.633807</td>\n",
              "      <td>0.136090</td>\n",
              "      <td>3.773348</td>\n",
              "      <td>0.226815</td>\n",
              "      <td>0.507296</td>\n",
              "      <td>0.007835</td>\n",
              "      <td>0.227672</td>\n",
              "      <td>0.907070</td>\n",
              "      <td>0.734044</td>\n",
              "      <td>0.023960</td>\n",
              "      <td>1.022132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>HERTA</td>\n",
              "      <td>1.176497</td>\n",
              "      <td>2.152701</td>\n",
              "      <td>0.839556</td>\n",
              "      <td>4.972494</td>\n",
              "      <td>0.238821</td>\n",
              "      <td>0.338416</td>\n",
              "      <td>2.370679</td>\n",
              "      <td>0.979826</td>\n",
              "      <td>0.835825</td>\n",
              "      <td>0.509907</td>\n",
              "      <td>0.669523</td>\n",
              "      <td>2.056866</td>\n",
              "      <td>1.313424</td>\n",
              "      <td>1.460252</td>\n",
              "      <td>4.788285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>12</td>\n",
              "      <td>C3S_STORM_TRACKS_ERA5</td>\n",
              "      <td>0.644110</td>\n",
              "      <td>0.095139</td>\n",
              "      <td>3.364392</td>\n",
              "      <td>2.349703</td>\n",
              "      <td>0.614161</td>\n",
              "      <td>0.211642</td>\n",
              "      <td>2.991942</td>\n",
              "      <td>0.103584</td>\n",
              "      <td>0.010931</td>\n",
              "      <td>0.295994</td>\n",
              "      <td>1.274047</td>\n",
              "      <td>1.759591</td>\n",
              "      <td>2.270806</td>\n",
              "      <td>0.463708</td>\n",
              "      <td>1.908599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>73</td>\n",
              "      <td>XAVER</td>\n",
              "      <td>9.164017</td>\n",
              "      <td>1.967819</td>\n",
              "      <td>2.317177</td>\n",
              "      <td>5.309236</td>\n",
              "      <td>2.678723</td>\n",
              "      <td>0.285856</td>\n",
              "      <td>6.795611</td>\n",
              "      <td>0.631347</td>\n",
              "      <td>0.524138</td>\n",
              "      <td>0.308713</td>\n",
              "      <td>1.864410</td>\n",
              "      <td>3.644108</td>\n",
              "      <td>4.920953</td>\n",
              "      <td>0.273122</td>\n",
              "      <td>0.432820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>8</td>\n",
              "      <td>C3S_STORM_TRACKS_ERA5</td>\n",
              "      <td>2.425189</td>\n",
              "      <td>1.976442</td>\n",
              "      <td>0.632020</td>\n",
              "      <td>2.847837</td>\n",
              "      <td>0.553720</td>\n",
              "      <td>0.094339</td>\n",
              "      <td>6.995991</td>\n",
              "      <td>0.857006</td>\n",
              "      <td>0.069633</td>\n",
              "      <td>0.164015</td>\n",
              "      <td>0.917848</td>\n",
              "      <td>1.430968</td>\n",
              "      <td>0.757042</td>\n",
              "      <td>0.510893</td>\n",
              "      <td>1.421436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>83</td>\n",
              "      <td>C3S_STORM_TRACKS_ERA5</td>\n",
              "      <td>2.600947</td>\n",
              "      <td>1.348670</td>\n",
              "      <td>0.844972</td>\n",
              "      <td>4.010580</td>\n",
              "      <td>0.809561</td>\n",
              "      <td>0.128716</td>\n",
              "      <td>4.218259</td>\n",
              "      <td>0.902120</td>\n",
              "      <td>0.365990</td>\n",
              "      <td>0.426293</td>\n",
              "      <td>1.721566</td>\n",
              "      <td>1.772529</td>\n",
              "      <td>1.847808</td>\n",
              "      <td>0.122705</td>\n",
              "      <td>2.275047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>43</td>\n",
              "      <td>C3S_STORM_TRACKS_ERA5</td>\n",
              "      <td>1.058049</td>\n",
              "      <td>1.236552</td>\n",
              "      <td>0.289159</td>\n",
              "      <td>2.254326</td>\n",
              "      <td>0.810320</td>\n",
              "      <td>1.549956</td>\n",
              "      <td>3.443604</td>\n",
              "      <td>1.579309</td>\n",
              "      <td>0.194101</td>\n",
              "      <td>0.072695</td>\n",
              "      <td>0.527556</td>\n",
              "      <td>1.218760</td>\n",
              "      <td>0.658943</td>\n",
              "      <td>1.477242</td>\n",
              "      <td>0.070396</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>63 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    storm_number             storm_name  cluster_0  cluster_1  cluster_2  \\\n",
              "0             82  C3S_STORM_TRACKS_ERA5   2.242762   1.441315   1.452814   \n",
              "1              1  C3S_STORM_TRACKS_ERA5   2.549915   0.631354   1.490921   \n",
              "2             13  C3S_STORM_TRACKS_ERA5   0.388078   0.111882   2.341128   \n",
              "3             95                 AIDEN    0.090794   0.788324   0.375771   \n",
              "4              7                  HERTA   1.176497   2.152701   0.839556   \n",
              "..           ...                    ...        ...        ...        ...   \n",
              "58            12  C3S_STORM_TRACKS_ERA5   0.644110   0.095139   3.364392   \n",
              "59            73                  XAVER   9.164017   1.967819   2.317177   \n",
              "60             8  C3S_STORM_TRACKS_ERA5   2.425189   1.976442   0.632020   \n",
              "61            83  C3S_STORM_TRACKS_ERA5   2.600947   1.348670   0.844972   \n",
              "62            43  C3S_STORM_TRACKS_ERA5   1.058049   1.236552   0.289159   \n",
              "\n",
              "    cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  cluster_8  \\\n",
              "0    1.705352   1.482131   2.280985   4.591923   1.618292   3.075134   \n",
              "1    3.063364   0.401477   0.314291   3.869775   0.122579   0.081556   \n",
              "2    0.081180   0.461070   0.290018   1.320162   0.097615   0.172890   \n",
              "3    2.405377   0.633807   0.136090   3.773348   0.226815   0.507296   \n",
              "4    4.972494   0.238821   0.338416   2.370679   0.979826   0.835825   \n",
              "..        ...        ...        ...        ...        ...        ...   \n",
              "58   2.349703   0.614161   0.211642   2.991942   0.103584   0.010931   \n",
              "59   5.309236   2.678723   0.285856   6.795611   0.631347   0.524138   \n",
              "60   2.847837   0.553720   0.094339   6.995991   0.857006   0.069633   \n",
              "61   4.010580   0.809561   0.128716   4.218259   0.902120   0.365990   \n",
              "62   2.254326   0.810320   1.549956   3.443604   1.579309   0.194101   \n",
              "\n",
              "    cluster_9  cluster_10  cluster_11  cluster_12  cluster_13  cluster_14  \n",
              "0    0.556794    1.619725    2.903208    3.807250    0.856282    0.391914  \n",
              "1    0.006147    1.818020    1.396851    1.348957    0.248937    0.762328  \n",
              "2    0.229829    1.615095    2.983414    4.470059    0.174978    0.238000  \n",
              "3    0.007835    0.227672    0.907070    0.734044    0.023960    1.022132  \n",
              "4    0.509907    0.669523    2.056866    1.313424    1.460252    4.788285  \n",
              "..        ...         ...         ...         ...         ...         ...  \n",
              "58   0.295994    1.274047    1.759591    2.270806    0.463708    1.908599  \n",
              "59   0.308713    1.864410    3.644108    4.920953    0.273122    0.432820  \n",
              "60   0.164015    0.917848    1.430968    0.757042    0.510893    1.421436  \n",
              "61   0.426293    1.721566    1.772529    1.847808    0.122705    2.275047  \n",
              "62   0.072695    0.527556    1.218760    0.658943    1.477242    0.070396  \n",
              "\n",
              "[63 rows x 17 columns]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_quantile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znZ0ZtutCwx2"
      },
      "source": [
        "The output csv files contain entries of quantile functions for 94 storms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2RLDoYRDmkV"
      },
      "source": [
        "## Combine input csvs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''var1 = input_1000_Z_max.to_numpy()[:,1:]\n",
        "var3 = input_800_rh_max.to_numpy()[:,1:]\n",
        "var2 = input_2m_dew_max.to_numpy()[:,1:]'''\n",
        "\n",
        "# Create and Normalize INPUT_TRAIN\n",
        "#INPUT_TRAIN = np.concatenate((var1, var2, var3), axis=1)\n",
        "#scaler = StandardScaler()  # or MinMaxScaler()\n",
        "#INPUT_TRAIN_NORMALIZED = np.asarray(scaler.fit_transform(INPUT_TRAIN), dtype=np.float32)\n",
        "# TO DO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "KwHsU5JgC26B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "# Convert the filtered DataFrame to a NumPy array (if needed, retain specific columns)\n",
        "INPUT_TRAIN = X_train.to_numpy(dtype=np.float32)\n",
        "scaler = StandardScaler()  # or MinMaxScaler()\n",
        "INPUT_TRAIN_NORMALIZED = np.asarray(scaler.fit_transform(INPUT_TRAIN), dtype=np.float32)\n",
        "\n",
        "INPUT_TEST = X_test.to_numpy(dtype=np.float32)\n",
        "INPUT_TEST_NORMALIZED = np.asarray(scaler.transform(INPUT_TEST), dtype=np.float32)\n",
        "\n",
        "INPUT_VALIDATION = X_validation.to_numpy(dtype=np.float32)\n",
        "INPUT_VALIDATION_NORMALIZED = np.asarray(scaler.transform(INPUT_VALIDATION), dtype=np.float32)\n",
        "\n",
        "# Create ML outputs\n",
        "filtered_output_train = output_quantile[output_quantile['storm_number'].isin(storm_index_training)]\n",
        "filtered_output_train = filtered_output_train.drop(columns=['storm_name', 'storm_number'])\n",
        "filtered_output_test = output_quantile[output_quantile['storm_number'].isin(storm_index_test)]\n",
        "filtered_output_test = filtered_output_test.drop(columns=['storm_name', 'storm_number'])\n",
        "filtered_output_validation = output_quantile[output_quantile['storm_number'].isin(storm_index_validation)]\n",
        "filtered_output_validation = filtered_output_validation.drop(columns=['storm_name', 'storm_number'])\n",
        "\n",
        "# Convert the filtered DataFrame to a NumPy array (if needed, retain specific columns)\n",
        "OUTPUT_TRAIN = filtered_output_train.to_numpy(dtype=np.float32)\n",
        "OUTPUT_TEST = filtered_output_test.to_numpy(dtype=np.float32)\n",
        "OUTPUT_VALIDATION = filtered_output_validation.to_numpy(dtype=np.float32)\n",
        "\n",
        "filtered_input_np = INPUT_TRAIN.T#.to_numpy(dtype=np.float32)\n",
        "for i in range(len(filtered_input_np)):\n",
        "    locals()[f'name_var{i+1}'] = selected_var[i]\n",
        "    locals()[f'var{i+1}'] = filtered_input_np[:,i:i+1]\n",
        "\n",
        "\n",
        "'''name_var1 = selected_var[0]\n",
        "name_var2 = selected_var[1]\n",
        "name_var3 = selected_var[2]\n",
        "name_var4 = selected_var[3]\n",
        "var1 = filtered_input_np[:, :1]\n",
        "var2 = filtered_input_np[:, 1:2]\n",
        "var3 = filtered_input_np[:, 2:3]\n",
        "var4 = filtered_input_np[:, 3:4]'''\n",
        "brchsize = [0] + [varobj.shape[1] for varobj in [var1, var2, var3, var4]]#, var5, var6, var7, var8, var9, var10, var11]]\n",
        "print(brchsize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMv77celI_oG"
      },
      "source": [
        "## Baseline ML model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "vcCLiVevIX6v"
      },
      "outputs": [],
      "source": [
        "# Convert NumPy arrays to PyTorch tensors\n",
        "x_tensor_train = torch.tensor(INPUT_TRAIN_NORMALIZED)\n",
        "y_tensor_train = torch.tensor(OUTPUT_TRAIN)\n",
        "\n",
        "x_tensor_test = torch.tensor(INPUT_TEST_NORMALIZED)\n",
        "y_tensor_test = torch.tensor(OUTPUT_TEST)\n",
        "\n",
        "x_tensor_validation = torch.tensor(INPUT_VALIDATION_NORMALIZED)\n",
        "y_tensor_validation = torch.tensor(OUTPUT_VALIDATION)\n",
        "\n",
        "# Create a DataLoader\n",
        "dataset_train = TensorDataset(x_tensor_train, y_tensor_train)\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=10, shuffle=True)\n",
        "\n",
        "dataset_test = TensorDataset(x_tensor_test, y_tensor_test)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=10, shuffle=True)\n",
        "\n",
        "dataset_validation = TensorDataset(x_tensor_validation, y_tensor_validation)\n",
        "dataloader_validation = DataLoader(dataset_validation, batch_size=10, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "sopFvxwQJzH5"
      },
      "outputs": [],
      "source": [
        "class baseline_ts_drop(torch.nn.Module):\n",
        "    def __init__(self,droprate,brchindices,num_vars):\n",
        "        super(baseline_ts_drop, self).__init__()\n",
        "        self.brchindices = brchindices\n",
        "        self.num_vars = num_vars\n",
        "        ############################################################\n",
        "        # Create regression layers dynamically for each input\n",
        "        ############################################################\n",
        "        brchsize = self.brchindices[1:]\n",
        "        self.input_layers = torch.nn.ModuleDict({\n",
        "            f\"input{i+1}\": torch.nn.Linear(int(brchsize[i]), 1) for i in range(self.num_vars)\n",
        "        })\n",
        "        ############################################################\n",
        "        # Create dropout layers\n",
        "        ############################################################\n",
        "        self.dropout_layers = torch.nn.ModuleDict({\n",
        "            f\"dropout{i+1}\": torch.nn.Dropout(droprate) for i in range(self.num_vars)\n",
        "        })\n",
        "        self.dropout_end = torch.nn.Dropout(droprate)\n",
        "        ############################################################\n",
        "        # Final Dense Layer\n",
        "        ############################################################\n",
        "        self.denseout = torch.nn.Linear(self.num_vars,15)\n",
        "\n",
        "    def forward(self,X):\n",
        "        brchindex = list(np.asarray(self.brchindices).cumsum())\n",
        "        ############################################################\n",
        "        # First regression layer\n",
        "        ############################################################\n",
        "        inputs = []\n",
        "        for i in range(self.num_vars):\n",
        "            # Extract the relevant branch input\n",
        "            X_branch = X[:, brchindex[i]:brchindex[i+1]]\n",
        "            # Apply dropout and linear layer\n",
        "            X_branch = self.dropout_layers[f\"dropout{i+1}\"](X_branch)\n",
        "            input_layer = self.input_layers[f\"input{i+1}\"](X_branch)\n",
        "\n",
        "            inputs.append(input_layer)\n",
        "        ############################################################\n",
        "        # Concat\n",
        "        ############################################################\n",
        "        bestPC = torch.cat(inputs,1)\n",
        "        ############################################################\n",
        "        # Prediction layer\n",
        "        ############################################################\n",
        "        bestPC = self.dropout_end(bestPC)\n",
        "        outpred = self.denseout(bestPC)\n",
        "        return outpred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EfF1fgbLBUE"
      },
      "source": [
        "### Hyperparameter tuning with Optuna (original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "xOSsz-QWLGRs"
      },
      "outputs": [],
      "source": [
        "dataloaders = {'train': dataloader_train, 'val': dataloader_validation}\n",
        "\n",
        "def objective(trial):\n",
        "  # Model Parameters\n",
        "  brchindices = brchsize\n",
        "  numvars = len(brchsize[1:])\n",
        "\n",
        "  # Initiatlize model\n",
        "  models,losses = [],[]\n",
        "  droprate = trial.suggest_float(\"droprate\",0.05,0.45)\n",
        "  model = baseline_ts_drop(droprate, brchindices, numvars)\n",
        "  lr = trial.suggest_float(\"lr\",1e-6,1e-3)#,log=True)\n",
        "\n",
        "  # Training parameters\n",
        "  optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
        "  criterion = torch.nn.L1Loss()\n",
        "  n_epochs = trial.suggest_int(\"n_epochs\",100,5000)\n",
        "  #n_epochs = 500\n",
        "\n",
        "  scheduler_baselr = trial.suggest_float(\"base_lr\",1e-8,1e-4)\n",
        "  scheduler_maxlr = trial.suggest_float(\"max_lr\",1e-4,1e-2)\n",
        "  scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=scheduler_baselr, max_lr=scheduler_maxlr, cycle_momentum=False)\n",
        "\n",
        "  train_losses = []\n",
        "  for epoch in range(1,n_epochs+1):\n",
        "    loss = 0\n",
        "    for features, labels in dataloader:\n",
        "      optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "      output = model(features)\n",
        "      batch_loss = criterion(output, labels)\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "      loss += batch_loss.item()\n",
        "    scheduler.step()\n",
        "    loss = loss/len(dataloader)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    #val_loss = ts_models.eval_model(model,\n",
        "    #                                val_loader,\n",
        "    #                                criterion,\n",
        "    #                         l2_lambda)\n",
        "    #    val_losses.append(val_loss)\n",
        "    if epoch%100 == 0:\n",
        "      print('Epoch: {}/{}.............'.format(epoch, n_epochs))\n",
        "      print(\"Loss: {:.4f}\".format(loss))\n",
        "    #if val_loss <= min(val_losses):\n",
        "    #    torch.save(model,'best_model'+str(trial.number))\n",
        "    #torch.save(model,'./tmp/bayesian/best_model.8.'+str(trial.number)+'.pt')\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### hyperparameters with validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataloader = dataloader_train\n",
        "val_loader = dataloader_validation\n",
        "\n",
        "def objective(trial):\n",
        "    # Model Parameters\n",
        "    brchindices = brchsize\n",
        "    numvars = len(brchsize[1:])\n",
        "\n",
        "    # Initialize model\n",
        "    droprate = trial.suggest_float(\"droprate\", 0.05, 0.45)\n",
        "    model = baseline_ts_drop(droprate, brchindices, numvars)\n",
        "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-3)  # log=True\n",
        "\n",
        "    # Training parameters\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = torch.nn.L1Loss()\n",
        "    n_epochs = trial.suggest_int(\"n_epochs\", 100, 5000)\n",
        "\n",
        "    scheduler_baselr = trial.suggest_float(\"base_lr\", 1e-8, 1e-4)\n",
        "    scheduler_maxlr = trial.suggest_float(\"max_lr\", 1e-4, 1e-2)\n",
        "    scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
        "        optimizer, base_lr=scheduler_baselr, max_lr=scheduler_maxlr, cycle_momentum=False\n",
        "    )\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for features, labels in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(features)\n",
        "            batch_loss = criterion(output, labels)\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += batch_loss.item()\n",
        "\n",
        "        train_loss /= len(dataloader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for val_features, val_labels in val_loader:\n",
        "                val_output = model(val_features)\n",
        "                val_batch_loss = criterion(val_output, val_labels)\n",
        "                val_loss += val_batch_loss.item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Checkpoint if best validation loss\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            #torch.save(model, f'best_model_trial_{trial.number}.pt')\n",
        "\n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "        trial.report(val_loss, epoch)\n",
        "\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # Logging\n",
        "        #if epoch % 100 == 0:\n",
        "            #print(f'Epoch: {epoch}/{n_epochs}')\n",
        "            #print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Return validation loss for Optuna to optimize\n",
        "    return best_val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNF8hvRiOOI-",
        "outputId": "f3df2406-dfee-411e-ea61-ef82dede6add"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-01-04 14:48:08,773] A new study created in memory with name: no-name-0bdc0fb7-5733-4874-a64c-ee4b627d8a48\n",
            "[I 2025-01-04 14:48:16,065] Trial 0 finished with value: 0.7926695346832275 and parameters: {'droprate': 0.40870797143833315, 'lr': 0.0006889834653397472, 'n_epochs': 2421, 'base_lr': 2.42451020642546e-05, 'max_lr': 0.0015959055695640462}. Best is trial 0 with value: 0.7926695346832275.\n",
            "[I 2025-01-04 14:48:25,752] Trial 1 finished with value: 0.7807731628417969 and parameters: {'droprate': 0.1430843411323627, 'lr': 0.000643408717501557, 'n_epochs': 3185, 'base_lr': 2.207136611589472e-05, 'max_lr': 0.007469607439900095}. Best is trial 1 with value: 0.7807731628417969.\n",
            "[I 2025-01-04 14:48:36,435] Trial 2 finished with value: 0.7847859859466553 and parameters: {'droprate': 0.24785534889379068, 'lr': 0.00018965523588339216, 'n_epochs': 3526, 'base_lr': 8.289918702364304e-05, 'max_lr': 0.004451906814795751}. Best is trial 1 with value: 0.7807731628417969.\n",
            "[I 2025-01-04 14:48:49,875] Trial 3 finished with value: 0.7923163771629333 and parameters: {'droprate': 0.4111735378428913, 'lr': 0.0007430551748192148, 'n_epochs': 4518, 'base_lr': 7.114623862860264e-05, 'max_lr': 0.002240402605737251}. Best is trial 1 with value: 0.7807731628417969.\n",
            "[I 2025-01-04 14:49:00,719] Trial 4 finished with value: 0.7859287858009338 and parameters: {'droprate': 0.4094233899611672, 'lr': 0.000855833628725576, 'n_epochs': 3682, 'base_lr': 3.4726128213678463e-06, 'max_lr': 0.007665210100722266}. Best is trial 1 with value: 0.7807731628417969.\n",
            "[I 2025-01-04 14:49:02,552] Trial 5 finished with value: 0.813017725944519 and parameters: {'droprate': 0.2930969678432639, 'lr': 0.00034951952535091345, 'n_epochs': 597, 'base_lr': 8.685473561420501e-05, 'max_lr': 0.005113042058361076}. Best is trial 1 with value: 0.7807731628417969.\n",
            "[I 2025-01-04 14:49:08,905] Trial 6 finished with value: 0.7815611362457275 and parameters: {'droprate': 0.3794537690059366, 'lr': 0.00043885601502506816, 'n_epochs': 2030, 'base_lr': 7.502126010769429e-05, 'max_lr': 0.008960336649840145}. Best is trial 1 with value: 0.7807731628417969.\n",
            "[I 2025-01-04 14:49:15,532] Trial 7 finished with value: 0.7865939736366272 and parameters: {'droprate': 0.42945862670319274, 'lr': 0.00010452394025582626, 'n_epochs': 2098, 'base_lr': 2.484532402086539e-05, 'max_lr': 0.009438619271955597}. Best is trial 1 with value: 0.7807731628417969.\n",
            "[I 2025-01-04 14:49:16,025] Trial 8 pruned. \n",
            "[I 2025-01-04 14:49:23,987] Trial 9 finished with value: 0.783219039440155 and parameters: {'droprate': 0.27043855935662564, 'lr': 0.0009037309406970189, 'n_epochs': 2511, 'base_lr': 6.66579210567552e-05, 'max_lr': 0.006982812272742671}. Best is trial 1 with value: 0.7807731628417969.\n",
            "[I 2025-01-04 14:49:24,444] Trial 10 pruned. \n",
            "[I 2025-01-04 14:49:24,458] Trial 11 pruned. \n",
            "[I 2025-01-04 14:49:24,472] Trial 12 pruned. \n",
            "[I 2025-01-04 14:49:29,916] Trial 13 finished with value: 0.7892654538154602 and parameters: {'droprate': 0.33469457119364826, 'lr': 0.00025602108670934787, 'n_epochs': 1742, 'base_lr': 4.758187446148275e-06, 'max_lr': 0.009886848755730504}. Best is trial 1 with value: 0.7807731628417969.\n",
            "[I 2025-01-04 14:49:29,929] Trial 14 pruned. \n",
            "[I 2025-01-04 14:49:33,735] Trial 15 finished with value: 0.773786187171936 and parameters: {'droprate': 0.08082312264179667, 'lr': 0.0009989770828560444, 'n_epochs': 1227, 'base_lr': 5.6014661628266764e-05, 'max_lr': 0.00830481796110949}. Best is trial 15 with value: 0.773786187171936.\n",
            "[I 2025-01-04 14:49:33,748] Trial 16 pruned. \n",
            "[I 2025-01-04 14:49:37,461] Trial 17 finished with value: 0.7699189782142639 and parameters: {'droprate': 0.11143296354623042, 'lr': 0.000773133116399915, 'n_epochs': 1195, 'base_lr': 1.474218062139913e-05, 'max_lr': 0.008267109038824378}. Best is trial 17 with value: 0.7699189782142639.\n",
            "[I 2025-01-04 14:49:37,475] Trial 18 pruned. \n",
            "[I 2025-01-04 14:49:37,489] Trial 19 pruned. \n",
            "[I 2025-01-04 14:49:37,505] Trial 20 pruned. \n",
            "[I 2025-01-04 14:49:37,519] Trial 21 pruned. \n",
            "[I 2025-01-04 14:49:37,806] Trial 22 pruned. \n",
            "[I 2025-01-04 14:49:37,821] Trial 23 pruned. \n",
            "[I 2025-01-04 14:49:37,835] Trial 24 pruned. \n",
            "[I 2025-01-04 14:49:37,850] Trial 25 pruned. \n",
            "[I 2025-01-04 14:49:37,865] Trial 26 pruned. \n",
            "[I 2025-01-04 14:49:38,012] Trial 27 pruned. \n",
            "[I 2025-01-04 14:49:38,027] Trial 28 pruned. \n",
            "[I 2025-01-04 14:49:38,469] Trial 29 pruned. \n",
            "[I 2025-01-04 14:49:38,909] Trial 30 pruned. \n",
            "[I 2025-01-04 14:49:38,924] Trial 31 pruned. \n",
            "[I 2025-01-04 14:49:40,927] Trial 32 pruned. \n",
            "[I 2025-01-04 14:49:40,942] Trial 33 pruned. \n",
            "[I 2025-01-04 14:49:40,958] Trial 34 pruned. \n",
            "[I 2025-01-04 14:49:41,685] Trial 35 pruned. \n",
            "[I 2025-01-04 14:49:41,700] Trial 36 pruned. \n",
            "[I 2025-01-04 14:49:41,975] Trial 37 pruned. \n",
            "[I 2025-01-04 14:49:42,330] Trial 38 pruned. \n",
            "[I 2025-01-04 14:49:42,346] Trial 39 pruned. \n",
            "[I 2025-01-04 14:49:42,362] Trial 40 pruned. \n",
            "[I 2025-01-04 14:49:42,378] Trial 41 pruned. \n",
            "[I 2025-01-04 14:49:53,700] Trial 42 finished with value: 0.7835745215415955 and parameters: {'droprate': 0.2745930922252668, 'lr': 0.0008871970274818059, 'n_epochs': 3490, 'base_lr': 8.601666021465336e-05, 'max_lr': 0.0070021219531811955}. Best is trial 17 with value: 0.7699189782142639.\n",
            "[I 2025-01-04 14:49:53,716] Trial 43 pruned. \n",
            "[I 2025-01-04 14:49:53,745] Trial 44 pruned. \n",
            "[I 2025-01-04 14:49:53,762] Trial 45 pruned. \n",
            "[I 2025-01-04 14:49:53,779] Trial 46 pruned. \n",
            "[I 2025-01-04 14:49:53,795] Trial 47 pruned. \n",
            "[I 2025-01-04 14:49:54,113] Trial 48 pruned. \n",
            "[I 2025-01-04 14:49:58,870] Trial 49 finished with value: 0.7692097425460815 and parameters: {'droprate': 0.14809839131128982, 'lr': 0.0004692752775261013, 'n_epochs': 1522, 'base_lr': 4.2258401561443366e-05, 'max_lr': 0.009424411628693995}. Best is trial 49 with value: 0.7692097425460815.\n",
            "[I 2025-01-04 14:50:03,168] Trial 50 finished with value: 0.787656307220459 and parameters: {'droprate': 0.13901252563631217, 'lr': 0.0004527556178366374, 'n_epochs': 1375, 'base_lr': 2.503668738901212e-05, 'max_lr': 0.00949033194059223}. Best is trial 49 with value: 0.7692097425460815.\n",
            "[I 2025-01-04 14:50:03,184] Trial 51 pruned. \n",
            "[I 2025-01-04 14:50:03,494] Trial 52 pruned. \n",
            "[I 2025-01-04 14:50:09,784] Trial 53 finished with value: 0.7856953740119934 and parameters: {'droprate': 0.26780214717893236, 'lr': 0.000389471008163444, 'n_epochs': 2002, 'base_lr': 4.529137614188651e-05, 'max_lr': 0.009173916955094008}. Best is trial 49 with value: 0.7692097425460815.\n",
            "[I 2025-01-04 14:50:09,801] Trial 54 pruned. \n",
            "[I 2025-01-04 14:50:09,818] Trial 55 pruned. \n",
            "[I 2025-01-04 14:50:14,616] Trial 56 finished with value: 0.7707358598709106 and parameters: {'droprate': 0.06352041681500657, 'lr': 0.0007488097990708584, 'n_epochs': 1519, 'base_lr': 5.766878857460536e-05, 'max_lr': 0.007262845156317832}. Best is trial 49 with value: 0.7692097425460815.\n",
            "[I 2025-01-04 14:50:14,633] Trial 57 pruned. \n",
            "[I 2025-01-04 14:50:14,650] Trial 58 pruned. \n",
            "[I 2025-01-04 14:50:14,667] Trial 59 pruned. \n",
            "[I 2025-01-04 14:50:14,685] Trial 60 pruned. \n",
            "[I 2025-01-04 14:50:14,704] Trial 61 pruned. \n",
            "[I 2025-01-04 14:50:14,722] Trial 62 pruned. \n",
            "[I 2025-01-04 14:50:16,705] Trial 63 pruned. \n",
            "[I 2025-01-04 14:50:16,722] Trial 64 pruned. \n",
            "[I 2025-01-04 14:50:17,718] Trial 65 pruned. \n",
            "[I 2025-01-04 14:50:17,735] Trial 66 pruned. \n",
            "[I 2025-01-04 14:50:17,754] Trial 67 pruned. \n",
            "[I 2025-01-04 14:50:17,772] Trial 68 pruned. \n",
            "[I 2025-01-04 14:50:18,176] Trial 69 pruned. \n",
            "[I 2025-01-04 14:50:18,195] Trial 70 pruned. \n",
            "[I 2025-01-04 14:50:18,213] Trial 71 pruned. \n",
            "[I 2025-01-04 14:50:18,234] Trial 72 pruned. \n",
            "[I 2025-01-04 14:50:18,253] Trial 73 pruned. \n",
            "[I 2025-01-04 14:50:18,272] Trial 74 pruned. \n",
            "[I 2025-01-04 14:50:18,290] Trial 75 pruned. \n",
            "[I 2025-01-04 14:50:18,309] Trial 76 pruned. \n",
            "[I 2025-01-04 14:50:18,329] Trial 77 pruned. \n",
            "[I 2025-01-04 14:50:18,348] Trial 78 pruned. \n",
            "[I 2025-01-04 14:50:18,367] Trial 79 pruned. \n",
            "[I 2025-01-04 14:50:18,487] Trial 80 pruned. \n",
            "[I 2025-01-04 14:50:18,692] Trial 81 pruned. \n",
            "[I 2025-01-04 14:50:18,913] Trial 82 pruned. \n",
            "[I 2025-01-04 14:50:18,932] Trial 83 pruned. \n",
            "[I 2025-01-04 14:50:18,952] Trial 84 pruned. \n",
            "[I 2025-01-04 14:50:18,973] Trial 85 pruned. \n",
            "[I 2025-01-04 14:50:18,993] Trial 86 pruned. \n",
            "[I 2025-01-04 14:50:19,205] Trial 87 pruned. \n",
            "[I 2025-01-04 14:50:19,227] Trial 88 pruned. \n",
            "[I 2025-01-04 14:50:19,247] Trial 89 pruned. \n",
            "[I 2025-01-04 14:50:19,267] Trial 90 pruned. \n",
            "[I 2025-01-04 14:50:19,288] Trial 91 pruned. \n",
            "[I 2025-01-04 14:50:19,308] Trial 92 pruned. \n",
            "[I 2025-01-04 14:50:19,658] Trial 93 pruned. \n",
            "[I 2025-01-04 14:50:19,679] Trial 94 pruned. \n",
            "[I 2025-01-04 14:50:19,700] Trial 95 pruned. \n",
            "[I 2025-01-04 14:50:20,026] Trial 96 pruned. \n",
            "[I 2025-01-04 14:50:20,048] Trial 97 pruned. \n",
            "[I 2025-01-04 14:50:20,069] Trial 98 pruned. \n",
            "[I 2025-01-04 14:50:20,091] Trial 99 pruned. \n",
            "[I 2025-01-04 14:50:27,837] Trial 100 finished with value: 0.7856603860855103 and parameters: {'droprate': 0.22502608281742742, 'lr': 0.0004503791948136195, 'n_epochs': 2435, 'base_lr': 3.155752645271181e-05, 'max_lr': 0.00971406283205123}. Best is trial 49 with value: 0.7692097425460815.\n",
            "[I 2025-01-04 14:50:27,857] Trial 101 pruned. \n",
            "[I 2025-01-04 14:50:34,700] Trial 102 finished with value: 0.777797520160675 and parameters: {'droprate': 0.29708854924883243, 'lr': 0.0005807692419854832, 'n_epochs': 2160, 'base_lr': 4.348000601563817e-05, 'max_lr': 0.009622559365120796}. Best is trial 49 with value: 0.7692097425460815.\n",
            "[I 2025-01-04 14:50:34,722] Trial 103 pruned. \n",
            "[I 2025-01-04 14:50:34,892] Trial 104 pruned. \n",
            "[I 2025-01-04 14:50:34,913] Trial 105 pruned. \n",
            "[I 2025-01-04 14:50:46,394] Trial 106 finished with value: 0.778110921382904 and parameters: {'droprate': 0.24374387400765843, 'lr': 0.0005956342909255577, 'n_epochs': 3554, 'base_lr': 3.732245120393404e-05, 'max_lr': 0.00944926523098437}. Best is trial 49 with value: 0.7692097425460815.\n",
            "[I 2025-01-04 14:50:46,416] Trial 107 pruned. \n",
            "[I 2025-01-04 14:50:46,439] Trial 108 pruned. \n",
            "[I 2025-01-04 14:50:46,460] Trial 109 pruned. \n",
            "[I 2025-01-04 14:50:46,483] Trial 110 pruned. \n",
            "[I 2025-01-04 14:50:46,506] Trial 111 pruned. \n",
            "[I 2025-01-04 14:50:46,530] Trial 112 pruned. \n",
            "[I 2025-01-04 14:50:46,552] Trial 113 pruned. \n",
            "[I 2025-01-04 14:50:46,576] Trial 114 pruned. \n",
            "[I 2025-01-04 14:50:46,599] Trial 115 pruned. \n",
            "[I 2025-01-04 14:50:46,622] Trial 116 pruned. \n",
            "[I 2025-01-04 14:50:46,645] Trial 117 pruned. \n",
            "[I 2025-01-04 14:50:46,668] Trial 118 pruned. \n",
            "[I 2025-01-04 14:50:46,691] Trial 119 pruned. \n",
            "[I 2025-01-04 14:50:46,714] Trial 120 pruned. \n",
            "[I 2025-01-04 14:50:46,738] Trial 121 pruned. \n",
            "[I 2025-01-04 14:50:46,761] Trial 122 pruned. \n",
            "[I 2025-01-04 14:50:52,576] Trial 123 finished with value: 0.7571823000907898 and parameters: {'droprate': 0.05020124278439857, 'lr': 0.00040883056777993407, 'n_epochs': 1837, 'base_lr': 7.28594690744838e-05, 'max_lr': 0.009068312862929105}. Best is trial 123 with value: 0.7571823000907898.\n",
            "[I 2025-01-04 14:50:52,667] Trial 124 pruned. \n",
            "[I 2025-01-04 14:50:52,691] Trial 125 pruned. \n",
            "[I 2025-01-04 14:50:52,715] Trial 126 pruned. \n",
            "[I 2025-01-04 14:50:52,739] Trial 127 pruned. \n",
            "[I 2025-01-04 14:50:52,761] Trial 128 pruned. \n",
            "[I 2025-01-04 14:50:52,784] Trial 129 pruned. \n",
            "[I 2025-01-04 14:50:52,809] Trial 130 pruned. \n",
            "[I 2025-01-04 14:50:52,831] Trial 131 pruned. \n",
            "[I 2025-01-04 14:50:52,854] Trial 132 pruned. \n",
            "[I 2025-01-04 14:50:52,878] Trial 133 pruned. \n",
            "[I 2025-01-04 14:50:52,903] Trial 134 pruned. \n",
            "[I 2025-01-04 14:50:52,926] Trial 135 pruned. \n",
            "[I 2025-01-04 14:50:52,950] Trial 136 pruned. \n",
            "[I 2025-01-04 14:50:52,975] Trial 137 pruned. \n",
            "[I 2025-01-04 14:50:53,125] Trial 138 pruned. \n",
            "[I 2025-01-04 14:50:53,149] Trial 139 pruned. \n",
            "[I 2025-01-04 14:50:53,172] Trial 140 pruned. \n",
            "[I 2025-01-04 14:50:53,198] Trial 141 pruned. \n",
            "[I 2025-01-04 14:50:53,223] Trial 142 pruned. \n",
            "[I 2025-01-04 14:50:53,248] Trial 143 pruned. \n",
            "[I 2025-01-04 14:50:53,271] Trial 144 pruned. \n",
            "[I 2025-01-04 14:50:53,297] Trial 145 pruned. \n",
            "[I 2025-01-04 14:50:53,323] Trial 146 pruned. \n",
            "[I 2025-01-04 14:50:53,347] Trial 147 pruned. \n",
            "[I 2025-01-04 14:50:53,372] Trial 148 pruned. \n",
            "[I 2025-01-04 14:50:55,360] Trial 149 pruned. \n",
            "[I 2025-01-04 14:50:55,386] Trial 150 pruned. \n",
            "[I 2025-01-04 14:50:55,413] Trial 151 pruned. \n",
            "[I 2025-01-04 14:50:55,441] Trial 152 pruned. \n",
            "[I 2025-01-04 14:50:55,468] Trial 153 pruned. \n",
            "[I 2025-01-04 14:50:55,498] Trial 154 pruned. \n",
            "[I 2025-01-04 14:50:55,626] Trial 155 pruned. \n",
            "[I 2025-01-04 14:50:55,654] Trial 156 pruned. \n",
            "[I 2025-01-04 14:50:55,681] Trial 157 pruned. \n",
            "[I 2025-01-04 14:50:55,707] Trial 158 pruned. \n",
            "[I 2025-01-04 14:50:55,734] Trial 159 pruned. \n",
            "[I 2025-01-04 14:50:55,761] Trial 160 pruned. \n",
            "[I 2025-01-04 14:50:56,266] Trial 161 pruned. \n",
            "[I 2025-01-04 14:50:56,294] Trial 162 pruned. \n",
            "[I 2025-01-04 14:50:56,322] Trial 163 pruned. \n",
            "[I 2025-01-04 14:50:56,348] Trial 164 pruned. \n",
            "[I 2025-01-04 14:50:56,375] Trial 165 pruned. \n",
            "[I 2025-01-04 14:50:56,402] Trial 166 pruned. \n",
            "[I 2025-01-04 14:50:56,429] Trial 167 pruned. \n",
            "[I 2025-01-04 14:50:56,454] Trial 168 pruned. \n",
            "[I 2025-01-04 14:50:59,374] Trial 169 finished with value: 0.7879743576049805 and parameters: {'droprate': 0.2568495541697244, 'lr': 0.000709353925021102, 'n_epochs': 904, 'base_lr': 8.388496535097468e-05, 'max_lr': 0.00809198925093261}. Best is trial 123 with value: 0.7571823000907898.\n",
            "[I 2025-01-04 14:50:59,400] Trial 170 pruned. \n",
            "[I 2025-01-04 14:50:59,427] Trial 171 pruned. \n",
            "[I 2025-01-04 14:50:59,989] Trial 172 pruned. \n",
            "[I 2025-01-04 14:51:00,467] Trial 173 pruned. \n",
            "[I 2025-01-04 14:51:00,492] Trial 174 pruned. \n",
            "[I 2025-01-04 14:51:00,519] Trial 175 pruned. \n",
            "[I 2025-01-04 14:51:00,546] Trial 176 pruned. \n",
            "[I 2025-01-04 14:51:00,574] Trial 177 pruned. \n",
            "[I 2025-01-04 14:51:00,599] Trial 178 pruned. \n",
            "[I 2025-01-04 14:51:00,626] Trial 179 pruned. \n",
            "[I 2025-01-04 14:51:00,652] Trial 180 pruned. \n",
            "[I 2025-01-04 14:51:00,679] Trial 181 pruned. \n",
            "[I 2025-01-04 14:51:00,800] Trial 182 pruned. \n",
            "[I 2025-01-04 14:51:00,827] Trial 183 pruned. \n",
            "[I 2025-01-04 14:51:00,854] Trial 184 pruned. \n",
            "[I 2025-01-04 14:51:00,881] Trial 185 pruned. \n",
            "[I 2025-01-04 14:51:00,908] Trial 186 pruned. \n",
            "[I 2025-01-04 14:51:00,934] Trial 187 pruned. \n",
            "[I 2025-01-04 14:51:00,962] Trial 188 pruned. \n",
            "[I 2025-01-04 14:51:00,989] Trial 189 pruned. \n",
            "[I 2025-01-04 14:51:01,017] Trial 190 pruned. \n",
            "[I 2025-01-04 14:51:01,046] Trial 191 pruned. \n",
            "[I 2025-01-04 14:51:01,073] Trial 192 pruned. \n",
            "[I 2025-01-04 14:51:01,101] Trial 193 pruned. \n",
            "[I 2025-01-04 14:51:01,215] Trial 194 pruned. \n",
            "[I 2025-01-04 14:51:01,244] Trial 195 pruned. \n",
            "[I 2025-01-04 14:51:01,528] Trial 196 pruned. \n",
            "[I 2025-01-04 14:51:01,555] Trial 197 pruned. \n",
            "[I 2025-01-04 14:51:01,585] Trial 198 pruned. \n",
            "[I 2025-01-04 14:51:01,648] Trial 199 pruned. \n",
            "[I 2025-01-04 14:51:01,677] Trial 200 pruned. \n",
            "[I 2025-01-04 14:51:01,706] Trial 201 pruned. \n",
            "[I 2025-01-04 14:51:01,735] Trial 202 pruned. \n",
            "[I 2025-01-04 14:51:01,765] Trial 203 pruned. \n",
            "[I 2025-01-04 14:51:01,793] Trial 204 pruned. \n",
            "[I 2025-01-04 14:51:01,845] Trial 205 pruned. \n",
            "[I 2025-01-04 14:51:01,874] Trial 206 pruned. \n",
            "[I 2025-01-04 14:51:01,903] Trial 207 pruned. \n",
            "[I 2025-01-04 14:51:01,931] Trial 208 pruned. \n",
            "[I 2025-01-04 14:51:01,975] Trial 209 pruned. \n",
            "[I 2025-01-04 14:51:02,006] Trial 210 pruned. \n",
            "[I 2025-01-04 14:51:02,035] Trial 211 pruned. \n",
            "[I 2025-01-04 14:51:02,152] Trial 212 pruned. \n",
            "[I 2025-01-04 14:51:02,295] Trial 213 pruned. \n",
            "[I 2025-01-04 14:51:02,324] Trial 214 pruned. \n",
            "[I 2025-01-04 14:51:02,355] Trial 215 pruned. \n",
            "[I 2025-01-04 14:51:02,383] Trial 216 pruned. \n",
            "[I 2025-01-04 14:51:02,414] Trial 217 pruned. \n",
            "[I 2025-01-04 14:51:02,443] Trial 218 pruned. \n",
            "[I 2025-01-04 14:51:02,473] Trial 219 pruned. \n",
            "[I 2025-01-04 14:51:02,505] Trial 220 pruned. \n",
            "[I 2025-01-04 14:51:02,962] Trial 221 pruned. \n",
            "[I 2025-01-04 14:51:02,991] Trial 222 pruned. \n",
            "[I 2025-01-04 14:51:03,022] Trial 223 pruned. \n",
            "[I 2025-01-04 14:51:03,052] Trial 224 pruned. \n",
            "[I 2025-01-04 14:51:03,082] Trial 225 pruned. \n",
            "[I 2025-01-04 14:51:03,112] Trial 226 pruned. \n",
            "[I 2025-01-04 14:51:03,143] Trial 227 pruned. \n",
            "[I 2025-01-04 14:51:03,174] Trial 228 pruned. \n",
            "[I 2025-01-04 14:51:03,206] Trial 229 pruned. \n",
            "[I 2025-01-04 14:51:03,236] Trial 230 pruned. \n",
            "[I 2025-01-04 14:51:03,268] Trial 231 pruned. \n",
            "[I 2025-01-04 14:51:03,299] Trial 232 pruned. \n",
            "[I 2025-01-04 14:51:03,329] Trial 233 pruned. \n",
            "[I 2025-01-04 14:51:03,361] Trial 234 pruned. \n",
            "[I 2025-01-04 14:51:03,392] Trial 235 pruned. \n",
            "[I 2025-01-04 14:51:03,424] Trial 236 pruned. \n",
            "[I 2025-01-04 14:51:03,454] Trial 237 pruned. \n",
            "[I 2025-01-04 14:51:03,578] Trial 238 pruned. \n",
            "[I 2025-01-04 14:51:03,611] Trial 239 pruned. \n",
            "[I 2025-01-04 14:51:03,642] Trial 240 pruned. \n",
            "[I 2025-01-04 14:51:03,810] Trial 241 pruned. \n",
            "[I 2025-01-04 14:51:03,842] Trial 242 pruned. \n",
            "[I 2025-01-04 14:51:04,517] Trial 243 pruned. \n",
            "[I 2025-01-04 14:51:04,550] Trial 244 pruned. \n",
            "[I 2025-01-04 14:51:04,654] Trial 245 pruned. \n",
            "[I 2025-01-04 14:51:04,685] Trial 246 pruned. \n",
            "[I 2025-01-04 14:51:04,718] Trial 247 pruned. \n",
            "[I 2025-01-04 14:51:04,750] Trial 248 pruned. \n",
            "[I 2025-01-04 14:51:04,782] Trial 249 pruned. \n",
            "[I 2025-01-04 14:51:04,814] Trial 250 pruned. \n",
            "[I 2025-01-04 14:51:04,846] Trial 251 pruned. \n",
            "[I 2025-01-04 14:51:04,879] Trial 252 pruned. \n",
            "[I 2025-01-04 14:51:04,910] Trial 253 pruned. \n",
            "[I 2025-01-04 14:51:04,943] Trial 254 pruned. \n",
            "[I 2025-01-04 14:51:04,976] Trial 255 pruned. \n",
            "[I 2025-01-04 14:51:05,008] Trial 256 pruned. \n",
            "[I 2025-01-04 14:51:05,393] Trial 257 pruned. \n",
            "[I 2025-01-04 14:51:05,439] Trial 258 pruned. \n",
            "[I 2025-01-04 14:51:05,472] Trial 259 pruned. \n",
            "[I 2025-01-04 14:51:05,504] Trial 260 pruned. \n",
            "[I 2025-01-04 14:51:05,536] Trial 261 pruned. \n",
            "[I 2025-01-04 14:51:05,570] Trial 262 pruned. \n",
            "[I 2025-01-04 14:51:05,601] Trial 263 pruned. \n",
            "[I 2025-01-04 14:51:05,634] Trial 264 pruned. \n",
            "[I 2025-01-04 14:51:05,665] Trial 265 pruned. \n",
            "[I 2025-01-04 14:51:05,699] Trial 266 pruned. \n",
            "[I 2025-01-04 14:51:05,733] Trial 267 pruned. \n",
            "[I 2025-01-04 14:51:06,122] Trial 268 pruned. \n",
            "[I 2025-01-04 14:51:06,155] Trial 269 pruned. \n",
            "[I 2025-01-04 14:51:06,189] Trial 270 pruned. \n",
            "[I 2025-01-04 14:51:06,221] Trial 271 pruned. \n",
            "[I 2025-01-04 14:51:06,254] Trial 272 pruned. \n",
            "[I 2025-01-04 14:51:06,287] Trial 273 pruned. \n",
            "[I 2025-01-04 14:51:06,319] Trial 274 pruned. \n",
            "[I 2025-01-04 14:51:06,354] Trial 275 pruned. \n",
            "[I 2025-01-04 14:51:06,388] Trial 276 pruned. \n",
            "[I 2025-01-04 14:51:06,421] Trial 277 pruned. \n",
            "[I 2025-01-04 14:51:06,453] Trial 278 pruned. \n",
            "[I 2025-01-04 14:51:06,487] Trial 279 pruned. \n",
            "[I 2025-01-04 14:51:06,522] Trial 280 pruned. \n",
            "[I 2025-01-04 14:51:06,555] Trial 281 pruned. \n",
            "[I 2025-01-04 14:51:06,987] Trial 282 pruned. \n",
            "[I 2025-01-04 14:51:07,020] Trial 283 pruned. \n",
            "[I 2025-01-04 14:51:07,113] Trial 284 pruned. \n",
            "[I 2025-01-04 14:51:07,149] Trial 285 pruned. \n",
            "[I 2025-01-04 14:51:07,183] Trial 286 pruned. \n",
            "[I 2025-01-04 14:51:07,327] Trial 287 pruned. \n",
            "[I 2025-01-04 14:51:07,362] Trial 288 pruned. \n",
            "[I 2025-01-04 14:51:07,398] Trial 289 pruned. \n",
            "[I 2025-01-04 14:51:07,432] Trial 290 pruned. \n",
            "[I 2025-01-04 14:51:07,465] Trial 291 pruned. \n",
            "[I 2025-01-04 14:51:07,499] Trial 292 pruned. \n",
            "[I 2025-01-04 14:51:07,535] Trial 293 pruned. \n",
            "[I 2025-01-04 14:51:07,668] Trial 294 pruned. \n",
            "[I 2025-01-04 14:51:07,704] Trial 295 pruned. \n",
            "[I 2025-01-04 14:51:07,740] Trial 296 pruned. \n",
            "[I 2025-01-04 14:51:07,774] Trial 297 pruned. \n",
            "[I 2025-01-04 14:51:07,810] Trial 298 pruned. \n",
            "[I 2025-01-04 14:51:07,847] Trial 299 pruned. \n",
            "[I 2025-01-04 14:51:07,882] Trial 300 pruned. \n",
            "[I 2025-01-04 14:51:07,916] Trial 301 pruned. \n",
            "[I 2025-01-04 14:51:07,951] Trial 302 pruned. \n",
            "[I 2025-01-04 14:51:07,985] Trial 303 pruned. \n",
            "[I 2025-01-04 14:51:08,020] Trial 304 pruned. \n",
            "[I 2025-01-04 14:51:08,055] Trial 305 pruned. \n",
            "[I 2025-01-04 14:51:08,624] Trial 306 pruned. \n",
            "[I 2025-01-04 14:51:08,658] Trial 307 pruned. \n",
            "[I 2025-01-04 14:51:08,694] Trial 308 pruned. \n",
            "[I 2025-01-04 14:51:08,730] Trial 309 pruned. \n",
            "[I 2025-01-04 14:51:09,468] Trial 310 pruned. \n",
            "[I 2025-01-04 14:51:09,503] Trial 311 pruned. \n",
            "[I 2025-01-04 14:51:09,538] Trial 312 pruned. \n",
            "[I 2025-01-04 14:51:09,573] Trial 313 pruned. \n",
            "[I 2025-01-04 14:51:09,608] Trial 314 pruned. \n",
            "[I 2025-01-04 14:51:09,742] Trial 315 pruned. \n",
            "[I 2025-01-04 14:51:09,780] Trial 316 pruned. \n",
            "[I 2025-01-04 14:51:09,817] Trial 317 pruned. \n",
            "[I 2025-01-04 14:51:09,853] Trial 318 pruned. \n",
            "[I 2025-01-04 14:51:09,889] Trial 319 pruned. \n",
            "[I 2025-01-04 14:51:09,973] Trial 320 pruned. \n",
            "[I 2025-01-04 14:51:10,011] Trial 321 pruned. \n",
            "[I 2025-01-04 14:51:10,046] Trial 322 pruned. \n",
            "[I 2025-01-04 14:51:10,082] Trial 323 pruned. \n",
            "[I 2025-01-04 14:51:10,188] Trial 324 pruned. \n",
            "[I 2025-01-04 14:51:10,226] Trial 325 pruned. \n",
            "[I 2025-01-04 14:51:10,263] Trial 326 pruned. \n",
            "[I 2025-01-04 14:51:10,301] Trial 327 pruned. \n",
            "[I 2025-01-04 14:51:10,402] Trial 328 pruned. \n",
            "[I 2025-01-04 14:51:10,439] Trial 329 pruned. \n",
            "[I 2025-01-04 14:51:10,476] Trial 330 pruned. \n",
            "[I 2025-01-04 14:51:10,513] Trial 331 pruned. \n",
            "[I 2025-01-04 14:51:10,636] Trial 332 pruned. \n",
            "[I 2025-01-04 14:51:10,672] Trial 333 pruned. \n",
            "[I 2025-01-04 14:51:10,709] Trial 334 pruned. \n",
            "[I 2025-01-04 14:51:10,747] Trial 335 pruned. \n",
            "[I 2025-01-04 14:51:10,783] Trial 336 pruned. \n",
            "[I 2025-01-04 14:51:10,820] Trial 337 pruned. \n",
            "[I 2025-01-04 14:51:10,958] Trial 338 pruned. \n",
            "[I 2025-01-04 14:51:10,996] Trial 339 pruned. \n",
            "[I 2025-01-04 14:51:11,032] Trial 340 pruned. \n",
            "[I 2025-01-04 14:51:11,070] Trial 341 pruned. \n",
            "[I 2025-01-04 14:51:11,106] Trial 342 pruned. \n",
            "[I 2025-01-04 14:51:11,144] Trial 343 pruned. \n",
            "[I 2025-01-04 14:51:11,320] Trial 344 pruned. \n",
            "[I 2025-01-04 14:51:11,357] Trial 345 pruned. \n",
            "[I 2025-01-04 14:51:11,394] Trial 346 pruned. \n",
            "[I 2025-01-04 14:51:11,432] Trial 347 pruned. \n",
            "[I 2025-01-04 14:51:11,468] Trial 348 pruned. \n",
            "[I 2025-01-04 14:51:11,506] Trial 349 pruned. \n",
            "[I 2025-01-04 14:51:11,541] Trial 350 pruned. \n",
            "[I 2025-01-04 14:51:11,581] Trial 351 pruned. \n",
            "[I 2025-01-04 14:51:11,680] Trial 352 pruned. \n",
            "[I 2025-01-04 14:51:11,827] Trial 353 pruned. \n",
            "[I 2025-01-04 14:51:11,865] Trial 354 pruned. \n",
            "[I 2025-01-04 14:51:11,906] Trial 355 pruned. \n",
            "[I 2025-01-04 14:51:11,945] Trial 356 pruned. \n",
            "[I 2025-01-04 14:51:11,984] Trial 357 pruned. \n",
            "[I 2025-01-04 14:51:12,023] Trial 358 pruned. \n",
            "[I 2025-01-04 14:51:12,458] Trial 359 pruned. \n",
            "[I 2025-01-04 14:51:12,496] Trial 360 pruned. \n",
            "[I 2025-01-04 14:51:12,606] Trial 361 pruned. \n",
            "[I 2025-01-04 14:51:12,645] Trial 362 pruned. \n",
            "[I 2025-01-04 14:51:12,779] Trial 363 pruned. \n",
            "[I 2025-01-04 14:51:12,821] Trial 364 pruned. \n",
            "[I 2025-01-04 14:51:12,861] Trial 365 pruned. \n",
            "[I 2025-01-04 14:51:12,899] Trial 366 pruned. \n",
            "[I 2025-01-04 14:51:12,939] Trial 367 pruned. \n",
            "[I 2025-01-04 14:51:13,015] Trial 368 pruned. \n",
            "[I 2025-01-04 14:51:13,055] Trial 369 pruned. \n",
            "[I 2025-01-04 14:51:13,095] Trial 370 pruned. \n",
            "[I 2025-01-04 14:51:13,135] Trial 371 pruned. \n",
            "[I 2025-01-04 14:51:13,186] Trial 372 pruned. \n",
            "[I 2025-01-04 14:51:13,288] Trial 373 pruned. \n",
            "[I 2025-01-04 14:51:13,328] Trial 374 pruned. \n",
            "[I 2025-01-04 14:51:13,367] Trial 375 pruned. \n",
            "[I 2025-01-04 14:51:13,405] Trial 376 pruned. \n",
            "[I 2025-01-04 14:51:13,447] Trial 377 pruned. \n",
            "[I 2025-01-04 14:51:13,878] Trial 378 pruned. \n",
            "[I 2025-01-04 14:51:13,918] Trial 379 pruned. \n",
            "[I 2025-01-04 14:51:13,960] Trial 380 pruned. \n",
            "[I 2025-01-04 14:51:14,000] Trial 381 pruned. \n",
            "[I 2025-01-04 14:51:14,041] Trial 382 pruned. \n",
            "[I 2025-01-04 14:51:14,081] Trial 383 pruned. \n",
            "[I 2025-01-04 14:51:14,122] Trial 384 pruned. \n",
            "[I 2025-01-04 14:51:14,163] Trial 385 pruned. \n",
            "[I 2025-01-04 14:51:14,203] Trial 386 pruned. \n",
            "[I 2025-01-04 14:51:14,360] Trial 387 pruned. \n",
            "[I 2025-01-04 14:51:14,607] Trial 388 pruned. \n",
            "[I 2025-01-04 14:51:14,647] Trial 389 pruned. \n",
            "[I 2025-01-04 14:51:14,686] Trial 390 pruned. \n",
            "[I 2025-01-04 14:51:16,046] Trial 391 finished with value: 0.7788357138633728 and parameters: {'droprate': 0.11280972153997779, 'lr': 0.0008661498113782022, 'n_epochs': 412, 'base_lr': 8.985417227869546e-05, 'max_lr': 0.00945406861910064}. Best is trial 123 with value: 0.7571823000907898.\n",
            "[I 2025-01-04 14:51:16,089] Trial 392 pruned. \n",
            "[I 2025-01-04 14:51:16,131] Trial 393 pruned. \n",
            "[I 2025-01-04 14:51:16,171] Trial 394 pruned. \n",
            "[I 2025-01-04 14:51:16,213] Trial 395 pruned. \n",
            "[I 2025-01-04 14:51:16,255] Trial 396 pruned. \n",
            "[I 2025-01-04 14:51:16,297] Trial 397 pruned. \n",
            "[I 2025-01-04 14:51:16,337] Trial 398 pruned. \n",
            "[I 2025-01-04 14:51:16,378] Trial 399 pruned. \n",
            "[I 2025-01-04 14:51:16,516] Trial 400 pruned. \n",
            "[I 2025-01-04 14:51:16,558] Trial 401 pruned. \n",
            "[I 2025-01-04 14:51:16,599] Trial 402 pruned. \n",
            "[I 2025-01-04 14:51:16,642] Trial 403 pruned. \n",
            "[I 2025-01-04 14:51:16,683] Trial 404 pruned. \n",
            "[I 2025-01-04 14:51:16,808] Trial 405 pruned. \n",
            "[I 2025-01-04 14:51:16,849] Trial 406 pruned. \n",
            "[I 2025-01-04 14:51:16,891] Trial 407 pruned. \n",
            "[I 2025-01-04 14:51:16,932] Trial 408 pruned. \n",
            "[I 2025-01-04 14:51:16,974] Trial 409 pruned. \n",
            "[I 2025-01-04 14:51:17,016] Trial 410 pruned. \n",
            "[I 2025-01-04 14:51:17,057] Trial 411 pruned. \n",
            "[I 2025-01-04 14:51:17,098] Trial 412 pruned. \n",
            "[I 2025-01-04 14:51:17,141] Trial 413 pruned. \n",
            "[I 2025-01-04 14:51:17,183] Trial 414 pruned. \n",
            "[I 2025-01-04 14:51:17,227] Trial 415 pruned. \n",
            "[I 2025-01-04 14:51:17,270] Trial 416 pruned. \n",
            "[I 2025-01-04 14:51:24,068] Trial 417 finished with value: 0.7764729857444763 and parameters: {'droprate': 0.11333995143681588, 'lr': 0.0003979149406624526, 'n_epochs': 2128, 'base_lr': 6.364638781318802e-05, 'max_lr': 0.008792893294403773}. Best is trial 123 with value: 0.7571823000907898.\n",
            "[I 2025-01-04 14:51:24,110] Trial 418 pruned. \n",
            "[I 2025-01-04 14:51:24,154] Trial 419 pruned. \n",
            "[I 2025-01-04 14:51:24,195] Trial 420 pruned. \n",
            "[I 2025-01-04 14:51:24,237] Trial 421 pruned. \n",
            "[I 2025-01-04 14:51:24,280] Trial 422 pruned. \n",
            "[I 2025-01-04 14:51:24,322] Trial 423 pruned. \n",
            "[I 2025-01-04 14:51:24,366] Trial 424 pruned. \n",
            "[I 2025-01-04 14:51:24,409] Trial 425 pruned. \n",
            "[I 2025-01-04 14:51:24,453] Trial 426 pruned. \n",
            "[I 2025-01-04 14:51:24,498] Trial 427 pruned. \n",
            "[I 2025-01-04 14:51:24,769] Trial 428 pruned. \n",
            "[I 2025-01-04 14:51:24,813] Trial 429 pruned. \n",
            "[I 2025-01-04 14:51:24,937] Trial 430 pruned. \n",
            "[I 2025-01-04 14:51:24,980] Trial 431 pruned. \n",
            "[I 2025-01-04 14:51:25,022] Trial 432 pruned. \n",
            "[I 2025-01-04 14:51:25,065] Trial 433 pruned. \n",
            "[I 2025-01-04 14:51:25,121] Trial 434 pruned. \n",
            "[I 2025-01-04 14:51:25,169] Trial 435 pruned. \n",
            "[I 2025-01-04 14:51:25,213] Trial 436 pruned. \n",
            "[I 2025-01-04 14:51:25,817] Trial 437 pruned. \n",
            "[I 2025-01-04 14:51:25,859] Trial 438 pruned. \n",
            "[I 2025-01-04 14:51:25,903] Trial 439 pruned. \n",
            "[I 2025-01-04 14:51:25,945] Trial 440 pruned. \n",
            "[I 2025-01-04 14:51:25,988] Trial 441 pruned. \n",
            "[I 2025-01-04 14:51:26,031] Trial 442 pruned. \n",
            "[I 2025-01-04 14:51:26,074] Trial 443 pruned. \n",
            "[I 2025-01-04 14:51:26,118] Trial 444 pruned. \n",
            "[I 2025-01-04 14:51:26,161] Trial 445 pruned. \n",
            "[I 2025-01-04 14:51:26,204] Trial 446 pruned. \n",
            "[I 2025-01-04 14:51:26,246] Trial 447 pruned. \n",
            "[I 2025-01-04 14:51:26,690] Trial 448 pruned. \n",
            "[I 2025-01-04 14:51:26,735] Trial 449 pruned. \n",
            "[I 2025-01-04 14:51:26,778] Trial 450 pruned. \n",
            "[I 2025-01-04 14:51:26,822] Trial 451 pruned. \n",
            "[I 2025-01-04 14:51:26,866] Trial 452 pruned. \n",
            "[I 2025-01-04 14:51:26,912] Trial 453 pruned. \n",
            "[I 2025-01-04 14:51:26,956] Trial 454 pruned. \n",
            "[I 2025-01-04 14:51:27,909] Trial 455 pruned. \n",
            "[I 2025-01-04 14:51:28,269] Trial 456 pruned. \n",
            "[I 2025-01-04 14:51:28,315] Trial 457 pruned. \n",
            "[I 2025-01-04 14:51:28,359] Trial 458 pruned. \n",
            "[I 2025-01-04 14:51:28,404] Trial 459 pruned. \n",
            "[I 2025-01-04 14:51:28,448] Trial 460 pruned. \n",
            "[I 2025-01-04 14:51:28,742] Trial 461 pruned. \n",
            "[I 2025-01-04 14:51:28,787] Trial 462 pruned. \n",
            "[I 2025-01-04 14:51:28,830] Trial 463 pruned. \n",
            "[I 2025-01-04 14:51:28,876] Trial 464 pruned. \n",
            "[I 2025-01-04 14:51:28,921] Trial 465 pruned. \n",
            "[I 2025-01-04 14:51:28,966] Trial 466 pruned. \n",
            "[I 2025-01-04 14:51:29,013] Trial 467 pruned. \n",
            "[I 2025-01-04 14:51:29,059] Trial 468 pruned. \n",
            "[I 2025-01-04 14:51:29,103] Trial 469 pruned. \n",
            "[I 2025-01-04 14:51:29,147] Trial 470 pruned. \n",
            "[I 2025-01-04 14:51:29,192] Trial 471 pruned. \n",
            "[I 2025-01-04 14:51:29,961] Trial 472 pruned. \n",
            "[I 2025-01-04 14:51:30,007] Trial 473 pruned. \n",
            "[I 2025-01-04 14:51:30,051] Trial 474 pruned. \n",
            "[I 2025-01-04 14:51:30,098] Trial 475 pruned. \n",
            "[I 2025-01-04 14:51:30,144] Trial 476 pruned. \n",
            "[I 2025-01-04 14:51:30,190] Trial 477 pruned. \n",
            "[I 2025-01-04 14:51:30,236] Trial 478 pruned. \n",
            "[I 2025-01-04 14:51:30,281] Trial 479 pruned. \n",
            "[I 2025-01-04 14:51:30,329] Trial 480 pruned. \n",
            "[I 2025-01-04 14:51:30,375] Trial 481 pruned. \n",
            "[I 2025-01-04 14:51:30,420] Trial 482 pruned. \n",
            "[I 2025-01-04 14:51:30,464] Trial 483 pruned. \n",
            "[I 2025-01-04 14:51:30,513] Trial 484 pruned. \n",
            "[I 2025-01-04 14:51:30,561] Trial 485 pruned. \n",
            "[I 2025-01-04 14:51:30,608] Trial 486 pruned. \n",
            "[I 2025-01-04 14:51:30,654] Trial 487 pruned. \n",
            "[I 2025-01-04 14:51:30,698] Trial 488 pruned. \n",
            "[I 2025-01-04 14:51:30,744] Trial 489 pruned. \n",
            "[I 2025-01-04 14:51:30,806] Trial 490 pruned. \n",
            "[I 2025-01-04 14:51:30,855] Trial 491 pruned. \n",
            "[I 2025-01-04 14:51:30,901] Trial 492 pruned. \n",
            "[I 2025-01-04 14:51:30,949] Trial 493 pruned. \n",
            "[I 2025-01-04 14:51:30,997] Trial 494 pruned. \n",
            "[I 2025-01-04 14:51:31,044] Trial 495 pruned. \n",
            "[I 2025-01-04 14:51:31,090] Trial 496 pruned. \n",
            "[I 2025-01-04 14:51:31,138] Trial 497 pruned. \n",
            "[I 2025-01-04 14:51:31,185] Trial 498 pruned. \n",
            "[I 2025-01-04 14:51:31,231] Trial 499 pruned. \n",
            "[I 2025-01-04 14:51:31,279] Trial 500 pruned. \n",
            "[I 2025-01-04 14:51:32,108] Trial 501 pruned. \n",
            "[I 2025-01-04 14:51:32,157] Trial 502 pruned. \n",
            "[I 2025-01-04 14:51:32,207] Trial 503 pruned. \n",
            "[I 2025-01-04 14:51:32,254] Trial 504 pruned. \n",
            "[I 2025-01-04 14:51:32,300] Trial 505 pruned. \n",
            "[I 2025-01-04 14:51:32,346] Trial 506 pruned. \n",
            "[I 2025-01-04 14:51:32,601] Trial 507 pruned. \n",
            "[I 2025-01-04 14:51:32,650] Trial 508 pruned. \n",
            "[I 2025-01-04 14:51:32,696] Trial 509 pruned. \n",
            "[I 2025-01-04 14:51:32,745] Trial 510 pruned. \n",
            "[I 2025-01-04 14:51:32,794] Trial 511 pruned. \n",
            "[I 2025-01-04 14:51:32,971] Trial 512 pruned. \n",
            "[I 2025-01-04 14:51:33,020] Trial 513 pruned. \n",
            "[I 2025-01-04 14:51:37,682] Trial 514 finished with value: 0.7747180461883545 and parameters: {'droprate': 0.24569287245829954, 'lr': 0.000813947968883997, 'n_epochs': 1457, 'base_lr': 6.995984891322642e-05, 'max_lr': 0.008923693499118741}. Best is trial 123 with value: 0.7571823000907898.\n",
            "[I 2025-01-04 14:51:38,485] Trial 515 pruned. \n",
            "[I 2025-01-04 14:51:38,534] Trial 516 pruned. \n",
            "[I 2025-01-04 14:51:38,581] Trial 517 pruned. \n",
            "[I 2025-01-04 14:51:38,629] Trial 518 pruned. \n",
            "[I 2025-01-04 14:51:38,675] Trial 519 pruned. \n",
            "[I 2025-01-04 14:51:38,723] Trial 520 pruned. \n",
            "[I 2025-01-04 14:51:38,773] Trial 521 pruned. \n",
            "[I 2025-01-04 14:51:38,824] Trial 522 pruned. \n",
            "[I 2025-01-04 14:51:38,873] Trial 523 pruned. \n",
            "[I 2025-01-04 14:51:38,923] Trial 524 pruned. \n",
            "[I 2025-01-04 14:51:38,974] Trial 525 pruned. \n",
            "[I 2025-01-04 14:51:39,025] Trial 526 pruned. \n",
            "[I 2025-01-04 14:51:39,294] Trial 527 pruned. \n",
            "[I 2025-01-04 14:51:39,344] Trial 528 pruned. \n",
            "[I 2025-01-04 14:51:39,394] Trial 529 pruned. \n",
            "[I 2025-01-04 14:51:39,445] Trial 530 pruned. \n",
            "[I 2025-01-04 14:51:39,610] Trial 531 pruned. \n",
            "[I 2025-01-04 14:51:39,658] Trial 532 pruned. \n",
            "[I 2025-01-04 14:51:39,706] Trial 533 pruned. \n",
            "[I 2025-01-04 14:51:39,759] Trial 534 pruned. \n",
            "[I 2025-01-04 14:51:39,806] Trial 535 pruned. \n",
            "[I 2025-01-04 14:51:39,858] Trial 536 pruned. \n",
            "[I 2025-01-04 14:51:39,906] Trial 537 pruned. \n",
            "[I 2025-01-04 14:51:39,955] Trial 538 pruned. \n",
            "[I 2025-01-04 14:51:40,006] Trial 539 pruned. \n",
            "[I 2025-01-04 14:51:40,057] Trial 540 pruned. \n",
            "[I 2025-01-04 14:51:40,108] Trial 541 pruned. \n",
            "[I 2025-01-04 14:51:40,158] Trial 542 pruned. \n",
            "[I 2025-01-04 14:51:40,207] Trial 543 pruned. \n",
            "[I 2025-01-04 14:51:40,272] Trial 544 pruned. \n",
            "[I 2025-01-04 14:51:40,321] Trial 545 pruned. \n",
            "[I 2025-01-04 14:51:40,371] Trial 546 pruned. \n",
            "[I 2025-01-04 14:51:49,180] Trial 547 finished with value: 0.7818715572357178 and parameters: {'droprate': 0.19083913117711465, 'lr': 0.0008896543717686429, 'n_epochs': 2714, 'base_lr': 8.95805567310757e-05, 'max_lr': 0.009133180261317429}. Best is trial 123 with value: 0.7571823000907898.\n",
            "[I 2025-01-04 14:51:49,229] Trial 548 pruned. \n",
            "[I 2025-01-04 14:51:49,281] Trial 549 pruned. \n",
            "[I 2025-01-04 14:51:49,330] Trial 550 pruned. \n",
            "[I 2025-01-04 14:51:49,380] Trial 551 pruned. \n",
            "[I 2025-01-04 14:51:49,430] Trial 552 pruned. \n",
            "[I 2025-01-04 14:51:49,480] Trial 553 pruned. \n",
            "[I 2025-01-04 14:51:49,530] Trial 554 pruned. \n",
            "[I 2025-01-04 14:51:49,578] Trial 555 pruned. \n",
            "[I 2025-01-04 14:51:49,628] Trial 556 pruned. \n",
            "[I 2025-01-04 14:51:49,677] Trial 557 pruned. \n",
            "[I 2025-01-04 14:51:49,728] Trial 558 pruned. \n",
            "[I 2025-01-04 14:51:49,779] Trial 559 pruned. \n",
            "[I 2025-01-04 14:51:49,831] Trial 560 pruned. \n",
            "[I 2025-01-04 14:51:49,883] Trial 561 pruned. \n",
            "[I 2025-01-04 14:51:49,933] Trial 562 pruned. \n",
            "[I 2025-01-04 14:51:49,984] Trial 563 pruned. \n",
            "[I 2025-01-04 14:51:50,037] Trial 564 pruned. \n",
            "[I 2025-01-04 14:51:50,087] Trial 565 pruned. \n",
            "[I 2025-01-04 14:51:50,139] Trial 566 pruned. \n",
            "[I 2025-01-04 14:51:50,190] Trial 567 pruned. \n",
            "[I 2025-01-04 14:51:50,242] Trial 568 pruned. \n",
            "[I 2025-01-04 14:51:50,294] Trial 569 pruned. \n",
            "[I 2025-01-04 14:51:50,580] Trial 570 pruned. \n",
            "[I 2025-01-04 14:51:50,633] Trial 571 pruned. \n",
            "[I 2025-01-04 14:51:50,684] Trial 572 pruned. \n",
            "[I 2025-01-04 14:51:50,980] Trial 573 pruned. \n",
            "[I 2025-01-04 14:51:51,032] Trial 574 pruned. \n",
            "[I 2025-01-04 14:51:51,365] Trial 575 pruned. \n",
            "[I 2025-01-04 14:51:51,416] Trial 576 pruned. \n",
            "[I 2025-01-04 14:51:51,470] Trial 577 pruned. \n",
            "[I 2025-01-04 14:51:51,522] Trial 578 pruned. \n",
            "[I 2025-01-04 14:51:51,921] Trial 579 pruned. \n",
            "[I 2025-01-04 14:51:51,973] Trial 580 pruned. \n",
            "[I 2025-01-04 14:51:52,023] Trial 581 pruned. \n",
            "[I 2025-01-04 14:51:52,074] Trial 582 pruned. \n",
            "[I 2025-01-04 14:51:52,124] Trial 583 pruned. \n",
            "[I 2025-01-04 14:51:52,176] Trial 584 pruned. \n",
            "[I 2025-01-04 14:51:52,227] Trial 585 pruned. \n",
            "[I 2025-01-04 14:51:52,281] Trial 586 pruned. \n",
            "[I 2025-01-04 14:51:52,332] Trial 587 pruned. \n",
            "[I 2025-01-04 14:51:52,383] Trial 588 pruned. \n",
            "[I 2025-01-04 14:51:52,434] Trial 589 pruned. \n",
            "[I 2025-01-04 14:51:52,551] Trial 590 pruned. \n",
            "[I 2025-01-04 14:51:52,604] Trial 591 pruned. \n",
            "[I 2025-01-04 14:51:52,655] Trial 592 pruned. \n",
            "[I 2025-01-04 14:51:52,719] Trial 593 pruned. \n",
            "[I 2025-01-04 14:51:52,772] Trial 594 pruned. \n",
            "[I 2025-01-04 14:51:52,825] Trial 595 pruned. \n",
            "[I 2025-01-04 14:51:52,880] Trial 596 pruned. \n",
            "[I 2025-01-04 14:51:52,935] Trial 597 pruned. \n",
            "[I 2025-01-04 14:51:52,987] Trial 598 pruned. \n",
            "[I 2025-01-04 14:51:53,040] Trial 599 pruned. \n",
            "[I 2025-01-04 14:51:53,093] Trial 600 pruned. \n",
            "[I 2025-01-04 14:51:53,146] Trial 601 pruned. \n",
            "[I 2025-01-04 14:51:53,198] Trial 602 pruned. \n",
            "[I 2025-01-04 14:51:53,252] Trial 603 pruned. \n",
            "[I 2025-01-04 14:51:53,306] Trial 604 pruned. \n",
            "[I 2025-01-04 14:51:53,361] Trial 605 pruned. \n",
            "[I 2025-01-04 14:51:53,413] Trial 606 pruned. \n",
            "[I 2025-01-04 14:51:53,467] Trial 607 pruned. \n",
            "[I 2025-01-04 14:51:53,517] Trial 608 pruned. \n",
            "[I 2025-01-04 14:51:53,568] Trial 609 pruned. \n",
            "[I 2025-01-04 14:51:53,667] Trial 610 pruned. \n",
            "[I 2025-01-04 14:51:53,717] Trial 611 pruned. \n",
            "[I 2025-01-04 14:51:53,770] Trial 612 pruned. \n",
            "[I 2025-01-04 14:51:53,820] Trial 613 pruned. \n",
            "[I 2025-01-04 14:51:53,871] Trial 614 pruned. \n",
            "[I 2025-01-04 14:51:54,325] Trial 615 pruned. \n",
            "[I 2025-01-04 14:51:54,772] Trial 616 pruned. \n",
            "[I 2025-01-04 14:51:54,829] Trial 617 pruned. \n",
            "[I 2025-01-04 14:51:54,882] Trial 618 pruned. \n",
            "[I 2025-01-04 14:51:54,939] Trial 619 pruned. \n",
            "[I 2025-01-04 14:51:54,992] Trial 620 pruned. \n",
            "[I 2025-01-04 14:51:55,042] Trial 621 pruned. \n",
            "[I 2025-01-04 14:51:55,096] Trial 622 pruned. \n",
            "[I 2025-01-04 14:51:55,150] Trial 623 pruned. \n",
            "[I 2025-01-04 14:51:55,518] Trial 624 pruned. \n",
            "[I 2025-01-04 14:51:55,573] Trial 625 pruned. \n",
            "[I 2025-01-04 14:51:55,811] Trial 626 pruned. \n",
            "[I 2025-01-04 14:51:55,866] Trial 627 pruned. \n",
            "[I 2025-01-04 14:51:55,921] Trial 628 pruned. \n",
            "[I 2025-01-04 14:51:55,974] Trial 629 pruned. \n",
            "[I 2025-01-04 14:51:56,030] Trial 630 pruned. \n",
            "[I 2025-01-04 14:51:56,085] Trial 631 pruned. \n",
            "[I 2025-01-04 14:51:56,136] Trial 632 pruned. \n",
            "[I 2025-01-04 14:51:56,192] Trial 633 pruned. \n",
            "[I 2025-01-04 14:51:56,244] Trial 634 pruned. \n",
            "[I 2025-01-04 14:51:56,300] Trial 635 pruned. \n",
            "[I 2025-01-04 14:51:56,353] Trial 636 pruned. \n",
            "[I 2025-01-04 14:51:56,724] Trial 637 pruned. \n",
            "[I 2025-01-04 14:51:56,779] Trial 638 pruned. \n",
            "[I 2025-01-04 14:51:56,850] Trial 639 pruned. \n",
            "[I 2025-01-04 14:51:56,904] Trial 640 pruned. \n",
            "[I 2025-01-04 14:51:56,960] Trial 641 pruned. \n",
            "[I 2025-01-04 14:51:57,014] Trial 642 pruned. \n",
            "[I 2025-01-04 14:51:57,067] Trial 643 pruned. \n",
            "[I 2025-01-04 14:51:57,122] Trial 644 pruned. \n",
            "[I 2025-01-04 14:51:57,177] Trial 645 pruned. \n",
            "[I 2025-01-04 14:51:57,234] Trial 646 pruned. \n",
            "[I 2025-01-04 14:51:57,291] Trial 647 pruned. \n",
            "[I 2025-01-04 14:51:57,348] Trial 648 pruned. \n",
            "[I 2025-01-04 14:51:57,402] Trial 649 pruned. \n",
            "[I 2025-01-04 14:51:57,457] Trial 650 pruned. \n",
            "[I 2025-01-04 14:51:57,514] Trial 651 pruned. \n",
            "[I 2025-01-04 14:51:57,571] Trial 652 pruned. \n",
            "[I 2025-01-04 14:51:57,671] Trial 653 pruned. \n",
            "[I 2025-01-04 14:51:57,726] Trial 654 pruned. \n",
            "[I 2025-01-04 14:51:57,782] Trial 655 pruned. \n",
            "[I 2025-01-04 14:51:57,838] Trial 656 pruned. \n",
            "[I 2025-01-04 14:51:57,892] Trial 657 pruned. \n",
            "[I 2025-01-04 14:51:57,949] Trial 658 pruned. \n",
            "[I 2025-01-04 14:51:58,006] Trial 659 pruned. \n",
            "[I 2025-01-04 14:51:58,062] Trial 660 pruned. \n",
            "[I 2025-01-04 14:51:58,121] Trial 661 pruned. \n",
            "[I 2025-01-04 14:51:58,180] Trial 662 pruned. \n",
            "[I 2025-01-04 14:51:58,240] Trial 663 pruned. \n",
            "[I 2025-01-04 14:51:58,298] Trial 664 pruned. \n",
            "[I 2025-01-04 14:51:58,358] Trial 665 pruned. \n",
            "[I 2025-01-04 14:51:58,417] Trial 666 pruned. \n",
            "[I 2025-01-04 14:51:59,042] Trial 667 pruned. \n",
            "[I 2025-01-04 14:51:59,101] Trial 668 pruned. \n",
            "[I 2025-01-04 14:51:59,156] Trial 669 pruned. \n",
            "[I 2025-01-04 14:51:59,246] Trial 670 pruned. \n",
            "[I 2025-01-04 14:51:59,305] Trial 671 pruned. \n",
            "[I 2025-01-04 14:51:59,364] Trial 672 pruned. \n",
            "[I 2025-01-04 14:51:59,423] Trial 673 pruned. \n",
            "[I 2025-01-04 14:51:59,478] Trial 674 pruned. \n",
            "[I 2025-01-04 14:51:59,639] Trial 675 pruned. \n",
            "[I 2025-01-04 14:51:59,696] Trial 676 pruned. \n",
            "[I 2025-01-04 14:51:59,765] Trial 677 pruned. \n",
            "[I 2025-01-04 14:52:00,525] Trial 678 pruned. \n",
            "[I 2025-01-04 14:52:00,582] Trial 679 pruned. \n",
            "[I 2025-01-04 14:52:00,641] Trial 680 pruned. \n",
            "[I 2025-01-04 14:52:00,699] Trial 681 pruned. \n",
            "[I 2025-01-04 14:52:00,756] Trial 682 pruned. \n",
            "[I 2025-01-04 14:52:00,812] Trial 683 pruned. \n",
            "[I 2025-01-04 14:52:00,871] Trial 684 pruned. \n",
            "[I 2025-01-04 14:52:01,152] Trial 685 pruned. \n",
            "[I 2025-01-04 14:52:01,207] Trial 686 pruned. \n",
            "[I 2025-01-04 14:52:01,265] Trial 687 pruned. \n",
            "[I 2025-01-04 14:52:01,323] Trial 688 pruned. \n",
            "[I 2025-01-04 14:52:01,382] Trial 689 pruned. \n",
            "[I 2025-01-04 14:52:01,439] Trial 690 pruned. \n",
            "[I 2025-01-04 14:52:01,497] Trial 691 pruned. \n",
            "[I 2025-01-04 14:52:01,554] Trial 692 pruned. \n",
            "[I 2025-01-04 14:52:01,612] Trial 693 pruned. \n",
            "[I 2025-01-04 14:52:01,671] Trial 694 pruned. \n",
            "[I 2025-01-04 14:52:01,730] Trial 695 pruned. \n",
            "[I 2025-01-04 14:52:01,789] Trial 696 pruned. \n",
            "[I 2025-01-04 14:52:01,849] Trial 697 pruned. \n",
            "[I 2025-01-04 14:52:02,093] Trial 698 pruned. \n",
            "[I 2025-01-04 14:52:02,154] Trial 699 pruned. \n",
            "[I 2025-01-04 14:52:02,210] Trial 700 pruned. \n",
            "[I 2025-01-04 14:52:02,271] Trial 701 pruned. \n",
            "[I 2025-01-04 14:52:02,328] Trial 702 pruned. \n",
            "[I 2025-01-04 14:52:02,388] Trial 703 pruned. \n",
            "[I 2025-01-04 14:52:02,447] Trial 704 pruned. \n",
            "[I 2025-01-04 14:52:02,509] Trial 705 pruned. \n",
            "[I 2025-01-04 14:52:02,852] Trial 706 pruned. \n",
            "[I 2025-01-04 14:52:02,913] Trial 707 pruned. \n",
            "[I 2025-01-04 14:52:02,972] Trial 708 pruned. \n",
            "[I 2025-01-04 14:52:03,033] Trial 709 pruned. \n",
            "[I 2025-01-04 14:52:03,090] Trial 710 pruned. \n",
            "[I 2025-01-04 14:52:03,774] Trial 711 pruned. \n",
            "[I 2025-01-04 14:52:03,832] Trial 712 pruned. \n",
            "[I 2025-01-04 14:52:03,889] Trial 713 pruned. \n",
            "[I 2025-01-04 14:52:03,949] Trial 714 pruned. \n",
            "[I 2025-01-04 14:52:04,010] Trial 715 pruned. \n",
            "[I 2025-01-04 14:52:04,069] Trial 716 pruned. \n",
            "[I 2025-01-04 14:52:04,127] Trial 717 pruned. \n",
            "[I 2025-01-04 14:52:04,187] Trial 718 pruned. \n",
            "[I 2025-01-04 14:52:04,259] Trial 719 pruned. \n",
            "[I 2025-01-04 14:52:04,578] Trial 720 pruned. \n",
            "[I 2025-01-04 14:52:04,639] Trial 721 pruned. \n",
            "[I 2025-01-04 14:52:04,699] Trial 722 pruned. \n",
            "[I 2025-01-04 14:52:04,759] Trial 723 pruned. \n",
            "[I 2025-01-04 14:52:04,817] Trial 724 pruned. \n",
            "[I 2025-01-04 14:52:04,876] Trial 725 pruned. \n",
            "[I 2025-01-04 14:52:04,936] Trial 726 pruned. \n",
            "[I 2025-01-04 14:52:04,994] Trial 727 pruned. \n",
            "[I 2025-01-04 14:52:05,055] Trial 728 pruned. \n",
            "[I 2025-01-04 14:52:05,118] Trial 729 pruned. \n",
            "[I 2025-01-04 14:52:05,174] Trial 730 pruned. \n",
            "[I 2025-01-04 14:52:05,234] Trial 731 pruned. \n",
            "[I 2025-01-04 14:52:05,297] Trial 732 pruned. \n",
            "[I 2025-01-04 14:52:09,517] Trial 733 finished with value: 0.7700369954109192 and parameters: {'droprate': 0.0694153185611646, 'lr': 0.00063956400106374, 'n_epochs': 1326, 'base_lr': 2.9632361112472285e-05, 'max_lr': 0.009280150307845311}. Best is trial 123 with value: 0.7571823000907898.\n",
            "[I 2025-01-04 14:52:09,573] Trial 734 pruned. \n",
            "[I 2025-01-04 14:52:09,852] Trial 735 pruned. \n",
            "[I 2025-01-04 14:52:09,914] Trial 736 pruned. \n",
            "[I 2025-01-04 14:52:09,973] Trial 737 pruned. \n",
            "[I 2025-01-04 14:52:10,036] Trial 738 pruned. \n",
            "[I 2025-01-04 14:52:10,096] Trial 739 pruned. \n",
            "[I 2025-01-04 14:52:10,156] Trial 740 pruned. \n",
            "[I 2025-01-04 14:52:10,220] Trial 741 pruned. \n",
            "[I 2025-01-04 14:52:10,280] Trial 742 pruned. \n",
            "[I 2025-01-04 14:52:10,343] Trial 743 pruned. \n",
            "[I 2025-01-04 14:52:10,404] Trial 744 pruned. \n",
            "[I 2025-01-04 14:52:10,466] Trial 745 pruned. \n",
            "[I 2025-01-04 14:52:10,525] Trial 746 pruned. \n",
            "[I 2025-01-04 14:52:10,590] Trial 747 pruned. \n",
            "[I 2025-01-04 14:52:10,653] Trial 748 pruned. \n",
            "[I 2025-01-04 14:52:10,720] Trial 749 pruned. \n",
            "[I 2025-01-04 14:52:10,782] Trial 750 pruned. \n",
            "[I 2025-01-04 14:52:10,844] Trial 751 pruned. \n",
            "[I 2025-01-04 14:52:10,907] Trial 752 pruned. \n",
            "[I 2025-01-04 14:52:10,971] Trial 753 pruned. \n",
            "[I 2025-01-04 14:52:11,036] Trial 754 pruned. \n",
            "[I 2025-01-04 14:52:11,102] Trial 755 pruned. \n",
            "[I 2025-01-04 14:52:11,163] Trial 756 pruned. \n",
            "[I 2025-01-04 14:52:11,225] Trial 757 pruned. \n",
            "[I 2025-01-04 14:52:11,667] Trial 758 pruned. \n",
            "[I 2025-01-04 14:52:11,742] Trial 759 pruned. \n",
            "[I 2025-01-04 14:52:11,802] Trial 760 pruned. \n",
            "[I 2025-01-04 14:52:11,862] Trial 761 pruned. \n",
            "[I 2025-01-04 14:52:11,927] Trial 762 pruned. \n",
            "[I 2025-01-04 14:52:11,986] Trial 763 pruned. \n",
            "[I 2025-01-04 14:52:12,047] Trial 764 pruned. \n",
            "[I 2025-01-04 14:52:12,109] Trial 765 pruned. \n",
            "[I 2025-01-04 14:52:12,168] Trial 766 pruned. \n",
            "[I 2025-01-04 14:52:12,231] Trial 767 pruned. \n",
            "[I 2025-01-04 14:52:12,293] Trial 768 pruned. \n",
            "[I 2025-01-04 14:52:12,355] Trial 769 pruned. \n",
            "[I 2025-01-04 14:52:12,418] Trial 770 pruned. \n",
            "[I 2025-01-04 14:52:12,480] Trial 771 pruned. \n",
            "[I 2025-01-04 14:52:12,543] Trial 772 pruned. \n",
            "[I 2025-01-04 14:52:12,609] Trial 773 pruned. \n",
            "[I 2025-01-04 14:52:12,673] Trial 774 pruned. \n",
            "[I 2025-01-04 14:52:13,671] Trial 775 pruned. \n",
            "[I 2025-01-04 14:52:13,735] Trial 776 pruned. \n",
            "[I 2025-01-04 14:52:13,797] Trial 777 pruned. \n",
            "[I 2025-01-04 14:52:13,859] Trial 778 pruned. \n",
            "[I 2025-01-04 14:52:13,924] Trial 779 pruned. \n",
            "[I 2025-01-04 14:52:13,986] Trial 780 pruned. \n",
            "[I 2025-01-04 14:52:14,047] Trial 781 pruned. \n",
            "[I 2025-01-04 14:52:14,108] Trial 782 pruned. \n",
            "[I 2025-01-04 14:52:14,618] Trial 783 pruned. \n",
            "[I 2025-01-04 14:52:14,682] Trial 784 pruned. \n",
            "[I 2025-01-04 14:52:14,747] Trial 785 pruned. \n",
            "[I 2025-01-04 14:52:14,814] Trial 786 pruned. \n",
            "[I 2025-01-04 14:52:14,876] Trial 787 pruned. \n",
            "[I 2025-01-04 14:52:14,938] Trial 788 pruned. \n",
            "[I 2025-01-04 14:52:15,002] Trial 789 pruned. \n",
            "[I 2025-01-04 14:52:15,065] Trial 790 pruned. \n",
            "[I 2025-01-04 14:52:15,128] Trial 791 pruned. \n",
            "[I 2025-01-04 14:52:15,192] Trial 792 pruned. \n",
            "[I 2025-01-04 14:52:15,527] Trial 793 pruned. \n",
            "[I 2025-01-04 14:52:15,591] Trial 794 pruned. \n",
            "[I 2025-01-04 14:52:15,658] Trial 795 pruned. \n",
            "[I 2025-01-04 14:52:15,723] Trial 796 pruned. \n",
            "[I 2025-01-04 14:52:15,799] Trial 797 pruned. \n",
            "[I 2025-01-04 14:52:15,863] Trial 798 pruned. \n",
            "[I 2025-01-04 14:52:15,927] Trial 799 pruned. \n",
            "[I 2025-01-04 14:52:15,991] Trial 800 pruned. \n",
            "[I 2025-01-04 14:52:16,055] Trial 801 pruned. \n",
            "[I 2025-01-04 14:52:16,118] Trial 802 pruned. \n",
            "[I 2025-01-04 14:52:16,184] Trial 803 pruned. \n",
            "[I 2025-01-04 14:52:25,257] Trial 804 finished with value: 0.7778416275978088 and parameters: {'droprate': 0.2570772783146293, 'lr': 0.000379153161096998, 'n_epochs': 2681, 'base_lr': 8.88226099102968e-05, 'max_lr': 0.009798954007179676}. Best is trial 123 with value: 0.7571823000907898.\n",
            "[I 2025-01-04 14:52:25,324] Trial 805 pruned. \n",
            "[I 2025-01-04 14:52:26,237] Trial 806 pruned. \n",
            "[I 2025-01-04 14:52:26,305] Trial 807 pruned. \n",
            "[I 2025-01-04 14:52:26,372] Trial 808 pruned. \n",
            "[I 2025-01-04 14:52:26,438] Trial 809 pruned. \n",
            "[I 2025-01-04 14:52:26,506] Trial 810 pruned. \n",
            "[I 2025-01-04 14:52:26,572] Trial 811 pruned. \n",
            "[I 2025-01-04 14:52:26,639] Trial 812 pruned. \n",
            "[I 2025-01-04 14:52:26,705] Trial 813 pruned. \n",
            "[I 2025-01-04 14:52:26,773] Trial 814 pruned. \n",
            "[I 2025-01-04 14:52:26,838] Trial 815 pruned. \n",
            "[I 2025-01-04 14:52:26,905] Trial 816 pruned. \n",
            "[I 2025-01-04 14:52:26,975] Trial 817 pruned. \n",
            "[I 2025-01-04 14:52:27,039] Trial 818 pruned. \n",
            "[I 2025-01-04 14:52:27,108] Trial 819 pruned. \n",
            "[I 2025-01-04 14:52:27,174] Trial 820 pruned. \n",
            "[I 2025-01-04 14:52:27,598] Trial 821 pruned. \n",
            "[I 2025-01-04 14:52:27,668] Trial 822 pruned. \n",
            "[I 2025-01-04 14:52:27,736] Trial 823 pruned. \n",
            "[I 2025-01-04 14:52:27,806] Trial 824 pruned. \n",
            "[I 2025-01-04 14:52:27,873] Trial 825 pruned. \n",
            "[I 2025-01-04 14:52:28,073] Trial 826 pruned. \n",
            "[I 2025-01-04 14:52:28,139] Trial 827 pruned. \n",
            "[I 2025-01-04 14:52:28,208] Trial 828 pruned. \n",
            "[I 2025-01-04 14:52:28,275] Trial 829 pruned. \n",
            "[I 2025-01-04 14:52:28,343] Trial 830 pruned. \n",
            "[I 2025-01-04 14:52:28,410] Trial 831 pruned. \n",
            "[I 2025-01-04 14:52:28,476] Trial 832 pruned. \n",
            "[I 2025-01-04 14:52:28,557] Trial 833 pruned. \n",
            "[I 2025-01-04 14:52:28,627] Trial 834 pruned. \n",
            "[I 2025-01-04 14:52:28,693] Trial 835 pruned. \n",
            "[I 2025-01-04 14:52:28,762] Trial 836 pruned. \n",
            "[I 2025-01-04 14:52:28,832] Trial 837 pruned. \n",
            "[I 2025-01-04 14:52:28,901] Trial 838 pruned. \n",
            "[I 2025-01-04 14:52:28,971] Trial 839 pruned. \n",
            "[I 2025-01-04 14:52:29,041] Trial 840 pruned. \n",
            "[I 2025-01-04 14:52:29,112] Trial 841 pruned. \n",
            "[I 2025-01-04 14:52:29,176] Trial 842 pruned. \n",
            "[I 2025-01-04 14:52:29,247] Trial 843 pruned. \n",
            "[I 2025-01-04 14:52:29,315] Trial 844 pruned. \n",
            "[I 2025-01-04 14:52:29,380] Trial 845 pruned. \n",
            "[I 2025-01-04 14:52:29,449] Trial 846 pruned. \n",
            "[I 2025-01-04 14:52:29,645] Trial 847 pruned. \n",
            "[I 2025-01-04 14:52:29,714] Trial 848 pruned. \n",
            "[I 2025-01-04 14:52:29,779] Trial 849 pruned. \n",
            "[I 2025-01-04 14:52:29,845] Trial 850 pruned. \n",
            "[I 2025-01-04 14:52:29,913] Trial 851 pruned. \n",
            "[I 2025-01-04 14:52:29,977] Trial 852 pruned. \n",
            "[I 2025-01-04 14:52:30,046] Trial 853 pruned. \n",
            "[I 2025-01-04 14:52:30,111] Trial 854 pruned. \n",
            "[I 2025-01-04 14:52:30,178] Trial 855 pruned. \n",
            "[I 2025-01-04 14:52:30,247] Trial 856 pruned. \n",
            "[I 2025-01-04 14:52:30,316] Trial 857 pruned. \n",
            "[I 2025-01-04 14:52:30,385] Trial 858 pruned. \n",
            "[I 2025-01-04 14:52:30,454] Trial 859 pruned. \n",
            "[I 2025-01-04 14:52:30,524] Trial 860 pruned. \n",
            "[I 2025-01-04 14:52:30,702] Trial 861 pruned. \n",
            "[I 2025-01-04 14:52:30,771] Trial 862 pruned. \n",
            "[I 2025-01-04 14:52:30,840] Trial 863 pruned. \n",
            "[I 2025-01-04 14:52:30,911] Trial 864 pruned. \n",
            "[I 2025-01-04 14:52:30,986] Trial 865 pruned. \n",
            "[I 2025-01-04 14:52:31,056] Trial 866 pruned. \n",
            "[I 2025-01-04 14:52:31,126] Trial 867 pruned. \n",
            "[I 2025-01-04 14:52:31,208] Trial 868 pruned. \n",
            "[I 2025-01-04 14:52:31,281] Trial 869 pruned. \n",
            "[I 2025-01-04 14:52:31,352] Trial 870 pruned. \n",
            "[I 2025-01-04 14:52:31,420] Trial 871 pruned. \n",
            "[I 2025-01-04 14:52:31,489] Trial 872 pruned. \n",
            "[I 2025-01-04 14:52:31,559] Trial 873 pruned. \n",
            "[I 2025-01-04 14:52:31,715] Trial 874 pruned. \n",
            "[I 2025-01-04 14:52:31,789] Trial 875 pruned. \n",
            "[I 2025-01-04 14:52:32,354] Trial 876 pruned. \n",
            "[I 2025-01-04 14:52:32,426] Trial 877 pruned. \n",
            "[I 2025-01-04 14:52:32,495] Trial 878 pruned. \n",
            "[I 2025-01-04 14:52:32,731] Trial 879 pruned. \n",
            "[I 2025-01-04 14:52:33,286] Trial 880 pruned. \n",
            "[I 2025-01-04 14:52:33,350] Trial 881 pruned. \n",
            "[I 2025-01-04 14:52:33,416] Trial 882 pruned. \n",
            "[I 2025-01-04 14:52:33,484] Trial 883 pruned. \n",
            "[I 2025-01-04 14:52:33,553] Trial 884 pruned. \n",
            "[I 2025-01-04 14:52:34,329] Trial 885 pruned. \n",
            "[I 2025-01-04 14:52:34,393] Trial 886 pruned. \n",
            "[I 2025-01-04 14:52:34,461] Trial 887 pruned. \n",
            "[I 2025-01-04 14:52:34,533] Trial 888 pruned. \n",
            "[I 2025-01-04 14:52:34,600] Trial 889 pruned. \n",
            "[I 2025-01-04 14:52:34,668] Trial 890 pruned. \n",
            "[I 2025-01-04 14:52:34,739] Trial 891 pruned. \n",
            "[I 2025-01-04 14:52:34,811] Trial 892 pruned. \n",
            "[I 2025-01-04 14:52:34,881] Trial 893 pruned. \n",
            "[I 2025-01-04 14:52:34,948] Trial 894 pruned. \n",
            "[I 2025-01-04 14:52:35,017] Trial 895 pruned. \n",
            "[I 2025-01-04 14:52:35,089] Trial 896 pruned. \n",
            "[I 2025-01-04 14:52:35,157] Trial 897 pruned. \n",
            "[I 2025-01-04 14:52:35,242] Trial 898 pruned. \n",
            "[I 2025-01-04 14:52:35,314] Trial 899 pruned. \n",
            "[I 2025-01-04 14:52:35,382] Trial 900 pruned. \n",
            "[I 2025-01-04 14:52:35,454] Trial 901 pruned. \n",
            "[I 2025-01-04 14:52:35,529] Trial 902 pruned. \n",
            "[I 2025-01-04 14:52:35,601] Trial 903 pruned. \n",
            "[I 2025-01-04 14:52:35,671] Trial 904 pruned. \n",
            "[I 2025-01-04 14:52:35,743] Trial 905 pruned. \n",
            "[I 2025-01-04 14:52:35,811] Trial 906 pruned. \n",
            "[I 2025-01-04 14:52:36,075] Trial 907 pruned. \n",
            "[I 2025-01-04 14:52:36,146] Trial 908 pruned. \n",
            "[I 2025-01-04 14:52:36,218] Trial 909 pruned. \n",
            "[I 2025-01-04 14:52:36,294] Trial 910 pruned. \n",
            "[I 2025-01-04 14:52:36,365] Trial 911 pruned. \n",
            "[I 2025-01-04 14:52:36,435] Trial 912 pruned. \n",
            "[I 2025-01-04 14:52:36,507] Trial 913 pruned. \n",
            "[I 2025-01-04 14:52:36,580] Trial 914 pruned. \n",
            "[I 2025-01-04 14:52:36,653] Trial 915 pruned. \n",
            "[I 2025-01-04 14:52:36,724] Trial 916 pruned. \n",
            "[I 2025-01-04 14:52:36,791] Trial 917 pruned. \n",
            "[I 2025-01-04 14:52:36,861] Trial 918 pruned. \n",
            "[I 2025-01-04 14:52:36,931] Trial 919 pruned. \n",
            "[I 2025-01-04 14:52:37,002] Trial 920 pruned. \n",
            "[I 2025-01-04 14:52:37,076] Trial 921 pruned. \n",
            "[I 2025-01-04 14:52:37,143] Trial 922 pruned. \n",
            "[I 2025-01-04 14:52:37,212] Trial 923 pruned. \n",
            "[I 2025-01-04 14:52:37,287] Trial 924 pruned. \n",
            "[I 2025-01-04 14:52:37,359] Trial 925 pruned. \n",
            "[I 2025-01-04 14:52:37,481] Trial 926 pruned. \n",
            "[I 2025-01-04 14:52:37,552] Trial 927 pruned. \n",
            "[I 2025-01-04 14:52:37,625] Trial 928 pruned. \n",
            "[I 2025-01-04 14:52:37,697] Trial 929 pruned. \n",
            "[I 2025-01-04 14:52:37,772] Trial 930 pruned. \n",
            "[I 2025-01-04 14:52:37,857] Trial 931 pruned. \n",
            "[I 2025-01-04 14:52:37,927] Trial 932 pruned. \n",
            "[I 2025-01-04 14:52:38,141] Trial 933 pruned. \n",
            "[I 2025-01-04 14:52:38,217] Trial 934 pruned. \n",
            "[I 2025-01-04 14:52:38,289] Trial 935 pruned. \n",
            "[I 2025-01-04 14:52:38,363] Trial 936 pruned. \n",
            "[I 2025-01-04 14:52:38,438] Trial 937 pruned. \n",
            "[I 2025-01-04 14:52:39,331] Trial 938 pruned. \n",
            "[I 2025-01-04 14:52:39,403] Trial 939 pruned. \n",
            "[I 2025-01-04 14:52:39,476] Trial 940 pruned. \n",
            "[I 2025-01-04 14:52:39,548] Trial 941 pruned. \n",
            "[I 2025-01-04 14:52:39,621] Trial 942 pruned. \n",
            "[I 2025-01-04 14:52:39,691] Trial 943 pruned. \n",
            "[I 2025-01-04 14:52:39,763] Trial 944 pruned. \n",
            "[I 2025-01-04 14:52:39,836] Trial 945 pruned. \n",
            "[I 2025-01-04 14:52:39,909] Trial 946 pruned. \n",
            "[I 2025-01-04 14:52:39,980] Trial 947 pruned. \n",
            "[I 2025-01-04 14:52:40,055] Trial 948 pruned. \n",
            "[I 2025-01-04 14:52:40,127] Trial 949 pruned. \n",
            "[I 2025-01-04 14:52:40,200] Trial 950 pruned. \n",
            "[I 2025-01-04 14:52:40,272] Trial 951 pruned. \n",
            "[I 2025-01-04 14:52:40,344] Trial 952 pruned. \n",
            "[I 2025-01-04 14:52:40,417] Trial 953 pruned. \n",
            "[I 2025-01-04 14:52:40,491] Trial 954 pruned. \n",
            "[I 2025-01-04 14:52:40,912] Trial 955 pruned. \n",
            "[I 2025-01-04 14:52:40,986] Trial 956 pruned. \n",
            "[I 2025-01-04 14:52:41,055] Trial 957 pruned. \n",
            "[I 2025-01-04 14:52:41,128] Trial 958 pruned. \n",
            "[I 2025-01-04 14:52:41,198] Trial 959 pruned. \n",
            "[I 2025-01-04 14:52:41,272] Trial 960 pruned. \n",
            "[I 2025-01-04 14:52:41,352] Trial 961 pruned. \n",
            "[I 2025-01-04 14:52:41,430] Trial 962 pruned. \n",
            "[I 2025-01-04 14:52:41,518] Trial 963 pruned. \n",
            "[I 2025-01-04 14:52:41,593] Trial 964 pruned. \n",
            "[I 2025-01-04 14:52:41,664] Trial 965 pruned. \n",
            "[I 2025-01-04 14:52:41,737] Trial 966 pruned. \n",
            "[I 2025-01-04 14:52:41,814] Trial 967 pruned. \n",
            "[I 2025-01-04 14:52:41,888] Trial 968 pruned. \n",
            "[I 2025-01-04 14:52:41,996] Trial 969 pruned. \n",
            "[I 2025-01-04 14:52:42,071] Trial 970 pruned. \n",
            "[I 2025-01-04 14:52:42,142] Trial 971 pruned. \n",
            "[I 2025-01-04 14:52:42,218] Trial 972 pruned. \n",
            "[I 2025-01-04 14:52:42,295] Trial 973 pruned. \n",
            "[I 2025-01-04 14:52:42,372] Trial 974 pruned. \n",
            "[I 2025-01-04 14:52:42,694] Trial 975 pruned. \n",
            "[I 2025-01-04 14:52:42,769] Trial 976 pruned. \n",
            "[I 2025-01-04 14:52:42,840] Trial 977 pruned. \n",
            "[I 2025-01-04 14:52:43,124] Trial 978 pruned. \n",
            "[I 2025-01-04 14:52:43,200] Trial 979 pruned. \n",
            "[I 2025-01-04 14:52:43,276] Trial 980 pruned. \n",
            "[I 2025-01-04 14:52:43,752] Trial 981 pruned. \n",
            "[I 2025-01-04 14:52:43,828] Trial 982 pruned. \n",
            "[I 2025-01-04 14:52:43,902] Trial 983 pruned. \n",
            "[I 2025-01-04 14:52:43,978] Trial 984 pruned. \n",
            "[I 2025-01-04 14:52:44,051] Trial 985 pruned. \n",
            "[I 2025-01-04 14:52:44,125] Trial 986 pruned. \n",
            "[I 2025-01-04 14:52:44,203] Trial 987 pruned. \n",
            "[I 2025-01-04 14:52:44,279] Trial 988 pruned. \n",
            "[I 2025-01-04 14:52:44,354] Trial 989 pruned. \n",
            "[I 2025-01-04 14:52:44,429] Trial 990 pruned. \n",
            "[I 2025-01-04 14:52:44,511] Trial 991 pruned. \n",
            "[I 2025-01-04 14:52:44,590] Trial 992 pruned. \n",
            "[I 2025-01-04 14:52:44,670] Trial 993 pruned. \n",
            "[I 2025-01-04 14:52:44,908] Trial 994 pruned. \n",
            "[I 2025-01-04 14:52:44,983] Trial 995 pruned. \n",
            "[I 2025-01-04 14:52:45,061] Trial 996 pruned. \n",
            "[I 2025-01-04 14:52:45,140] Trial 997 pruned. \n",
            "[I 2025-01-04 14:52:45,215] Trial 998 pruned. \n",
            "[I 2025-01-04 14:52:45,295] Trial 999 pruned. \n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(directions=[\"minimize\"], pruner=optuna.pruners.MedianPruner())\n",
        "study.optimize(objective, \n",
        "               n_trials=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIGUizShPEnV",
        "outputId": "9d6d0e40-0bab-448d-b11f-d9e461a2896f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'droprate': 0.05020124278439857,\n",
              " 'lr': 0.00040883056777993407,\n",
              " 'n_epochs': 1837,\n",
              " 'base_lr': 7.28594690744838e-05,\n",
              " 'max_lr': 0.009068312862929105}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the tuned hyperparameters\n",
        "study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGC88QnUOPT1"
      },
      "source": [
        "### Train model with tuned hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "RFza2tBZWH2V"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, criterion, optimizer, scheduler, train_loader, epoch):\n",
        "  train_losses = []\n",
        "  for epoch in range(epoch):\n",
        "    model.train()\n",
        "    training_loss = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "      # Forward pass\n",
        "      predictions = model(inputs)\n",
        "      loss = criterion(predictions, targets)\n",
        "      # Zero out existing gradients\n",
        "      optimizer.zero_grad()\n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      training_loss += loss.item()\n",
        "    # Adjust learning rates after every epoch\n",
        "    scheduler.step()\n",
        "    # Check overall training loss\n",
        "    training_loss = training_loss/len(train_loader)\n",
        "    train_losses.append(training_loss)\n",
        "    # Print training loss\n",
        "    if epoch % 100 == 0:\n",
        "      print(training_loss)\n",
        "  return model, train_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "wBfxpOiWOukh"
      },
      "outputs": [],
      "source": [
        "# Parameter setup\n",
        "droprate = study.best_params['droprate']\n",
        "brchindices = brchsize\n",
        "numvars = len(brchsize[1:])\n",
        "# Initialize model\n",
        "model = baseline_ts_drop(droprate, brchindices, numvars)\n",
        "# Define a loss function and optimizer\n",
        "loss_fn = torch.nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=study.best_params['lr'])\n",
        "# Use cyclical LR scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=study.best_params['base_lr'], max_lr=study.best_params['max_lr'], cycle_momentum=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZJL2ExbOylS",
        "outputId": "e5c367cb-41c3-4ea0-a1d7-264ec570db78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.7421462059020996\n",
            "1.413552451133728\n",
            "1.0957757949829101\n",
            "0.9884430050849915\n",
            "0.9777918338775635\n",
            "0.9728657126426696\n",
            "0.9692647695541382\n",
            "0.9755651712417602\n",
            "0.971016013622284\n",
            "0.9607572913169861\n",
            "0.9318708062171936\n",
            "0.9455866694450379\n",
            "0.9520484447479248\n",
            "0.9209734201431274\n",
            "0.9436138868331909\n",
            "0.9360065460205078\n",
            "0.9339823961257935\n",
            "0.9298583149909974\n",
            "0.9423739790916443\n"
          ]
        }
      ],
      "source": [
        "trained_model, train_loss = training_loop(model,loss_fn,optimizer,scheduler,dataloader,study.best_params['n_epochs'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "HhYe1zbVaqEv",
        "outputId": "8c34aa4b-9234-45ec-c56c-3fb295381c6f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_loss)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check the RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_rmse(model, test_loader):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    predictions, targets = [], []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for inputs, target in test_loader:\n",
        "            output = model(inputs)\n",
        "            predictions.append(output.cpu().numpy())  # Convert to numpy\n",
        "            targets.append(target.cpu().numpy())\n",
        "\n",
        "    # Flatten the lists and calculate RMSE\n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    targets = np.concatenate(targets, axis=0)\n",
        "    SStot = np.sum((targets - np.mean(targets)) ** 2)\n",
        "    SSres = np.sum((targets - predictions) ** 2)\n",
        "    r2 = 1 - SSres / SStot\n",
        "    rmse = np.sqrt(mean_squared_error(targets, predictions))\n",
        "    return r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test RMSE: 1.8852\n",
            "Validation RMSE: 1.1494\n",
            "Train RMSE: 1.5256\n"
          ]
        }
      ],
      "source": [
        "# Evaluate RMSE on the test set\n",
        "rmse_test = evaluate_rmse(trained_model, dataloader_test)\n",
        "print(f\"Test RMSE: {rmse_test:.4f}\")\n",
        "rmse_validation = evaluate_rmse(trained_model, dataloader_validation)\n",
        "print(f\"Validation RMSE: {rmse_validation:.4f}\")\n",
        "rmse_train = evaluate_rmse(trained_model, dataloader_train)\n",
        "print(f\"Train RMSE: {rmse_train:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tensor_flow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
